\renewcommand{\mat}{\mathbf}

\begin{itemize}
\item $n$ sample points $x_i \in \R^d$, $i = 1, \ldots, n$
\item $d = 2$ where not stated.
\end{itemize}

\section{Overview}

Linear regression lays down a linear surface over $\R^d$. The parameters of
that surface ($\vec w$) are scored according to the sum of squared distances of
the $y$ values from the surface.

Logistic regression lays down a logistic surface over $\R^d$. The parameters of
that surface ($\vec w$) are scored according to the probability of drawing the
$y$ values from the probability distribution given by the surface.

In both cases, the score (cost) is a measure of distance of the $y$ values from
the surface, i.e. a distance between $\vec y$ and the predictions
$\hat \y$.

The surface over $\R^d$ maps the $n$ sample points $\x_i \in \R^d$ to their
predictions $y_i$ in $\R$ or $[0, 1]$.

\section{Neural networks}

A neural net with one hidden layer of $K$ units first maps
$\x_i \in \R^d \to \R^K$ using parameter matrix $\mat V$, and then maps
$\R^K \to \R$ using parameters $\vec w$. Again, the cost associated with
parameters $(\mat V, \vec w)$ is a distance between $\vec y$ and the
$\hat \y$ values in the output layer.

\subsection{Backpropagation algorithm}

\subsubsection{No hidden layers: linear regression}

\begin{align*}
  \x \xrightarrow{\w} (\hat y = \x^\T\w) \rightarrow L
\end{align*}

We want to do gradient descent on $\w$.

\begin{align*}
  \partiald{L}{w_j}
  =& \sum_i \partiald{L}{\hat{y}_i} \partiald{\hat{y}_i}{w_j} \\
  =& \sum_i 2(\hat{y}_i - y_i) x_{ij} \\
  =& \sum_i 2(\x_i^\T \w - y_i) x_{ij} \\
\end{align*}

Alternatively we can compute the gradient using the chain rule:
\begin{align*}
L(\w) = ||\X\w - \y||^2 = (\X\w - \y)^\T(\X\w - \y)
\end{align*}
so
\begin{align*}
  \grad_\w L = 2\X^\T\X\w - 2\X^\T \y
\end{align*}
the $j$-th component of which is
\begin{align*}
  \partiald{L}{w_j} = 2w_j 2X_{\cdot j} \cdot \y
\end{align*}



\begin{comment}
\textbf{Duplicated chain rule linear regression gradient derivation}\\
What is $\grad_\w L$? Chain rule: let $L = g \circ f$, i.e. $L(\w) = g(f(\w))$, where

\begin{align*}
  f:& \R^n \to \R^n~~~~~~~~~~~~~~~~~~~~~~~~g(\w) = \X\w - \y \\
  g:& \R^n \to \R~~~~~~~~~~~~~~~~~~~~~~~~~f(\vec v) = \vec{v}^\T \vec v \\
\end{align*}

The chain rule says that $\grad (g \circ f) = (D f)^\T \grad g$, where $D f$ is
the derivative of $f$, i.e. the Jacobian matrix of first partial derivatives, so

\begin{align*}
  \grad_\w L &= 2\X^\T(\X\w - \y) \\
             &= 2\X^\T\X\w - 2\X^\T \y
\end{align*}
\end{comment}


\subsubsection{One hidden layer}
Consider classification using a neural net with one hidden layer $\vec h$ of $H$ units.

We consider one sample point $x$ at a time.

There are $K$ possible categories, and the predictions $\hat y$ in the output
layer can be interpreted as the probabilities each category given the input $x$.

\textbf{Model specification}:

$K$ possible output categories; one hidden layer of $H$ units; $\tanh$
activation in the hidden layer; logistic activation in the output
layer. Notation:

\begin{tabular}{l|l|l|l}
                        &                                  & indices   & dimensions \\
  \hline
  \textbf{Input layer}  & $\x$                             & $x_j$     & $d \times 1$ \\
  \textbf{Weights}      & $\V$                             & $V_{hj}$  & $H \times d$ \\
  \textbf{Hidden layer} & $\z = \tanh(\mat V \x)$          & $z_h$     & $H \times 1$ \\
  \textbf{Weights}      & $\W$                             & $W_{kh}$  & $K \times H$ \\
  \textbf{Ouput layer}  & $\vec \yhat = \sigma(\W \z)$     & $\yhat_k$ & $K \times 1$ \\
  \textbf{Loss}         & $L(\vec \yhat, \vec y)$          &           & scalar \\
\end{tabular}

where $\sigma$ is the logistic function $\sigma(x) = (1-e^{-x})^{-1}$, and
$\tanh$ and $\sigma$ act elementwise.

The loss (cost) function is the cross-entropy (log likelihood of training labels given
predictions)

\begin{align*}
  -L(\yhat, \y) = \sum_k y_k \log(\yhat_k) + (1 - y_k) \log( 1 - \yhat_k).
\end{align*}

\subsubsection{Gradient descent algorithm}

We want to do gradient descent on the full set $(\mat V, \mat W)$ of
parameters. This involves computing gradients of the loss function $\grad_V L$
and $\grad_W L$. We derive the gradients with respect to one row of these
matrices at a time, and give code fragments showing how to compute the matrix
of derivatives efficiently.

% Recall that $\sigma' = \sigma (1 - \sigma)$, and $\tanh' = 1 - \tanh^2$\footnote{
% The definition of $\tanh$ is $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$, so
% the derivative is

% \begin{align*}
%   \frac{d}{dx} \tanh(x)
%   = \frac{(e^x + e^{-x})^2 - (e^x - e^{-x})^2}{(e^x + e^{-x})^2}
%   = 1 - \frac{(e^x - e^{-x})^2}{(e^x + e^{-x})^2}
%   = 1 - \tanh^2(x)
%   = \sech^2(x).
% \end{align*}
% }


\subsubsection{Gradient with respect to weight matrix $\W$}

$\W_k$ is one row of $\W$, of length $H+ 1$. We have

\begin{align*}
  \grad_{\W_k} L = \partiald{L}{\yhat_k}\grad_{\W_k} \yhat_k.
\end{align*}

Now, $\yhat_k = \sigma(\W_k\z)$, so
\begin{align*}
  \grad_{\W_k} \yhat_k = \z\yhat_k(1 - \yhat_k).
\end{align*}
This expression is still correct if the offset is implemented as an additional
``dimension'', in which case the last element of $\W_k$ is the offset and the
last element of $\z$ is 1.

The derivative of the loss with respect to $\yhat_k$ is
\begin{align*}
  \partiald{L}{\yhat_k} =
  -\frac{y_k}{\yhat_k} + \frac{1 - y_k}{1 - \yhat_k} =
  \frac{\yhat_k - y_k}{\yhat_k(1 - \yhat_k)}.
\end{align*}
Multiplying these quantities gives
\begin{align*}
  \grad_{\W_k} L = \z(\yhat_k - y_k).
\end{align*}

In code we can compute the full matrix of derivatives $\grad_{\W}$ using
vector/matrix primitives as
\begin{align*}
  \diag(\vec{\yhat} - \y) ~ \mat Z,
\end{align*}
where the rows of $\mat Z$ are each equal to $\z$:

\begin{minted}{python3}
  grad__L__z = (W.T * (yhat - y)).sum(axis=1)
  zz = z.reshape((1, H + 1)).repeat(K, 0)
  grad__L__W = diag(yhat - y) @ zz
\end{minted}


\subsubsection{Gradient with respect to weight matrix $\V$}

$\V_h$ is one row of $\V$, of length $d + 1$. We have

\begin{align*}
  \grad_{\V_h} L = \partiald{L}{\z_h}\grad_{\V_h} \z_h.
\end{align*}

Now,
$\partiald{L}{z_h} = \sum_k \partiald{L}{\yhat_k} \partiald{\yhat_k}{z_h}$.
We've already found $\partiald{L}{\yhat_k}$ above, and
$\partiald{\yhat_k}{z_h} = W_{kh}\yhat_k(1 - \yhat_k)$, giving
\begin{align*}
  \partiald{L}{z_h} = \sum_k W_{kh} (\yhat_k - y_k).
\end{align*}

$\z_h = \tanh(\V_h \x)$, so $\grad_{\V_h} \z_h = \x(1 - z_h^2)$, and
multiplying the two quantities gives

\begin{align*}
  \grad_{\V_h} L =  \x(1 - z_h^2) \sum_k W_{kh} (\yhat_k - y_k).
\end{align*}


Again, in code we can compute the full matrix of derivatives $\grad_{\V} L$
using vector/matrix primitives:

\begin{minted}{python3}
  grad__L__z = (W.T * (yhat - y)).sum(axis=1)
  xx = x.reshape((1, d + 1)).repeat(H + 1, 0)
  grad__L__V = diag((1 - z ** 2) * grad__L__z) @ xx
\end{minted}

\newpage

\subsection{Other neural network notes}

So the objective function is $L({\hat \y}(\z(\x)))$, or
\begin{align*}
  \x \xrightarrow{\mat V} \z \xrightarrow{\mat W} \hat \y \rightarrow L
\end{align*}

We want to compute the gradient vector, i.e. partials $\partiald{L}{V_{hj}}$
and $\partiald{L}{w_k}$.

Recall that $\sigma' = \sigma (1 - \sigma)$, and note that
$\yhat_k = \sigma(\vec w_k^\T \z)$, so

\begin{align*}
  \partiald{\yhat_k}{W_{kh}} &= \yhat_k (1 - \yhat_k) \partiald{\vec w_k^\T \z}{W_{kh}} = \yhat_k (1 - \yhat_k)z_h.
\end{align*}

The gradient with respect to $\mat W$ is

\begin{align*}
  \grad_{\w_k} L = \z(y_k - \yhat_k)
\end{align*}

(proof similar to that in Logistic Regression section), or non-vectorized version:

\begin{align*}
  \partiald{L}{W_{kh}}
  &= \partiald{}{W_{kh}} \sum_{k'} y_{k'} \log(\yhat_{k'}) + (1 - y_{k'}) \log( 1 - \yhat_{k'}) \\
  &=        y_k      \frac{ \yhat_k (1 - \yhat_k)z_h }{ \yhat_k } -
           (1 - y_k) \frac{ \yhat_k (1 - \yhat_k)z_h}{ 1 - \yhat_k} \\
  &= z_h \big(y_k (1 - \yhat_k) - (1 - y_k) \yhat_k \big) \\
  &= z_h \big(y_k - \yhat_k\big)
\end{align*}

The gradient with respect to $\mat V$ is given by


\begin{align*}
  \grad_{\V_h} L = \sum_k \partiald{L}{\yhat_k} \grad_{\v_h} \yhat_k
\end{align*}

\includegraphics[width=500pt]{img/machine-learning-neural-net-backpropagation-1.png}

\subsection{Trivial case}

\newcommand{\matrixiijj}[4]{\begin{bmatrix}#1 & #3\\#2 & #4\end{bmatrix}}
\newcommand{\matrixijj}[2]{\begin{bmatrix}#1 & #2\end{bmatrix}}

\subsubsection{Forwards}

\begin{enumerate}
\item 1d input 0 with offset dimension: $x=\cvec{0}{1}$
\item $K=1$. Label $y = 1$
\item Initial $V = \matrixiijj{0}{0}{0}{0}$ (last row ignored)
\item $Vx = \matrixiijj{0}{0}{0}{0} \cvec{0}{1} = \cvec{0}{0}$ (last element ignored)
\item 1 hidden unit. $z \leftarrow \cvec{\tanh(0)}{1} = \cvec{0}{1}$
\item Initial $W = \matrixijj{0}{0}$
\item $Wz = \matrixijj{0}{0} \cvec{0}{1} = 0$
\item $\yhat = s(0) = 0.5$
\end{enumerate}

\subsubsection{One iteration of backpropagation}

\begin{enumerate}
\item $\grad_{W_k} \yhat_k = \z \yhat_k(1 - \yhat_k) = \cvec{0}{0.25}$
\end{enumerate}

\begin{align*}
  \grad_{W_k} L &= \partiald{L}{\yhat_k} \grad_{W_k} \yhat_k \\
  \partiald{L}{\yhat_k} &= \frac{y_k - \yhat_k}{\yhat_k (1 - \yhat_k)} \\
  \grad_{W_k} \yhat_k &= z \yhat_k (1 - \yhat_k) \\
\end{align*}

\begin{align*}
  \grad_z L &= \sum_k \partiald{L}{\yhat_k} \grad_z \yhat_k \\
  \grad_z \yhat_k &= W_k \yhat_k (1 - \yhat_k) \\
\end{align*}

\begin{align*}
  \grad_{V_h} L &= \partiald{L}{z_h} \grad_{V_h} z_h \\
  \grad_{V_h} z_h &= x(1 - z_h^2)
\end{align*}




\section{Classification}

A \textbf{decision boundary} is a curve separating the plane (sample space)
into two regions.

Some classifiers involve a \textbf{decision function} $f$, in which case
$f(\x) = 0$ describes the decision boundary.

A \textbf{linear classifier} uses a linear decision function
$f(x) = \w \cdot \x + \alpha$. This is scalar-valued: it's a plane over
the plane (sample space). Its intersection defines a linear decision boundary.

In $d$-dimensions the decision boundary is a hyperplane
($(d-1)$-dimensional). This still separates the sample space into two regions.

\textbf{Example:} $f(x) = \cvec{1}{1} \cdot \cvec{x_1}{x_2} + 4$
\includegraphics[width=200pt]{img/machine-learning-linear-decision-boundary.png}
\begin{itemize}
\item A plane sloping up at 45° in the north-east direction.
\item Each input feature has equal influence on the classification.
\item Decision boundary is line $x_1 + x_2 = -4$.
\item $\w$ is normal to the decision boundary since $\w \cdot (\x_1 - \x_2) = -4 - (-4) = 0$.
\item If one feature has a very high weight then $\w$ points close to that
  axis and the decision boundary is almost perpendicular to that axis (other
  features almost don't matter).
\end{itemize}

\textbf{Distance from the decision boundary to a point}: For some point $\x_i$,
the height of the decision function plane above $\x_i$ is
$\w \cdot \x_i + \alpha$. At the decision boundary, this height is
zero. Looking ``straight up'' the slope of the decision function, its gradient
is $\sqrt{w_1^2 + w_2^2} = |\w|$. So the distance of a point $\x_i$ from the
hyperplane is $\frac{\w \cdot \x_i + \alpha}{|\w|}$. If $\w$ is not a unit
vector, the problem can be rescaled so that it is, in which case the distance
is $\w \cdot \x_i + \alpha$.
\\
% \begin{mdframed}
%   \includegraphics[width=300pt]{img/machine-learning-linear-decision-boundary-2.png}
% \end{mdframed}

\textbf{Examples of linear classifiers}:
\begin{itemize}
\item \textbf{Centroid method}:  Decision boundary perpendicular to and bisects line
  connecting means of labeled training points.
\item \textbf{Perceptron}:
\item \textbf{Maximum margin classifier}:
\item \textbf{LDA}:  Fit Gaussians to each class, same covariance across classes.
\end{itemize}

\subsection{Perceptron}

Labels $y_i \in \{-1, 1\}$. Assume $\alpha=0$ for now (decision boundary through origin).

\textbf{Goal}: find line separating points (separating hyperplane). I.e. Find $\w$ such that
\begin{align*}
  \begin{cases}
    \x_i \cdot \w \leq 0, &y_i = -1 \\
    \x_i \cdot \w \geq 0, &y_i = +1.
  \end{cases}
\end{align*}
This is equivalent to the \textbf{constraint} $y_i\x_i \cdot \w \geq 0$.

\textbf{Cost function}: total distance $R(\w)$ of misclassified points from the
decision boundary.
\\
\begin{mdframed}
  \textbf{Optimization problem}: Find $\w$ that minimizes
  \begin{align*}
    R(w) = \sum_i L(\x_i \cdot \w, y_i) = \sum_{i \in V} -y_i\x_i \cdot \w,
  \end{align*}
  where $V$ are the misclassified points.
\end{mdframed}

Per-training point loss function
\begin{align*}
  L(\text{prediction}_i, y_i) = L(\x_i \cdot \w, y_i) =
  \begin{cases}
    0, &\text{correct}, y_i\x_i \cdot \w \geq 0\\
    -y_i\x_i \cdot \w, &\text{misclassified}
  \end{cases}
\end{align*}


\textbf{Gradient descent}: Find $w$ that minimizes $R(w)$.


\begin{align*}
  \nabla_w R = \cveccc{-\sum_i y_iX_{i1}}
                      {\vdots}
                      {-\sum_i y_iX_{id}}
\end{align*}
\begin{itemize}
\item On each iteration, compute the gradient; update $\w$ by taking a step
  downhill of size $\rho$: $\w \leftarrow \w + \rho \sum_{i \in V} y_i\x_i$.
\item A misclassified data point far out in dimension $j$ will cause the
  gradient to have a large component $-\sum_i y_iX_{ij}$ in that dimension.
\item $\w$ thus becomes more closely aligned with that axis and the decision
  boundary.
\item Decision boundary therefore becomes more perpendicular to that axis (axis
  becomes more ``important'').
\end{itemize}

\textbf{Stochastic gradient descent (Perceptron)}: on each iteration pick one misclassified
point and update $\w$ using gradient for that point:
$\w \leftarrow \w + \rho y_{i^*}\x_{i^*}$

\textbf{Allow decision boundaries that do not pass through origin}: add a
fictitious dimension so that sample points now lie on the plane $x_{d+1} = 1$
in $(d+1)$ dimensions. Run algorithm as above, just with the new
dimensionality.
\begin{align*}
  \w \cdot \x + \alpha &= 0 \\
  \cveccc{w_1}
         {w_2}
         {\alpha} \cdot \cveccc{x_1}{x_2}{1} &= 0.
\end{align*}

\subsection{Optimization in weight space}

\begin{tabular}{l|l}
  $\x$-space    & $\w$-space \\
  \hline
  hyperplane & point $\w$ is normal vector to hyperplane \\
  point      & hyperplane whose normal vector is the $\x$ point (? don't understand this yet)
\end{tabular}

\includegraphics[width=200pt]{img/ml-perceptron-x-space-w-space.png}

\subsection{Maximum margin classifiers}
\textbf{Margin} is distance from hyperplane to nearest sample point.

Previously, in the perceptron, we used the constraint
$$y_i \x_i \cdot \w \geq 0.$$
Now, we demand that there is a non-zero margin between the decision boundary
and the points:
$$y_i (\x_i \cdot \w + \alpha) \geq 1,$$
The 1 on the RHS is arbitrary; I think $\w$ and $\alpha$ will adapt to make it
true for any positive value, so the point is that we're demanding a strictly
non-zero margin.
\\
\begin{mdframed}
  \textbf{Optimization problem (quadratic program)}:\\
    Find $\w, \alpha$ that minimize $|\w|^2$ such that
    $y_i(\x_i \cdot \w + \alpha) \geq 1$ for all points $i$.
\end{mdframed}

\subsection{Soft margin SVMs}
\footnote{\url{https://people.eecs.berkeley.edu/~jrs/189/lec/04.pdf}}
\footnote{ \url{https://www.youtube.com/watch?v=HOZ6ZpPA_Ks}}


\begin{itemize}
\item Still quadratic program but allow points to violate margin via
  \textbf{slack variables} $\xi_i \geq 0$:
\item Constraint is $y_i(\x_i \cdot \w + \alpha) \geq 1 - \xi_i$
\item Find non-linear decision boundaries by introducing new features
  comprising non-linear functions of base features (``lift points into
  higher-dimensional space'').
\end{itemize}

\includegraphics[width=300pt]{img/machine-learning-svm-1.png}
% \includegraphics[width=300pt]{img/ml-svm-parabolic-lifting.png}



% \section{5 Machine Learning Abstractions and Numerical Optimization}
% \url{https://people.eecs.berkeley.edu/~jrs/189/lec/05.pdf}\\
% \url{https://www.youtube.com/watch?v=DeIAXPUfCbQ}

\newpage
\section{Decision Theory}
\footnote{\url{https://people.eecs.berkeley.edu/~jrs/189/lec/06.pdf}}
\footnote{\url{https://www.youtube.com/watch?v=aXkenQ01qYI}}

Suppose there are two possible \textbf{classes}: $\{C, D\}$

\textbf{Decision rule}: $r(\x): \R^d \to \{C, D\}$

\textbf{Loss function}: E.g. 0-1 loss:
\begin{align*}
  L(y_i \to \hat y_i) =
  \begin{cases}
    0, & \hat y_i = y_i ~~~~~~~~~~~~~~~~~~~~\text{(correct classification)}\\
    1, & \otherwise
  \end{cases}
\end{align*}

\textbf{Risk}: Functional $R(r)$: expected loss for rule $r$, over $\p(X, Y)$.
\footnote{
\begin{align*}
  R(r) = &\pi(Y=-1)\E_\X L(-1 \to r(X)) + \\
         &\pi(Y=+1)\E_\X L(+1 \to r(X))~~~~~~~~~~~~~~~~~~~~~~~\text{over $\p(Y)\p(X|Y)$}
\end{align*}
\begin{align*}
  = \sum_X \p(X)\big(&\pi(Y=-1)L(-1 \to r(X)) + \\
                     &\pi(Y=+1)L(+1 \to r(X))\big)~~~~~\text{over $\p(X)\p(Y|X)$}
\end{align*}
}

So what rule function $r$ minimizes the functional $R$?

\textbf{Bayes decision rule}: Assign $\x$ to class $C$ if

\begin{center}
(C posterior at $\x$) $\times$ (penalty for misclassifing a true C)
\end{center}

is largest for class $C$. I.e. if
\begin{align*}
  \p(C|\x)L(D|C) > \p(D|\x)L(C|D).
\end{align*}

With 0-1 loss, this is: ``assign to class with highest posterior''.

With 0-1 loss and two classes, it's: ``assign to class with posterior $> 0.5$''.

\textbf{Empirical risk}: Discriminative methods (e.g. logistic regression) lack
any model for $X$. How can we estimate expected loss over $p(X,Y)$? Take the
observed sample points as defining a discrete, uniform distribution, in which
case
\begin{align*}
  \hat R(r) = \frac{1}{n}\sum L(r(x_i), y_i).
\end{align*}
This provides a justification for minimizing the sum/mean of per-sample loss.

\newpage
\section{Statistical justifications}

Regression: want to estimate a function $f$ such that
$y_i = f(x_i) + \epsilon$, where $\epsilon$ has unknown distribution but mean
0. Ideal would be to estimate $f$ with $h(x_i) = \E(Y|x_i)$ since this is equal
to $f(x_i)$.

Likelihood justification for linear regression cost function.

Logistic Regression from Maximum Likelihood

\section{Bias-Variance Decomposition}

\includegraphics[width=300pt]{img/machine-learning-bias-variance-decomp-1.png}





\newpage
\section{Gaussian discriminant analysis}
\footnote{\url{https://people.eecs.berkeley.edu/~jrs/189/lec/07.pdf}}
\footnote{\url{https://www.youtube.com/watch?v=4CefboCXxZs}}

\textbf{Anisotropic}:
\begin{align*}
  \p(\x) = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} \exp\(-\frac{1}{2}(\x - \mu)^\T\Sigma^\1(\x - \mu)\)
\end{align*}

\textbf{Isotropic}:
\begin{align*}
  \p(\x) = \frac{1}{(2\pi)^{d/2}\sigma^d} \exp\(-\frac{|\x - \mu|^2}{2\sigma^2}\)
\end{align*}

\newcommand{\class}{\text{class~}}

\subsection{Isotropic Gaussians}

Multivariate data $\x$ but features uncorrelated and all features same variance.

\subsubsection{QDA}
Fit separate Gaussians to the training data in each class. The likelihood is
\begin{align*}
  \p(\x|\class C) = \frac{1}{(2\pi)^{d/2}\sigma_C^d}\exp\(-\frac{|\x - \mu_C|^2}{\sigma_C^2}\)
\end{align*}

and we compare the value of $\p(\x|\class C) \cdot \pi_C \cdot L(D|C)$.

The decision boundaries are where the posterior $\times$ loss are equal. It's
easier to compare the log of this:
\begin{align*}
  Q_C(\x) = - \frac{|\x - \mu_C|^2}{\sigma_C^2} - d\log \sigma_C + \log \pi_C + \log L(D|C)
\end{align*}

\begin{comment}
  So the decision boundary is at $\x$ satisfying
  \begin{align*}
    Q_C(\x) - Q_D(\x) &= 0 \\
    \frac{|\x - \mu_D|^2}{\sigma_D^2} - \frac{|\x - \mu_C|^2}{\sigma_C^2} +
    d\log \frac{\sigma_D}{\sigma_C} +
    \log \frac{\pi(C)}{\pi(D)} +
    \log \frac{L(D|C)}{L(C|D)}
     &= 0 \\
  \end{align*}
\end{comment}

The posterior probability of class $C$ at point $\x$ is\footnote{This is
  assuming 0-1 loss, so the loss doesn't affect $Q_C(\x)$}
\begin{align*}
  \p(C|\x)
  = \frac{\pi_C\p(\x|C)}{\pi_C\p(\x|C) + \pi_D\p(\x|D)}
  = \frac{1}{1 + e^{-(Q_C(\x) - Q_D(\x))}},
\end{align*}
so logistic in the quadratic expression $Q_C(\x) - Q_D(\x)$.

\subsubsection{LDA}
Estimate separate class means but same variance for all classes. So now
\begin{align*}
  Q_C(\x) - Q_D(\x)
  &= \frac{|\x - \mu_D|^2 - |\x - \mu_C|^2}{\sigma^2} + \log \frac{\pi_C}{\pi_D} + \log \frac{L(D|C)}{L(C|D)} \\
  &= \frac{(\x - \mu_D)\cdot(\x - \mu_D) - (\x - \mu_C)\cdot(\x - \mu_C)}{\sigma^2} + \log \frac{\pi_C}{\pi_D} + \log \frac{L(D|C)}{L(C|D)} \\
% &= \frac{ 2(\x\cdot\mu_C - \x\cdot\mu_D) + |\mu_D|^2 - |\mu_C|^2}{\sigma^2} + \log \frac{\pi_C}{\pi_D} + \log \frac{L(D|C)}{L(C|D)} \\
  &= \x\cdot\frac{2(\mu_C - \mu_D)}{\sigma^2} + \(\frac{|\mu_D|^2 - |\mu_C|^2}{\sigma^2} + \log \frac{\pi_C}{\pi_D} + \log \frac{L(D|C)}{L(C|D)}\) \\
  &= \x\cdot\w + \alpha
\end{align*}
This means that the decision boundary is linear, and (with 0-1 loss) the
posterior is a logistic function which is constant parallel to the decision
boundary.

\section{Symmetric matrices, quadratic forms and eigenvectors}
\footnote{\url{https://people.eecs.berkeley.edu/~jrs/189/lec/08.pdf}}


\textbf{Spectral theorem}: A symmetric matrix has $n$ \textbf{orthogonal}
eigenvectors\footnote{There may be more than $n$ (infinite) eigenvectors, but
  $n$ orthogonal.}\footnote{Non-symmetric matrices have non-orthogonal eigenvectors in
  general.}


To understand a symmetric matrix $\A$, consider its \textbf{quadratic form}
$|\A\x|^2 = \x^\T \A^2\x$ (right). Compare this to the graph of $|\z|^2$
(left). The graphs are related by the following changes of coordinates:

$\z \gets \A\x$ changes the elliptical contours into circles; scale by eigenvalues of $\A$.

$\A^\1 \z \to \x$ changes circles into ellipses; scale by reciprocal of eigenvalues.

\includegraphics[width=300pt]{img/machine-learning-quadratic-form-eigenvectors.png}

$|\A\x|^2 = 1$ is the equation of an ellipsoid. Its axes are $v_1,\ldots,v_n$
and its radii are $\frac{1}{\lambda_1},\ldots,\frac{1}{\lambda_n},$

Bigger eigenvalue $\iff$ steeper hill.

Alternate interpretation: the ellipsoids are spheres in a space with a
different distance metric. The distance metric (metric tensor) is $\mat M=\A^2$:
\begin{align*}
  d(\x,\x') = |\A\x| - |\A\x'| = \sqrt{(\x - \x')\A^2(\x - \x')}
\end{align*}


\begin{minipage}{\textwidth}
These are diagrams of $\x^\T\A\x$ (not $\x^\T\A^2\x$ since $\A^2$ has no negative eigenvalues):

\includegraphics[width=300pt]{img/machine-learning-quadratic-form-eigenvectors-2.png}

\begin{tabular}{ l | l | l }
  \textbf{positive definite}     & eigenvalues $> 0$    & $\x^\T\A\x > 0 ~~~~~~ \forall \x \neq \0$ \\
  \textbf{positive semidefinite} & eigenvalues $\geq 0$ & $\x^\T\A\x >= 0 ~~~~ \forall \x$ \\
  \textbf{indefinite}            & some positive and some negative eigenvalues & \\
  \textbf{singular}              & some zero eigenvalue \\
\end{tabular}
\end{minipage}

Let $\Lambda$ be a diagonal matrix containing the eigenvalues and $\V$ contain
normalized eigenvectors:
\begin{align*}
V = \begin{bmatrix}
\mid & \mid & & \mid \\
\v_1 & \v_2 & \cdots & \v_n\\
\mid & \mid & & \mid \\
\end{bmatrix}
\end{align*}

Note that for an \textbf{orthonormal} matrix like this:
\begin{enumerate}
\item It rotates / reflects the input vectors, without changing their length.
\item  $\V^T\V = \I$, therefore $\V^\1 = \V^\T$.
\end{enumerate}


By the definition of eigenvector we have
\begin{align*}
  \A\V = \V\Lambda
\end{align*}
and therefore the \textbf{eigendecomposition} of $\A$
\begin{align*}
  \A = \V\Lambda\V^\T.
\end{align*}

So we can perform $\A\x$ as $\V\Lambda\V^\T\x$, and $\A^k\x$ as
$\V\Lambda^k\V^\T\x$:

\begin{enumerate}
\item $\V^\T = \V^\1$ rotates the input vector into axis-aligned coordinates.
\item $\Lambda$ scales along different axes.
\item $\V$ returns to the original coordinates.
\end{enumerate}

$\Lambda$ is said to be the diagonalized version of $\A$.
\section{The Anisotropic Multivariate Normal Distribution, QDA, and LDA}

\newpage
\section{Regression}
\subsection{Linear Least Squares Regression}
Use fictitious dimension trick, so that $\w$ includes the offset term $\alpha$
and $\X$ is $(n \times (d + 1))$.
\\
\begin{mdframed}
Find $\w$ that minimizes cost function $J(w)$: sum of squared difference between
linear predictor and observed training point.
\begin{align*}
  J(w) = |\X \w - \y|^2 = \sum_i (\x_i^\T\w - y_i)^2
\end{align*}
\end{mdframed}

Solve by differentiating and finding the critical point:
\begin{align*}
  |\X \w - \y|^2          &= \w^\T\X^\T\X\w - 2\y^\T\X\w + \y^\T\y \\
  \grad_\w |\X \w - \y|^2 &= 2 \X^\T\X\w - 2\X^\T\y \\
  \w^*                    &= (\X^\T\X)^{-1}\X^\T\y =: \X^+\y
\end{align*}
\begin{mdframed}
For a new sample point $\x$, the prediction is $\hat y = \x \cdot \w^*$.
\end{mdframed}

\subsubsection{Related concepts}
\begin{itemize}
\item \textbf{normal equations}: linear system of $d$ equations in unknown $\w$ resulting from
  setting the gradient equal to zero: $\X^\T\X\w - \X^\T\y = \vec 0$
\item \textbf{pseudoinverse}: The matrix $\X^+ = (\X^\T\X)^{-1}\X^\T$ maps $\y$
  to $\w^*$. In general there's no $\w$ that solves $\X\w = \y$, but
  $\w^* = \X^+\y$ makes the LHS as close as possible to $\y$. So it behaves as
  a ``left inverse'' of $\X$, since $\X^+\X = \I$ and left-multiplying by $\X^+$
  gives the ``solution'' to $\X\w = \y$.
\item \textbf{projection matrix} or \textbf{hat matrix}: Still focusing on the
  training phase, the predictions are $\hat \y = \X\w^* = \X\X^+\y$. So
  $\X\X^+$ puts that hat on $\y$, or projects $\y$ onto the hyperplane, in the
  viewpoint described below.
\end{itemize}

\subsubsection{Projection interpretation}
Usually we think of $n$ points in $\R^d$. But instead, consider a separate
column of the data for each feature: these are $d$ points in $\R^n$. The
observed training data $\y$ is also a point in $\R^n$, and so is the prediction
$\hat \y = \X\w$.

As we vary $\w$, the prediction $\X\w$ describes a hyperplane spanned by the
columns of $\X$.

We want to find the $\w^*$ corresponding to the closest point on the hyperplane
to $\y$. So $X\w^* - \y$ must be orthogonal to the hyperplane:
\begin{align*}
  \X^\T \cdot (\X\w^* - \y) = \vec 0.
\end{align*}
Which are the normal equations (linear system of $d$ equations), derived
differently.

\subsubsection{Weighted linear regression}
Sample point $i$ has weight $b_i$. Diagonal $n \by n$ matrix $\mat B$ contains weights.
\begin{align*}
  J(\w) &= \sum_i b_i (\x_i^\T\w - y_i)^2 \\
        & = (\X\w - \y)^\T \B (\X\w - \y) \\
        &= \w^\T\X^\T \B \X\w - 2\y^\T \B \X\w + \y^\T\y
\end{align*}
Gradient
\begin{align*}
  \grad_\w J(\w) = 2\X^\T \B \X\w - 2\X^\T\B\y
\end{align*}
Solution
\begin{align*}
  \w^* = \(\X^\T \B \X\)^{-1}\X^\T\B\y
\end{align*}

\subsubsection{How to compute the gradient}

The cost function is $J(\w) = |\X\w - \y|^2$. We could write this as a dot product and
multiply out:
\begin{align*}
  J(\w) &= (\X\w - \y) \cdot (\X\w - \y) \\
  &= \X\w \cdot \X\w - 2 \X\w \cdot \y + \y \cdot \y \\
  &= (\X\w)^\T \X\w - 2 (\X\w)^\T \y + \y^\T \y \\
  &= \w^T\X^\T\X\w - 2 \w^\T\X^\T\y + \y^\T \y,
\end{align*}
and then we'd need to differentiate those terms w.r.t. $\w$. However, a better
way is to use the chain rule. Define $f$ and $g$ such that
$J: \R^d \rightarrow \R$ is their composition $J = g \circ f$:
\begin{align*}
  &f: \R^d \rightarrow \R^n     &f(\w) = \X\w - \y \\
  &g: \R^n \rightarrow \R       &g(\vec z) = |\vec z|^2.
\end{align*}
The chain rule says that $\grad (g \circ f) = (D f)^\T \grad g$, where $D f$ is
the derivative of $f$, i.e. the Jacobian matrix of first partial
derivatives\footnote{The gradient $\grad$ applies only to scalar-valued
functions.}. We have $D f(\w) = \X$ and $\grad g(\z) = 2\z$, so
\begin{align*}
  \grad J(\w)
  &= 2\X^\T(\X\w - \y) \\
  &= 2\X^\T\X\w - 2\X^\T\y.
\end{align*}

\subsection{Penalized Regression}
TODO

\subsection{Logistic Regression}

\begin{itemize}
\item Two classes.
\item The observations $y_i$ are class labels (or probabilities thereof).
\item The model states that the probability of being in class 1 is given by the
  usual linear model, mapped onto $(0, 1)$ by the logistic function $s$:
  $$y_i \sim \Bern(s(\x_i^\T\w)),$$
  $$s(z) = \frac{1}{1 + e^{-z}}$$
\end{itemize}
Note that $s'(z) = \frac{e^{-z}}{(1 + e^{-z})^2} = s(z)(1 - s(z))$.

\subsubsection{Likelihood}
Let $s_i = s(\x_i^\T\w)$.
\begin{align*}
  \L(\w)       &= \prod_i s_i^{y_i} \(1 - s_i\)^{(1 - y_i)} \\
  \l(\w)       &= \sum_i y_i \log s_i + (1 - y_i) \log\(1 - s_i\) \\
\grad \l(\w) &= \sum_i \frac{y_i}{s_i}(s_i)(1 - s_i)\x_i + \frac{1 - y_i}{1 - s_i}(-1)(s_i)(1 - s_i)\x_i \\
             &= \sum_i \x_i\(y_i(1 - s_i) - (1 - y_i)s_i\) \\
             &= \sum_i \x_i\(y_i - s_i\) \\
             &= \X^\T\(\y - \s(\X\w)\) ~~~~~~~~~ (d \by 1)
\end{align*}

where $\s: \R^n \to \R^n$ applies $s$ componentwise to the rows.

\textbf{Optimization problem}: Find $\w$ that minimizes the cost function
$J(\w) = -\l(\w)$.

Because the weights $\w$ are tied up inside $s_i = s(\x_i^\T\w)$ it's not
possible to find the minimum $\w^*$ by setting the gradient equal to zero
(i.e. by solving a linear system). We can use gradient descent, or Newton's
method.

For Newton's method, we need the Hessian of the objective function. This is the
$d \by \d$ matrix of partial derivatives of the gradient, i.e. $\X^\T$
multiplied by the derivative (Jacobian matrix) of $\s(\X\w)$. Define
$\f(\w) = \X\w$ so now $\s(\X\w) = (\s \circ \f)(\w)$.

\begin{tabular}{l | l | l | l}
  Function         & domain $\to$ range    & Jacobian                               & dim Jacobian \\
  \hline
  $\f(\w) = \X\w$  &$\R^d \rightarrow \R^n$ &$\D\f = \X$                            & $n \by d$\\
  $\s(\z)$         &$\R^n \rightarrow \R^n$ &$\D\s(\z) = \S$ & $n \by n$
\end{tabular}

where $\S$ is a diagonal matrix with
$\S_{ii} = \s(\x_i^\T\w)(1 - \s(\x_i^\T\w))$. Now by the chain rule,
\begin{align*}
%\D_\w \s(\X\w) &= \D_\w \s(\f(\w)) = ((\D_f \s) (\D_\w \f))(\w) = \s(\f(\w))(1 - \s(\f(\w)))^\T \X
\hess J(\w) &= \X^\T \D_\w \s(\X\w) \\
            &= \X^\T (\D_\f \s) (\D_w \f) \\
            &= \X^\T\S\X.
\end{align*}

\subsection{Simulating from linear regression models}

Each observation is $(\x, \y)$, where the input $\x$ is a list of $d$ feature values (dependent
variable) and $\y$ is the output (dependent variable).

(For convenience we append a 1 to the input data $\x$ to represent the intercept term.)

In classical regression analysis, we consider $\x$ to be fixed and learn a model $p(\y|\x)$. The
model is specified by parameters $\w \in \R^d$:

\begin{itemize}
\item Linear Regression: $\y|\x \sim \Norm(\x \cdot \w, \sigma^2)$, where $\sigma^2$ is a variance
  parameter.
\item Logistic Regression: $\y|\x \sim \Bern(f(\x \cdot \w))$, where $f: \R \to [0,1]$ is the logistic
  function.
\end{itemize}

Now consider both $\x$ and $\y$ to be random variables. Given $\x$ we can simulate $\y$ from the
above model.

How can we simulate $\x$ given $\y$?

If $\x$ were one-dimensional we...could simulate by picking a random uniform and inverting the CDF.

What's the equivalent of that for multidimensional $\x$?

\begin{align*}
  p(x|y) &= \frac{p(x, y)}{p(y)}
\end{align*}

% \section{11 More Regression; Newton’s Method; ROC Curves}
% \section{Decision Trees}
