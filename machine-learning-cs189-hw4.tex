\documentclass[section]{problemset}
\def\coursenumber{CS 189:289A}
\def\coursename{Introduction to \mbox{Machine Learning}}
\def\coursesemester{Spring 2017}

\usepackage{notes}
\usepackage[margin=1in]{geometry}

\usepackage{verbatim}
\usepackage{fancyhdr}
\usepackage{booktabs}
\usepackage{setspace}
\usepackage{amsmath,mathrsfs}
\usepackage{multicol}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{txfonts}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{array}
\usepackage{xcolor}
\usepackage{float}
\usepackage[shortlabels]{enumitem}
\usepackage{mathcomp}
\usepackage{tabularx}
\usepackage{wasysym}
\usepackage{pbox}
\usepackage[normalem]{ulem}
\usepackage{soul}
\usepackage{bbm}
\usepackage{tikz}
\usepackage{tkz-euclide}
\usepackage{mathtools}
\usepackage{fancyhdr}
\usepackage{mdframed}
\usepackage{minted}

\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}

\newcommand{\solcircle}[1]{
	\solution{\vspace{0.1in}}{#1}{
		\textcolor{red}{#1}
	}
} \newcommand{\myhline}{ \begin{center} \line(1,0){450}
\begin{center}
тв
\end{center}\end{center}
}

\newcommand{\mcqbubble}{\bigcirc}
\newcommand{\mcqbubblefill}{\Large\newmoon}

\newcommand{\mcqb}{$\bigcirc$\ \ }
\newcommand{\mcqs}{\solution{\vspace{0.1in}}{\mcqb}{$\Large\newmoon$\ \ }}

\newcommand{\easy}{(easy)}
\newcommand{\med}{(medium)}
\newcommand{\hard}{(hard)}
\newcommand{\remove}{\textbf{(remove)}}
\newcommand{\ambi}{(ambiguous)}

\newcommand{\bb}{{\mathbf{b}}}
\newcommand{\bx}{{\mathbf{x}}}
\newcommand{\bmu}{{\mathbf{\mu}}}
\newcommand{\bX}{{\mathbf{X}}}
%\newcommand{\by}{{\mathbf{y}}}
\newcommand{\bY}{{\mathbf{Y}}}
\newcommand{\bz}{{\mathbf{z}}}
\newcommand{\bw}{{\mathbf{w}}}

\newcommand{\auth}[1]{}
\newcommand{\com}[1]{(#1)}
\newcommand{\delete}[1]{}

\newcommand{\todo}[1]{\textcolor{blue}{#1}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\1{{\mathbf 1}}

\newcommand{\argmin}{\mathrm{argmin}}

%\newcommand{\B}{\textbf{B}}


\title{Homework 4: Regression}
\subtitle{\texttt{ddavison@berkeley.edu}}

\begin{document}

\newpage


\begin{problem}{Logistic Regression with Newton's Method}

Consider sample points $X_1, X_2, \ldots, X_n \in \mathbb{R}^d$ and
associated values $y_1, y_2, \ldots, y_n \in \{ 0, 1 \}$,
an $n \times d$ design matrix $X = [X_1~~~~\dots~~~~X_n]^{\T}$ and
an $n$-vector $y = [y_1~~~~\dots~~~~y_n]^{\T}$.

If we add $\ell_2$-regularization to logistic regression,
the cost function is
\[
J(w) = \lambda \, |w|^2_2 -
\sum_{i=1}^n \left(y_i \ln s_i + (1-y_i) \ln (1 - s_i) \rule{0pt}{18pt} \right)
\]
where $s_i = s(X_i \cdot w)$, $s(\gamma) = 1 / (1 + e^{- \gamma})$, and
$\lambda > 0$ is the regularization parameter.
As in lecture, the vector $s = [s_1~~~~\dots~~~~s_n]^{\T}$ is
a useful shorthand.

In this problem, you will use Newton's method to minimize this cost function
on the four-point, two dimensional training set
\[
X = \begin{bmatrix} 0 &3 \\ 1& 3\\ 0 &1\\ 1 &1 \end{bmatrix},
\hspace{1in}
y = \begin{bmatrix} 1\\1\\0\\0 \end{bmatrix}.
\]
You may want to draw these points on paper to see what they look like.
The $y$-vector implies that the first two sample points are in class $1$, and
the last two are in class $0$.

These sample points cannot be separated by a decision boundary that
passes through the origin.
As described in lecture, append a $1$ to each $X_i$ vector and
use a weight vector $w \in \mathbb{R}^3$ whose last component is
the bias term (the term we call $\alpha$ in lecture).

\begin{enumerate}
\item
Derive the gradient of the cost function $J(w)$.
Your answer should be a simple matrix-vector expression.
Do NOT write your answer in terms of the individual components of
the gradient vector.

\begin{comment}
\begin{mdframed}
\textbf{Initial verbose version}:

The gradient of the cost function is
\begin{align*}
  \nabla J(\w) = \lambda \nabla |\w|^2 - \sum_i
  y_i     \nabla \ln s_i(\w) +
  (1-y_i) \nabla \ln(1 - s_i(\w)).
\end{align*}
Let's compute the three derivatives separately.
\hlinee
First the regularization term:
\begin{align*}
  \nabla |\w|^2 = \nabla \sum_{j=1}^d w_j^2 = \cvecccc{2w_1}{\vdots}{2w_d}{2\alpha} = 2\w.
\end{align*}
\hlinee
Next $\ln s_i(\w)$. The partial derivative with respect to one component $w_j$ is
\begin{align*}
  \partiald{\ln s_i(\w)}{w_j}
  = \frac{1}{s_i(\w)} \partiald{s_i(\w)}{w_j}
\end{align*}
and the partial derivative of the logistic function is
\begin{align*}
  \partiald{s_i(\w)}{w_j} &= \partiald{~(1 + e^{-\w \cdot \x_i})^{-1}}{w_j} \\
  &= -(1 + e^{-\w \cdot \x_i})^{-2} (-x_{ij})e^{-\w\cdot\x_i} \\
  &= x_{ij}\frac{e^{-\w\cdot\x_i}}{(1 + e^{-\w \cdot \x_i})^2} \\
  &= x_{ij}s_i(\w)(1 - s_i(\w)),
\end{align*}
so putting the last two results together gives
\begin{align*}
  \partiald{\ln s_i(\w)}{w_j} = x_{ij}(1 - s_i(\w)),
\end{align*}
and therefore the gradient of $\ln s_i(\w)$ is
\begin{align*}
  \nabla \ln s_i(\w) =
\cvecccc
{\partiald{\ln s_i(\w)}{w_1}}
{\vdots}
{\partiald{\ln s_i(\w)}{w_d}}
{\partiald{\ln s_i(\w)}{\alpha}}
=
\cvecccc
{x_{i1}(1 - s_i(\w))}
{\vdots}
{x_{id}(1 - s_i(\w))}
{1 - s_i(\w)}
= (1 - s_i(\w))\x_i.
\end{align*}
\hlinee
Now $\ln(1 - s_i(\w))$. We've already computed $(s_i(\w))'$, so
\begin{align*}
  \partiald{\ln \(1 - s_i(\w)\)}{w_1}
  &= \frac{1}{1 - s_i(\w)}(-1)x_{ij}s_i(\w)(1 - s_i(\w)) \\
  &= -x_{ij}s_i(\w),
\end{align*}
and
\begin{align*}
  \nabla \ln (1 - s_i(\w)) =
\cvecccc
{-x_{i1}s_i(\w)}
{\vdots}
{-x_{id}s_i(\w)}
{-s_i(\w)} =
-s_i(\w)\x_i.
\end{align*}
\hlinee
Finally, putting the three derivatives together gives the gradient of the cost function:
\begin{align*}
  \nabla J(\w)
  &= \lambda \nabla |\w|^2 - \sum_i
  y_i     \nabla \ln s_i(\w) +
  (1-y_i) \nabla \ln(1 - s_i(\w)) \\
  &= 2\lambda\w - \sum_i y_i (1 - s_i(\w))\x_i - (1 - y_i) s_i(\w)\x_i \\
  &= 2\lambda\w - \sum_i y_i\x_i - s_i(\w)\x_i \\
  &= 2\lambda\w - \X^\T\y - \X^\T s(\X\w) \\
  &= 2\lambda\w - \X^\T(\y - s(\X\w)),
\end{align*}
where
$s(\X\w) = \cveccc{1/(1+e^{-\X_1\cdot\w})}{\vdots}{1/(1+e^{-\X_n\cdot\w})}$
contains the predicted values (class probability) for each sample point, given
parameters $\w$.
\end{mdframed}
\end{comment}

\begin{mdframed}
First note that
\begin{align*}
  \partiald{s_i(\w)}{w_j} &= \partiald{~(1 + e^{-\w \cdot \x_i})^{-1}}{w_j} \\
  &= -(1 + e^{-\w \cdot \x_i})^{-2} (-x_{ij})e^{-\w\cdot\x_i} \\
  &= x_{ij}\frac{e^{-\w\cdot\x_i}}{(1 + e^{-\w \cdot \x_i})^2} \\
  &= x_{ij}s_i(\w)(1 - s_i(\w)),
\end{align*}
and therefore that
\begin{align*}
  \nabla s_i(\w) = s_i(\w)(1 - s_i(\w))\X_i.
\end{align*}
Then the gradient of the cost function is
\begin{align*}
  \nabla J(\w)
  &= \lambda \nabla |\w|^2 - \sum_i
    y_i     \nabla \ln s_i(\w) +
    (1-y_i) \nabla \ln(1 - s_i(\w)) \\
  &= 2\lambda\w - \sum_i
    \frac{y_i}{s_i(\w)} \nabla s_i(\w) -
    \frac{1 - y_i}{1 - s_i(\w)} \nabla s_i(\w) \\
  &= 2\lambda\w - \sum_i \nabla s_i(\w) \(
    \frac{y_i - s_i(\w)}
         {s_i(\w)(1 - s_i(\w))}
\) \\
  &= 2\lambda\w - \sum_i \X_i\(y_i - s_i(\w)\) \\
  &= 2\lambda\w - \X^\T\(\y - s(\X\w)\),
\end{align*}
where
$s(\X\w) = \cveccc{1/(1+e^{-\X_1\cdot\w})}{\vdots}{1/(1+e^{-\X_n\cdot\w})}$
contains the predicted values (class probability) for each sample point, given
parameters $\w$.

We can interpret this expression a bit. $s(\X\w)$ is an $n$-vector containing
the predicted values for each sample point, so $\y - s(\X\w)$ is the error in
the current predicted values, and $\X^\T(\y - s(\X\w))$ is a $d$-vector whose
$j$-th component is large if feature $j$ is correlated with (has a large dot
product with) the current errors. So the steepest direction downhill will tend
to put more weight on features that are correlated with the current error in
the predictions.
\end{mdframed}
~\\~\\


\item
Derive the Hessian of $J(w)$.
Again, your answer should be a simple matrix-vector expression.

\begin{mdframed}
Using $\X_{.j}$ to denote the $n$-vector containing the $j$-th feature
values, the $j$-th component of the gradient is
\begin{align*}
  \partiald{J}{w_j} = 2\lambda w_j - \X_{.j}\cdot \y + \X_{.j} \cdot s(\X\w),
\end{align*}
so the $(j,k)$-th entry of the Hessian is
\begin{align*}
  \partiald{}{w_k}\partiald{J}{w_j}
  &= \sum_i x_{ij} \partiald{~s(\x_i\cdot \w)}{w_k} \\
  &= 2\lambda 1_{\{j = k\}} + \sum_i x_{ij} x_{ik} s_i(\w)(1 - s_i(\w)) \\
  &= \X^\T \B \X,
\end{align*}
where $\B$ is an $(n \times n)$ diagonal matrix with
$B_{ii} = s_i(\w)(1-s_i(\w) + 2\lambda$, and $1_{\{\cdot\}}$ is an indicator
function that takes the value 1 when its argument is true and 0 otherwise.
\end{mdframed}

\item State the update equation for one iteration of Newton's method for this
  problem.

\begin{mdframed}
The quadratic approximation to the cost function at $\v$ is
\begin{align*}
  q(\w)
  &= J(\v) + (\w - \v)^\T\(\grad J(\v)\) + \frac{1}{2}(\w - \v)^\T \(\hess J(\v)\) (\w - \v).
  % &= J(\w_0) + \\
  % &~~~~2\lambda(\w - \w_0)^\T \w_0 - \\
  % &~~~~(\w - \w_0)^\T\X^\T\(\y - s(\X\w_0)\) + \\
  % &~~~~\frac{1}{2}(\w - \w_0)^\T \X^\T \B|_{\w_0} \X (\w - \w_0) \\
\end{align*}
We want to find the $\w$ that minimizes this. The gradient of this is something
like
\begin{align*}
  \grad q(\w) = \grad J(\v) + \(\hess J(\v)\)\w,
\end{align*}
but that's not quite right. Anyway, from the lecture notes, setting the
gradient equal to zero gives
\begin{align*}
  \w = \v -\(\hess J(\v)\)^{-1} \grad J(\v).
\end{align*}
For our problem, this is (writing $\w^{(l)}$ instead of $\v$ for the value of $\w$ at iteration $l$.)
\begin{align*}
  \w^{(l+1)} = \w^{(l)} -\(\X^\T\B\X\)^{-1} \(2\lambda\w^{(l)} - \X^\T\(\y - s(\X\w^{(l)})\)\),
\end{align*}
where again $\B$ is an $(n \times n)$ diagonal matrix with
$B_{ii} = s_i(\w^{(l)})\(1-s_i(\w^{(l)})\) + 2\lambda$
\end{mdframed}


\item
We are given a regularization parameter of $\lambda = 0.07$ and
a starting point of $\w^{(0)} = \begin{bmatrix} -2 & 1 & 0 \end{bmatrix}^\T$.
\begin{mdframed}
  \begin{minted}{python3}
from numpy import array
from numpy import diag
from numpy import exp
from numpy.linalg import inv


def q1_4():
    X = array([[0, 3, 1],
               [1, 3, 1],
               [0, 1, 1],
               [1, 1, 1]])
    y = array([[1],
               [1],
               [0],
               [0]])
    lambda_ = 0.07

    w0 = array([[-2],
                [ 1],
                [ 0]])

    s0 = logistic(X @ w0)

    w1 = logistic_regression_newton_update(w0, X, y, lambda_)

    s1 = logistic(X @ w1)

    w2 = logistic_regression_newton_update(w1, X, y, lambda_)


def logistic_regression_newton_update(w, X, y, lambda_):
    s = logistic(X @ w)
    gradient = 2 * lambda_ * w - X.T @ (y - s)
    B = diag((s * (1 - s) + 2 * lambda_).ravel())
    hessian = X.T @ B @ X
    return w - inv(hessian) @ gradient


def logistic(z):
    return 1 / (1 + exp(-z))
  \end{minted}
\end{mdframed}

\begin{itemize}
\item[(a)]
State the value of $s^{(0)}$ (the value of $s$ before any iterations).

\begin{mdframed}
\begin{verbatim}
[[ 0.95257413]
 [ 0.73105858]
 [ 0.73105858]
 [ 0.26894142]]
\end{verbatim}
\end{mdframed}


\item[(b)]
State the value of $w^{(1)}$ (the value of $w$ after one iteration).
\begin{mdframed}
\begin{verbatim}
[[ 0.03660748]
 [ 1.77901816]
 [-3.1787346 ]]
\end{verbatim}
\end{mdframed}


\item[(c)]
State the value of $s^{(1)}$.
\begin{mdframed}
\begin{verbatim}
[[ 0.89644368]
 [ 0.89979306]
 [ 0.19786111]
 [ 0.20373548]]
\end{verbatim}
\end{mdframed}


\item[(d)]
State the value of $w^{(2)}$ (the value of $w$ after two iterations).
\begin{mdframed}
\begin{verbatim}
[[-0.84243273]
 [ 1.2968546 ]
 [-1.60471569]]
\end{verbatim}
\end{mdframed}

\end{itemize}
\end{enumerate}
\end{problem}


\newpage


\begin{problem}{$\ell_1$- and $\ell_2$-Regularization}


Consider sample points $X_1, X_2, \ldots, X_n \in \mathbb{R}^d$ and
associated values $y_1, y_2, \ldots, y_n \in \mathbb{R}$,
an $n \times d$ design matrix $X = [X_1~~~~\dots~~~~X_n]^{\T}$ and
an $n$-vector $y = [y_1~~~~\dots~~~~y_n]^{\T}$.
For the sake of simplicity, assume that the sample data
has been centered and whitened so that
each feature has mean $0$ and variance $1$ and the features are uncorrelated;
i.e., $X^\T X = nI$.
For this question, we will not use a fictitious dimension nor a bias term;
our linear regression function will be zero for $x = 0$.

Consider linear least-squares regression with
regularization in the $\ell_1$-norm, also known as Lasso.
The Lasso cost function is
\[
J(w) = |Xw - y|^2 +\lambda \, \|w\|_{\ell_1}
\]
where $w \in \mathbb{R}^d$ and $\lambda > 0$ is the regularization parameter.
Let $w^* = \argmin_{w \in \mathbb{R}^d} \, J(w)$ denote
the weights that minimize the cost function.

In the following steps, we will show that whitened training data decouples the features, so that $w^*_i$ is determined by the $i^\mathrm{th}$ feature alone (i.e., column $i$ of the design matrix $X$), regardless of the other features.  This is true for both Lasso and ridge regression.

\begin{enumerate}
\item
We use the notation $X_{*1}, X_{*2}, \ldots, X_{*d}$ to denote column $i$ of the design matrix $X$, which represents the $i^\mathrm{th}$ feature.
(Not to be confused with row $i$ of $X$, the sample point $X_i^\T$.)
Write $J(w)$ in the following form for appropriate functions $g$ and $f$.
\[
J(w) = g(y) + \sum_{i=1}^d f(X_{*i}, w_i, y, \lambda)
\]

\begin{mdframed}
  The cost function is
  \begin{align*}
    J(\w)
    =& |\X\w - \y|^2 + \lambda ||\w||_1 \\
    =& \w^\T\X^\T\X\w - 2y^\T\X\w + \y^\T\y + \lambda ||\w||_1 \\
    =& n\w^\T\w - 2y^\T\X\w + \y^\T\y + \lambda ||\w||_1 ~~~~~~~~~~~~~~~\text{(because $\X^\T\X=n\I$)}.\\
  \end{align*}
Now $\w^\T\w = \sum_{i=1}^d w_i^2$, and $||\w||_1 = \sum_{i=1}^d |w_i|$, and
\begin{align*}
  y^\T\X\w &= (\y^\T\X) \w = \sum_{i=1}^d \X_{*i}^\T ~\y w_i, \\
\end{align*}
so
\begin{align*}
  J(\w) &= g(\y) + \sum_{i=1}^d f(\X_{*i}, w_i, y, \lambda), ~~~~~~~~\text{where}\\
  g(\y) &= \y^\T\y ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\text{and}\\
  f(\X_{*i}, w_i, \y, \lambda) &= nw_i^2 + \lambda|w_i| - 2\X_{*i}^\T ~\y w_i.
%                              &= |w_i|\(n|w_i| + \lambda\) - 2\X_{*i}^\T ~\y w_i.
\end{align*}
\end{mdframed}

\item
If $w_i^* > 0$, what is the value of $w_i^*$?

\begin{mdframed}
For $w_i \geq 0$, the $i$-th component of $J(\w)$ is
\begin{align*}
  J(\w)_i = nw_i^2 + w_i (\lambda  - 2\X_{*i}^\T ~\y) + \constant.
\end{align*}
so
\begin{align*}
  \partiald{J}{w_i} = 2nw_i + \lambda - 2\X_{*i}^\T ~\y,
\end{align*}
and setting the gradient equal to zero gives
\begin{align*}
  w^*_i =
  \begin{cases}
    \frac{ 2\X_{*i}^\T ~\y - \lambda }{2n}, &\X_{*i}^\T ~\y  > \frac{\lambda}{2}\\
    0,                                     &\text{otherwise}.
  \end{cases}
\end{align*}
\end{mdframed}

\item
If $w_i^* < 0$, what is the value of $w_i^*$?

\begin{mdframed}
For $w_i \leq 0$, the $i$-th component of $J(\w)$ is
\begin{align*}
  J(\w)_i = nw_i^2 - w_i (\lambda  + 2\X_{*i}^\T ~\y) + \constant.
\end{align*}
so
\begin{align*}
  \partiald{J}{w_i} = 2nw_i - \lambda - 2\X_{*i}^\T ~\y,
\end{align*}
and setting the gradient equal to zero gives
\begin{align*}
  w^*_i =
  \begin{cases}
    \frac{ \lambda + 2\X_{*i}^\T ~\y }{2n}, &\X_{*i}^\T ~\y  < -\frac{\lambda}{2}\\
    0,                                     &\text{otherwise}.
  \end{cases}
\end{align*}
\end{mdframed}


\item
Considering parts 2 and 3, what is the condition for $w_i^*$ to be zero?
\begin{mdframed}
  $|\X_{*i}^\T ~\y| \leq \frac{\lambda}{2}$.
\end{mdframed}

\item
Now consider ridge regression, which uses the $\ell_2$ regularization term $\lambda \, |w|^2$.
How does this change the function $f(\cdot)$ from part 1?
What is the new condition in which $w_i^* = 0$?
How does it differ from the condition you obtained in part 4?

\begin{mdframed}
For ridge regression we have
\begin{align*}
  J(\w) &= g(\y) + \sum_{i=1}^d f(\X_{*i}, w_i, y, \lambda), ~~~~~~~~~~~\text{where}\\
  f(\X_{*i}, w_i, \y, \lambda) &= (n+\lambda)w_i^2 - 2\X_{*i}^\T ~\y w_i,
\end{align*}
and $g$ is as above. So
\begin{align*}
  \partiald{J}{w_i} = 2(n + \lambda)w_i - 2\X_{*i}^\T ~\y,
\end{align*}
and
\begin{align*}
  w^*_i = \frac{ \X_{*i}^\T ~\y }{n + \lambda}.
\end{align*}
So the weight for the $i$-th feature is zero if and only if
$\X_{*i}^\T ~\y = 0$, i.e. the $n$-vector containing the $i$-th feature is
orthogonal to the observed training values $\y$.

This is in contrast to Lasso, for which the $i$-th feature receives a weight of
zero if $|\X_{*i}^\T ~\y| \leq \frac{\lambda}{2}$, i.e. if the dot product of
the $i$-th feature with the training values $\y$ falls below $\lambda/2$.

This result is consistent with the general notion that Lasso tends to set some
weights to exactly zero whereas ridge regression would set them to a small but
usually non-zero value.
\end{mdframed}

\end{enumerate}
\end{problem}


\newpage


\begin{problem} {Regression and Dual Solutions}


\begin{enumerate}[a)]

\item
For a vector $w$, derive $\nabla \, |w|^4$.
Then derive $\nabla_w \, |Xw - y|^4$.

\begin{mdframed}
Suppose $\w \in \R^d$. Then $|\w|^4 \in \R$ is
\begin{align*}
  |\w|^4 = \(\sum_{j=1}^d w_j^2\)^2 = \sum_{j=1}^d \sum_{k=1}^d w_j^2w_k^2.
\end{align*}
Now consider the $j$-th component. Viewed as a function of $\w_j$, we have
\begin{align*}
  |\w|^4 &= w_j^4 + 2w_j^2\sum_{k \neq j} w_k^2 + \constant
\end{align*}
therefore
\begin{align*}
  \partiald{|\w|^4}{w_j}
  &= 4w_j^3 + 4w_j\sum_{k \neq j} w_k^2 \\
  &= 4|\w|^2w_j
\end{align*}
so
\begin{align*}
  \grad_\w |\w|^4 = 4|\w|^2\w.
\end{align*}
\end{mdframed}
~\\
\begin{mdframed}
Now let $|\X\w - \y|^4 = (g \circ f)(\w)$, where
\begin{align*}
  &f: \R^d \rightarrow \R^n     &f(\w) = \X\w - \y \\
  &g: \R^n \rightarrow \R       &g(\vec z) = |\vec z|^4.
\end{align*}
The chain rule states that $\grad (g \circ f) = (Df)^\T \grad g$, where $Df$
is the Jacobian matrix of first partial derivatives of $f$. We have
$\grad g(\z) = 4|\z|^2\z$ and $Df = \X$, so
\begin{align*}
  \grad_w |\X\w - \y|^4
  &= (Df)^\T \grad g \\
  &= 4|\X\w - \y|^2\X^\T (\X\w - \y) \\
  &= 4|\X\w - \y|^2\X^\T\X\w - \X^\T\y
\end{align*}


\end{mdframed}

\item
Consider sample points $X_1, X_2, \ldots, X_n \in \mathbb{R}^d$ and
associated values $y_1, y_2, \ldots, y_n \in \mathbb{R}$,
an $n \times d$ design matrix $X = [X_1~~~~\dots~~~~X_n]^{\T}$ and
an $n$-vector $y = [y_1~~~~\dots~~~~y_n]^{\T}$, and
the regularized regression problem
\[
w^* = \argmin_{w \in \R^d} \; |Xw - y|^4 + \lambda \, |w|^2,
\]
which is similar to ridge regression, but we take the fourth power
of the error instead of the squared error.
(It is not possible to write the optimal solution $w^*$ as
the solution of a system of linear equations, but
it can be found by gradient descent or Newton's method.)

Show that the optimum $w^*$ is unique.
By setting the gradient of the objective function to zero,
show that $w^*$ can be written as
a linear combination $w^* = \sum_{i=1}^n a_i X_i$ for some scalars
$a_1, \ldots, a_n$.
Write the vector $a$ of dual coefficients in terms of $X$, $y$, and
the optimal solution $w^*$.

\item Consider the regularized regression problem
\[
w^* = \argmin_{w \in \R^d} \; \frac{1}{n} \sum_{i=1}^n L(w^{\T} X_i, y_i) + \lambda \, |w|^2
\]
where the loss function $L$ is convex in its first argument.
Prove that the optimal solution has the form $w^* = \sum_{i=1}^n a_i X_i$.
If the loss function is not convex, does the optimal solution always have
the form $w^* = \sum_{i=1}^n a_i X_i$?
Justify your answer.
\end{enumerate}
\end{problem}


\newpage


\begin{problem}{Franzia Classification + Logistic Regression = Party!}

  Daylen is planning the frat party of the semester. He's completely stocked up
  on Franzia. Unfortunately, the labels for 497 boxes (test set) have been
  scratched off, and he needs to quickly find out which boxes contain Red wine
  (label 1) and White wine (label 0). Fortunately, for him the boxes still have
  their Nutrition Facts (features) intact and detail the chemical composition
  of the wine inside the boxes (the description of these features and the
  features themselves are provided in {\tt data.mat}). He also has 6,000 boxes
  with Nutrition Facts and labels intact (train set). Help Daylen figure out
  what the labels should be for the 497 mystery boxes.

\begin{enumerate}
\item Derive and write down the batch gradient descent update equation for
  logistic regression with $\ell_2$ regularization.

\begin{mdframed}
From Q1, the gradient of the cost function is
\begin{align*}
  \nabla J(\w) = 2\lambda\w - \X^\T\(\y - s(\X\w)\),
\end{align*}
where
$s(\X\w) = \cveccc{1/(1+e^{-\X_1\cdot\w})}{\vdots}{1/(1+e^{-\X_n\cdot\w})}$
contains the predicted values (class probability) for each sample point, given
parameters $\w$.

Therefore the batch gradient descent update equation with learning rate $\epsilon$ is
\begin{align*}
  \w^{(i+1)} = \w^{(i)} - \epsilon\(2\lambda\w^{(i)} - \X^\T\(\y - s(\X\w^{(i)})\)\).
\end{align*}

\end{mdframed}


  Choose a reasonable regularization parameter value and a reasonable learning
  rate.  Run your algorithm and plot the cost function as a function of the
  number of iterations.  (As this is batch descent, one ``iteration'' should
  use every sample point once.)

\item Derive and write down the stochastic gradient descent update equation for
  logistic regression with $\ell_2$ regularization.  Choose a suitable learning
  rate.  Run your algorithm and plot the cost function as a function of the
  number of iterations---where now each ``iteration'' uses {\em just one}
  sample point.

  Comment on the differences between the convergence of batch and stochastic
  gradient descent.

\item Instead of a constant learning rate $\epsilon$, repeat part 2 where the
  learning rate decreases as $\epsilon \propto 1/t$ for the $t^\mathrm{th}$
  iteration. Plot the cost function vs.\ the number of iterations. Is this
  strategy better than having a constant $\epsilon$?

\item Finally, train your classifier on the entire training set. Submit your
  predictions for the test set to Kaggle. You can only submit twice per day, so
  get started early! In your writeup, include your Kaggle display name and
  score and describe the process you used to decide which parameters to use for
  your best classifier.

\end{enumerate}

\end{problem}


\newpage


\begin{problem}{Real World Spam Classification}


\textbf{Motivation}: After taking CS 189 or CS 289A, students should be able to wrestle with ``real-world'' data and problems. These issues might be deeply technical and require a theoretical background, or might demand specific domain knowledge. Here is an example that a past TA encountered.

Daniel (a past CS 189 TA) interned as an anti-spam product manager for an email service provider. His company uses a linear SVM to predict whether an incoming spam message is spam or ham. He notices that the number of spam messages received tends to spike upwards a few minutes before and after midnight. Eager to obtain a return offer, he adds the timestamp of the received message, stored as number of milliseconds since the previous midnight, to each feature vector for the SVM to train on, in hopes that the ML model will identify the abnormal spike in spam volume at night. To his dismay, after testing with the new feature, Daniel discovers that the linear SVM's success rate barely improves.

Why can't the linear SVM utilize the new feature well, and what can Daniel do to improve his results? Daniel is unfortunately limited to a quadratic kernel i.e. the features are at most polynomials of degree 2 over the original variables. This is an actual interview question Daniel received for a machine learning engineering position!

Write a short explanation. This question is open ended, and there can be many correct answers.

\end{problem}


\newpage


\section*{Submission Instructions}
Please submit
\begin{itemize}
\item a PDF write-up containing your \textit{answers, plots, and code} to Gradescope. Be sure to include your Kaggle display name and score in the PDF.
\item a .zip file of your \textit{code} and a README explaining how to run your code to Gradescope.
\item your CSV file of predictions to Kaggle.
\end{itemize}


\end{document}
