\section{Introduction to Probabilities, Graphs, and Causal Models}

\begin{enumerate}
\item Bayesian interpretation of probability
\item Probabilities are degrees of belief
\item Beliefs are updated in response to observing data
\item E.g. $\p(H|e)$ is belief about hypothesis given evidence
  Alternatively: within the realities in which $E=e$, what is the distribution of $H$?
\item E.g. $\p(Y|x)$ is belief about outcome variable given treatment observed to be $X=x$
  Alternatively: within the cases in which $X=x$, what is the distribution of $Y$?
\item {\it independence}: $\forall x$ we have $\p(Y|x) = \p(Y)$
\end{enumerate}

Conditional probabilities are primitive; they are not defined in terms of joint probabilities. Instead
\begin{align*}
  \p(x, y) = \p(x)\p(y|x).
\end{align*}
Thus the law of total probability $\p(X) = \sum_Z\p(X, Z)$ gives rise to
\begin{align*}
  \p(y|x) = \sum_z \p(z)\p(y|x,z).
\end{align*}
This theorem lies behind the notion of ``controlling'' for a covariate $Z$ when studying the effect of $X$ on $Y$.

\subsection{Bayes' rule}

Since conditional probabilities are primitive, Bayes' rule
\begin{align*}
  \p(H|e) = \p(H) \p(e|H) \times \p(e)^{-1}
\end{align*}
is a ``normative rule for updating beliefs in response to evidence'', as opposed to a tautology (theorem) derivable from definitions of conditional probability.

\begin{quote}
  {\it Accordingly, (1.14) [Bayes' rule] is not a definition but rather an empirically verifiable relationship
    between English expressions.}
\end{quote}
\begin{question}
  What does that mean?
\end{question}

\begin{enumerate}
\item Belief in $h$ after observing $e$ increases (relative to $\p(h,e)$) in proportion to the degree of surprise of observing $e$.
\item Belief in $h$ after observing $e$ is never lower than prior belief in $(h, e)$.
\end{enumerate}

\begin{intuition}
  We're imagining possible pairs $(H, E)$ of explanation (hypothesis) and data (evidence) and have formed prior beliefs about their joint value. Then data $e$ becomes known. Belief about $h$ is now equal to $\p(h,e)$ multiplied by the surprise $\p(e)^{-1}$ of observing $e$.

  I'm not sure that's an interesting way of describing it really. I think I'd describe it as:
  \begin{enumerate}
  \item We have formed prior beliefs about $(H, E)$ pairs.
  \item We observe data.
  \item This causes all but one column of the joint distribution to be set to zero.
  \item To view the result as a probability distribution, we have to rescale (and drop a now-pointless dimension).
  \end{enumerate}
\end{intuition}


\subsection{Probability models, Boolean logic}

Given atomic propositions $A, B, C, \ldots$, a \defn{elementary event} is a conjunction involving the joint values of all of them, i.e. a sentence in which each proposition, or its negation, occurs exactly once, e.g.
\begin{align*}
  S = (A \land B) \lor \lnot C.
\end{align*}

\begin{theorem*}
  Elementary events are mutually exclusive and every boolean formula can be expressed as a disjunction of elementary events.
\end{theorem*}

The {\it sample space} of probability textbooks is (for discrete RVs) equivalent to the set of {\it elementary events} in propositional logic.

A \defn{probability model} gives the probability of every well-formed sentence. A joint probability distribution is a probability model.

\begin{quote}
  {\it In practice, however, joint distribution functions are rarely specified explicitly. In
    the analysis of continuous random variables, the distribution functions are given by
    algebraic expressions such as those describing normal or exponential distributions; for
    discrete variables, indirect representation methods have been developed where the overall
    distribution is inferred from local relationships among small groups of variables.
    Graphical models, the most popular of these representations, provide the basis of
    discussion throughout this book.}
\end{quote}

\begin{question}
  Is that saying graphical models apply only to discrete RVs??
\end{question}



\subsection{Odds, likelihood ratios}

The \defn{odds} of $H$ are a measure of strength of belief in $H$:
\begin{align*}
  O(H) = \frac{\p(H)}{\p(\lnot H)} = \frac{\p(H)}{1 - \p(H)}.
\end{align*}
Bayes' rule says that
\begin{align*}
 \p(H|e) = \p(H) \p(e|H) \times \p(e)^{-1},
\end{align*}
therefore the posterior strength of belief in $H$ is equal to the prior strength of belief, multiplied by the likelihood ratio:
\begin{align*}
  O(H|e) = O(H) \times \frac{\p(e|H)}{~\p(e|\lnot H)}.
\end{align*}

\subsection{Expected values}

\begin{tabular}{l l l}
Expectation                  & $\E[f(x)] = \sum_x f(x) \p(x)$                                                & centroid \\
Variance                       & $\Var[Z] = \sigma^2_Z = \E\Big[(Z - \E[Z])^2\Big]$                     & average distance (squared) from center in 1D \\
Covariance                    & $\Cov[X, Y] = \sigma_{XY} = \E\Big[(X - \E[X])(Y - \E[Y])\Big]$   & average size of squares from center in 2D \\
Correlation coefficient     & $\rho_{XY} = \sigma_{XY} / \sigma_X\sigma_Y$                             & average size of squares from center in 2D, scaled by both variances\\
Regression coefficient      & $r_{XY} = \sigma_{XY} / \sigma^2_{Y}$                                      & average size of squares from center in 2D, scaled by one variance
\end{tabular}

\subsection{Conditional independence and graphoids}

Now let $X, Y, Z$ be {\it subsets} of random variables.

We write $X \indep Y|Z$ to mean that the subset $X$ is independent of $Y$ given $Z$, i.e. that
\begin{align*}
  \p(x|y, z) = \p(x | z)
\end{align*}
for all tuples $x$, $y$, $z$. This implies the conditional independence of all pairs of component variables, but the converse is not necessarily true. $Z$ may be empty in which case the statement is that $X \indep Y$.


\section{Bayesian Networks}


Consider some collection of random variables $X_1, \ldots, X_n$. We can think of them as vertices of a graph:

\begin{align*}
\begin{tikzpicture}
  \graph[math nodes, spring layout, edges=white]{X_1 -> {X_2, X_3, X_4}, X_2 -> {X_3, X_4}, X_3 -> {X_4}, X_4};
\end{tikzpicture}
\end{align*}


If we know some of the conditional distributions, then we can pick an ordering and factorise the joint
distribution:
\begin{align*}
  \p(X_1, X_2, X_3, X_4) = \p(X_1) \p(X_2|X_1) \p(X_3|X_2, X_1) \p(X_4|X_3, X_2, X_1).
\end{align*}

When $X_1$ and $X_2$ are not independent, we draw a directed edge from $X_1$ to $X_2$ to refer to the conditional probability distribution $\p(X_2|X_1)$. The graph corresponding to the above factorisation is
\begin{align*}
\begin{tikzpicture}
  \graph[math nodes, spring layout]{X_1 -> {X_2, X_3, X_4}, X_2 -> {X_3, X_4}, X_3 -> {X_4}, X_4};
\end{tikzpicture}
\end{align*}

That graph is complete (contains every possible edge). However, if some variables are conditionally independent of others, then we can get rid of some of the edges.

{\bf Example}: will my team win this football match?
Let:
\begin{itemize}[label=$\circ$]
\item $X_1 = ~$ is our best player playing?
\item $X_2 = ~$ are we playing at home?
\item $X_3 = ~$ number of goals each teams scores in the match
\item $X_4 = ~$ do we win the match?
\end{itemize}

Notice that there is now some conditional independence:
\begin{itemize}[label=$\circ$]
\item The number of goals scored by both teams is influenced by whether our best player is playing and whether it's a home game.
\item For the sake of argument, we can say that whether we're playing at home doesn't influence whether our best player is playing.
\item Whether we win the match is determined by the number of goals scored: if we know the number of goals scored then knowing whether our best player was playing, or whether it was a home game, doesn't provide any further information about whether we won the match.
\end{itemize}
The conditional independence simplifies the graph:

\begin{tikzpicture}
  \graph[math nodes, spring layout]{X_1 -> {X_3}, X_2 -> {X_3}, X_3 -> {X_4}, X_4};
\end{tikzpicture}

So what is this network diagram really? It represents the joint distribution. In other words, it specifies a probability distribution which spits out tuples $(X_1, X_2, X_3, X_4)$. The first few tuples generated from such a probability distribution might be

\begin{verbatim}
(Yes, No,  1-2, Yes)
(No,  No,  1-1, No)
(Yes, Yes, 2-1, Yes)
        ...
\end{verbatim}

Each time a random value is generated from the probability distribution, we can imagine the values of the tuple appearing at the nodes of the graph:

\begin{tikzpicture}
  \graph[math nodes, spring layout]{"X_1 (Yes)" -> {"X_3 (1-2)"}, "X_2 (No)" -> {"X_3 (1-2)"}, "X_3 (1-2)" -> {"X_4 (Yes)"}, "X_4 (Yes)"};
\end{tikzpicture}