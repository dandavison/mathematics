\section{Billingsley Section 1}
[Berkeley 202a]
[Billingsley - Probability \& Measure]

Why does he say ``closed under countable unions and intersections.​''?

  Billingsley p.19:
  \begin{quote}
    ...require a collection that contains the intervals and is closed under countable unions and intersections.
    Note that a singleton $\{x\}$ is a countable intersection of intervals:
    \begin{align*}
      \bigcap_{n=1}^\infty \Big(x -\frac{1}{n}, x\Big] = \{x\}.
    \end{align*}
  \end{quote}


\begin{itemize}
\item $\Omega = [0, 1]$
\item $\om \in \Omega$
\item $d_n(\om) \in \{0, 1\} = $ $n$-th digit in binary expansion of $\om$
\item Rademacher function $r_n(\om) = 2d_n(\om) - 1 \in \{-1, 1\}$
\end{itemize}

\subsection{Weak Law of Large Numbers}

Define the partial sum $s_n(\om) = \sum_{i=1}^n r_i(\om)$, i.e. the number of $1$s minus the number of $0$s in
the first $n$ digits of the binary expansion of $\om$. (The displacement of the random walk after $n$ steps.)

\begin{lemma}
  \begin{align*}
    \int_0^1 s_n(\om)^2 \d\om = n
  \end{align*}
\end{lemma}

I.e., viewed as a sequence of $n$ coin tosses yielding $-1$ or $+1$, the variance (expected squared distance
from mean) of their sum is $n$.

\begin{proof}
  Note that $s_n(\om)^2 = \sum_{i=1}^n r_i(\om)^2 - 2\sum_{i<j}r_i(\om)r_j(\om)$. Integrating over $[0, 1]$ we have
  \begin{align*}
    \int_0^1 s_n(\om)^2 \d\om
    &= \sum_{i=1}^n \int_0^1 r_i(\om)^2 \d\om - 2\sum_{i<j}\int_0^1r_i(\om)r_j(\om) \d\om \\
    &= \sum_{i=1}^n \int_0^1 1 \d\om - 0 \\
    &= n.
  \end{align*}
  We used there the fact that $\int_0^1r_i(\om)r_j(\om) \d\om = 0$ for $i < j$, i.e that the Rademacher
  functions are orthogonal. An argument for this is that as we move through a rank $i$ dyadic
  interval, $r_i(\omega)$ is constant (either $-1$ or $+1$) while at rank $j$ below, $r_j(\omega)$ flickers
  between $-1$ and $+1$, spending an equal amount of time in each.
\end{proof}


\begin{lemma}[Markov's Inequality]
  Let $f: [0, 1] \to \R^+$ be a step function. Then
  \begin{align*}
    P\Big(\Big\{x: f(x) \geq \alpha\Big\}\Big) \leq \frac{1}{\alpha}\int_0^1 f(x) \dx.
  \end{align*}
\end{lemma}




\begin{intuition}
  Think of the statement in rearranged form:
  \begin{align*}
    \alpha P\Big(\Big\{x: f(x) \geq \alpha\Big\}\Big) \leq \int_0^1 f(x) \dx.
  \end{align*}



  \includegraphics[width=400pt]{img/analysis--real-analysis--measure-theory--weak-law-of-large-numbers-c9c2.png}

  If $X \sim \Unif(0, 1)$ then the RHS is $\E[X]$.

\end{intuition}


\begin{proof}
  [me]

  Clearly
  \begin{align*}
    \int_{f(x) \geq \alpha} f \leq \int_{[0, 1]} f.
  \end{align*}
  Therefore
  \begin{align*}
    \int_{f(x) \geq \alpha} \alpha \leq \int_{[0, 1]} f
  \end{align*},
  or equivalently
\begin{align*}
  \alpha \int \textbf{1}_{f(x) \geq \alpha} \leq \int_{[0, 1]} f,
\end{align*}
which is the same thing as
  \begin{align*}
    \alpha P\Big(\Big\{x: f(x) \geq \alpha\Big\}\Big) < \int_0^1 f(x) \dx.
  \end{align*}
\end{proof}

\begin{theorem}[Weak Law of Large Numbers]
  Fix an $\epsilon > 0$. Then
  \begin{align*}
    \lim_{n \to \infty}P\Big(\Big\{\om: \frac{1}{n}\big|\sum_{i=1}^n r_i(\om)\big| \geq \epsilon\Big\}\Big) = 0.
  \end{align*}
\end{theorem}

In other words: we move through all the $\om \in [0, 1]$. For a given $\om \in [0, 1]$, compare the number
of $0$s and $1$s in the first $n$ digits of the binary expansion, and record the excess as a proportion of $n$;
this is $\frac{1}{n}|s_n(\om)|$. The theorem states that for all $\epsilon > 0$ the probability measure
associated with the set of $\om$s for which $\frac{1}{n}|s_n(\om)| > \epsilon$ goes to $0$ as $n \to \infty$.

\begin{proof}
  Fix an $\epsilon > 0$. We square both sides of the inequality, instead of working with the absolute value. So
  what we want to show is that $P\big(\big\{\om: s_n^2(\om) \geq n^2\epsilon^2\big\}\big) \to 0$
  as $n \to \infty$.

  It would be nice to find an expression for this probability measure as a function of $n$. However, what we'll
  do is find an upper bound: that will suffice also.

  Note that $s_n(\om)$ is a step function (and so $s_n^2(\om)$ is also):

\includegraphics[width=200pt]{img/analysis--real-analysis--measure-theory--weak-law-of-large-numbers-c049.png}


By Markov's inequality / ``Shaded Area lemma​'' we have
\begin{align*}
  n^2\epsilon^2P\Big(\Big\{\om: s_n^2(\om) \geq n^2\epsilon^2\Big\}\Big) \leq \int_0^1 s_n^2(\om) \d\om = n
\end{align*}
and therefore
\begin{align*}
  P\Big(\Big\{\om: s_n^2(\om) \geq n^2\epsilon^2\Big\}\Big) \leq  \frac{1}{n\epsilon^2},
\end{align*}
which proves the desired result since it shows that the probability measure is bounded above by a quantity that
goes to $0$ as $n \to \infty$.
\end{proof}

\subsection{Strong Law of Large Numbers}

\red{TODO} Relation of Borel's normal number theorem to SLNN.

\begin{definition}[negligible, null set]
  A set $A$ is \defn{negligible} if, for any $\epsilon > 0$, it can be covered by a finite or countable
  union $\bigcup_k I_k$ of intervals with $\sum_k |I_k| < \epsilon$.
\end{definition}

Recall the weak law of large numbers:
  \begin{align*}
    \lim_{n \to \infty}P\Big(\Big\{\om: \frac{1}{n}\big|s_n(\om)\big| \geq \epsilon\Big\}\Big) = 0.
  \end{align*}

\begin{definition}[Normal numbers]
  Define the set of \defn{normal numbers} to be
  \begin{align*}
    N = \Big\{\om ~:~ \lim_{n\to\infty} \frac{1}{n}s_n(\om) = 0 \Big\}.
  \end{align*}
\end{definition}

\begin{theorem*}[Borel's normal number theorem]
  $N^c = \R \setminus N$ is negligible.
\end{theorem*}

\begin{intuition}
  Note that the set of normal $\om$ can be written as the set of $\om$ that

  ``eventually stay within $1$​'' AND
  ``eventually stay within $1/2$'' AND
  ``eventually stay within $1/3$'' AND
  ``eventually stay within $1/4$'' ...
  \begin{align*}
    N = \bigcap_{k=1}^\infty \bigcup_{m=1}^\infty\bigcap_{n \geq m} \big\{ \om ~:~ \big|\frac{1}{n}S_n(\om)\big| < \frac{1}{k} \big\}.
  \end{align*}

  Visualize the $s_n$ sequence of a non-normal number $\om$, stretching off to infinity. However far we’ve
  gone, there will always be another point further along at which an excursion of the random walk sticks out
  further than $\eps$. But despite the fact that this must always happen, it’s less and less likely the further
  we go. The fact that it must always happen again corresponds to the fact that we can write the event as a
  countable union: (happened by this generation) union with (happened at the next generation), etc. But at the
  same time, since it’s getting harder and harder, for any given $\gamma >0$ we can find some generation $m$
  beyond which the union sums to less than $\gamma$. nevertheless , the event is equal to the union beyond that
  point, since the departures must always keep occurring (otherwise the number would be normal). So the union
  doesn’t have to include earlier generations.

  This is why the complement of the normal numbers is negligible. Perhaps it’s typical of negligible sets that
  they correspond to an event that must always occur one more time, and yet get ever less and less likely?
\end{intuition}

\begin{proof}
  Let $(\eps_n)$ be a sequence that converges to zero, and define a sequence of sets $(A_n)$, where
  \begin{align*}
    A_n = \Big\{\om : \Big|\frac{1}{n} s_n(\om)\Big| \geq \eps_n\Big\}.
  \end{align*}
  (We can think of $A_n$ as the set of $\om$ whose binary expansions are ``not normal so far​''.)

  Note that, for any given $m$, we have the following: a number that stays inside $\eps_n$ for ever is normal:
  \begin{align*}
    \Big(\bigcap_{n=m}^\infty A_n^c\Big) \subset N.
  \end{align*}
  Equivalently, a non-normal number must stray outside $\eps_n$ at some point:
  \begin{align*}
    N^c \subset \Big(\bigcup_{n=m}^\infty A_n\Big).
  \end{align*}
  Recall that our aim is to cover $N^c$ with a countable union of intervals, where the total length of the
  intervals is arbitrarily small (an ``efficent covering​​''). If we can show that the $A_n$ meet that
  description then we are done.

  Recall that $s_n$ is a step function such that, if $\om \in A_n$ then $\om' \in A_n$ for every $\om'$ in the
  same rank-$n$ dyadic interval as $\om$. Therefore each set $A_n$ is a finite disjoint
  union $\bigcup_{k}I_{nk}$ of intervals, and $P(A_n) = \sum_k |I_{nk}|$.

  So what we need to do is show that, for any given $\gamma > 0$, there exists a sequence $(\eps_n)$ converging
  to zero, and an $m$, such that $\sum_{n=m}^\infty P(A_n) < \gamma$.

  At this point, we need to find an expression for an upper bound on $P(A_n)$ in terms of $n$ and $\eps_n$.
  From the lemma, we have
  \begin{align*}
    P(A_n) \leq \frac{3}{n^2\eps_n^4},
  \end{align*}
  so we would like to find $(\eps_n)$ and $m$ such that
  \begin{align*}
    \sum_{n=m}^\infty \frac{3}{n^2\eps_n^4} < \gamma.
  \end{align*}
  To do so, we need only choose $(\eps_n)$ so that the series $\sum_nn^{-2}\eps_n^{-4}$
  converges: $\eps_n = n^{-1/8}$ will do. Then, since the series converges, there exists an $m$ such that the
  tail sums to less than $\gamma$, as required.
\end{proof}

\begin{lemma}
  Let $A_n = \Big\{\om : \Big|\frac{1}{n} s_n(\om)\Big| \geq \eps\Big\}$.

  For all $n \in \N$, we have (by taking the 4th power of both sides of the inequality and applying Markov's
  inequality)
  \begin{align*}
    P(A_n) \leq \frac{1}{n^4\eps^4} \int_0^1 s_n^4(\om) \d\om,
  \end{align*}
  and (by considering integrals of products of four Rademacher functions)
  \begin{align*}
    \int_0^1 s_n^4(\om) \d\om \leq 3n^2.
  \end{align*}
  Therefore
  \begin{align*}
    P(A_n) \leq \frac{3}{n^2\eps^4}.
  \end{align*}
\end{lemma}

\newpage
\subsection{An interval of  positive length is not negligible}

\begin{definition*}[length of an interval]
  The \defn{length} of $(a, b)$ is $|(a, b)| = b - a$.
\end{definition*}

\begin{theorem*}
  Let $I$ be an interval of positive length and let $I_1, I_2, \ldots$ be intervals.
  \begin{enumerate}
  \item If $\bigcup_k I_k \subseteq I$ (disjoint) then $\sum_k |I_k| \leq |I|$
  \item If $\bigcup_k I_k \supseteq I$ then $\sum_k |I_k| \geq |I|$. I.e. no cover of $I$ is negligible.
  \end{enumerate}
  A corollary is that if $\bigcup_k I_k = I$ then $\sum_k |I_k| = |I|$.
\end{theorem*}

\begin{proof}
  Let $I = (a, b)$ and let $I_k = (a_k, b_k)$ for all $k$.

  First, we show that if $\bigcup_k I_k \subseteq I$ (with the $I_k$ disjoint) then $\sum_k |I_k| \leq |I|$.

  There are two cases:
  \begin{enumerate}
  \item {\bf Finite cover}:

    The claim is that for any collection of $n$ disjoint intervals, if $\bigcup_{k=1}^n I_k \subseteq I$
    then $\sum_{k=1}^n |I_k| \leq |I|$.

    This is clearly true for a collection of intervals of size $n = 1$.

    Assume it's true for any collection of intervals of size $n-1$, and consider a collection of $n$ disjoint
    intervals with $\bigcup_{k=1}^n I_k \subseteq I$.

    Label the intervals $I_1, \ldots, I_n$, sorted by their left endpoint in ascending order. Note that the
    union of the first $n-1$ intervals is contained within $(a, a_n)$ and that the $n$-th interval has
    length $b_n - a_n \leq b - a_n$. Thus we have
    \begin{align*}
      \sum_{k=1}^n |I_k|
      &= \sum_{k=1}^{n-1}|I_k| + |I_n| \\
      &\leq (a_n - a) + (b - a_n) \\
      &= b - a.
    \end{align*}
    Therefore it is true for all $n$ by induction.

  \item {\bf Infinite cover}:

    The claim is that for any countably infinite collection of $n$ disjoint intervals,
    if $\bigcup_{k=1}^\infty I_k \subseteq I$ then $\sum_{k=1}^\infty |I_k| \leq |I|$.

    Consider an infinite collection of intervals satisfying $\bigcup_{k=1}^\infty I_k \subseteq I$.

    Note that for every finite subcollection of size $n$ we have $\sum_{k=1}^n |I_k| \leq |I|$ by the finite case.

    Therefore $\sum_{k=1}^\infty |I_k| = \sup \sum_{k=1}^n |I_k| \leq |I|$ where the supremum is over the set
    of all finite subcollections. Since this set is non-empty, the supremum is a finite positive number.
  \end{enumerate}
  ~\\~\\
  Finally, we show that if $\bigcup_k I_k \supseteq I$ then $\sum_k |I_k| \geq |I|$.
  Again, there are two cases:
  \begin{enumerate}
  \item {\bf Finite cover}:

    The claim is that for any collection of $n$ intervals, if $\bigcup_{k=1}^n I_k \supseteq I$
    then $\sum_{k=1}^n |I_k| \geq |I|$.

    In other words, that the total length of a finite cover of $I$ is bounded below by $|I| > 0$.

    Again, it's obvious for a cover comprising a single interval ($n=1$).

    Assume it's true for any cover comprising $n-1$ intervals, and consider a cover comprising $n$ intervals.

    Again, label the intervals $I_1, \ldots, I_n$, sorted by their left endpoint in ascending order.

    Note that the first $n-1$ intervals cover the interval $(a, b_{n-1})$ and that $|I_n| \geq b - b_{n-1}$.
    Thus we have
    \begin{align*}
      \sum_{k=1}^n |I_k|
      &= \sum_{k=1}^{n-1}|I_k| + |I_n| \\
      &\geq (b_{n-1} - a) + (b - b_{n-1}) \\
      &= b - a.
    \end{align*}

  \item {\bf Infinite cover}\\

    The claim is that for any infinite cover $\bigcup_{k=1}^\infty I_k \supseteq I$ we have $\sum_{k=1}^\infty |I_k| \geq |I|$.

    Consider a countably infinite cover of $I$.

    We might think that we could make an argument analogous to the one above:

    Note that for every finite subcover of size $n$ we have $\sum_{k=1}^n |I_k| \geq |I|$.

    Therefore $\sum_{k=1}^\infty |I_k| = \inf \sum_{k=1}^n |I_k|  \geq |I|$ where the infimum is over the set of all finite subcovers.

    However, we have to show that a finite subcover exists; otherwise the infimum would be $+\infty$.

    So what we do is construct a closed interval $[a + \eps, b]$ which is covered by a countably infinite open
    cover. Since that closed interval is compact from Heine-Borel, there exists a finite open subcover.

    \begin{quote}
      I think your second statement is true except for an infinite open cover of (a, b] which has finite total
      measure, but no finite subcovers. Such a cover does exist, and it’s the the object I was (clumsily)
      trying to refer to. So I think your second statement is false, but not nonsense, as it fails for the same
      reason that the second proof is more difficult.
    \end{quote}
  \end{enumerate}
\end{proof}


\begin{theorem}
  Every proper open subset of $\R$ is a countable union of disjoint open intervals and open rays.
\end{theorem}
\begin{proof}
  HW2 Q1 (uses an equivalence relation to partition the open set), Billingsley Example 2.6 (uses an
  uncountable union of intervals with rational endpoints which must contain duplicates).
\end{proof}


\begin{proof}
  Let $\ms A$ be a finite class of $n$ sets. Taking complements gives $2n$ sets.

  In the first iteration, each set can be involved in $2n$ unions and $2n$ intersections, for a total of $4n$
  new sets, which becomes $8n$ on taking complements.

  So after $k$ generations, we


\end{proof}
\subsection{Cantor sets}

\subsubsection{Middle-thirds Cantor set}

\begin{definition*}
We start at generation $0$ with
\begin{align*}
  C_0 = \Big[0, 1\Big].
\end{align*}

At generation $1$ we have removed the middle-third open interval $(\frac{1}{3}, \frac{2}{3})$,
leaving
\begin{align*}
  C_1 = \Big[0, \frac{1}{3}\Big] \cup \Big[\frac{2}{3}, 1\Big].
\end{align*}

At generation $2$ we have removed the middle-third open interval from each remaining interval, leaving
\begin{align*}
  C_2 = \Big[0, \frac{1}{9}\Big] \cup \Big[\frac{2}{9}, \frac{3}{9}\Big] ~~\cup~~ \Big[\frac{6}{9}, \frac{7}{9}\Big] \cup \Big[\frac{8}{9}, 1\Big].
\end{align*}
The Cantor set is defined to be the set of all points that are never removed:
\begin{align*}
  C := \bigcap_{n=1}^\infty C_n.
\end{align*}
Since the sets form a chain $C_0 \supset C_1 \cdots$, we can also write
\begin{align*}
  C = \lim_{n\to\infty} C_n.
\end{align*}
\end{definition*}

\begin{theorem*}
  The middle-thirds Cantor set:
  \begin{enumerate}
  \item is uncountable;
  \item is compact;
  \item every point is a limit point;
  \item contains no intervals;
  \item has zero measure;
  \end{enumerate}
\end{theorem*}
\begin{proof}[zero measure]
At generation $n$ we have $2^n$ closed intervals each of length $(1/3)^n$. Therefore
\begin{align*}
  \mu(C_n) = (2/3)^n
\end{align*}
and therefore
\begin{align*}
  \mu(C) = 0.
\end{align*}
\end{proof}

\begin{intuition*}
  The closed intervals that exist at generation $n$ of the Cantor set constructions become singletons in the
  limit $n \to \infty$. For example $[0,\frac{1}{3}]$ becomes $[0,\frac{1}{9}]$, etc. In the limit this gives
  rise to a singleton:
  \begin{align*}
    \lim_{n\to\infty} \Big[0, \frac{1}{3^n}\Big] = \{0\}.
  \end{align*}
  However, this singleton is arbitrarily close to another singleton. In fact, every point in the Cantor set is
  a limit point.

  Somehow however, points other than endpoints are in the Cantor set. For example, $1/4$.
\end{intuition*}

\subsubsection{Generalized Cantor sets}
    Let $a \in (0, 1)$. The Cantor set of measure $a$ is formed as follows:

    Note that $\sum_{n=1}^\infty \frac{1 - a}{2^n} = 1 - a$. So we will design an algorithm that
    removes $\frac{1-a}{2^n}$ at each iteration, for $n=1, 2, \ldots$. Note that at the start of iteration $n$
    there are $2^{n-1}$ intervals. So we remove
    \begin{align*}
      \frac{1-a}{2^{n}}\cdot\frac{1}{2^{n-1}} = \frac{1 - a}{2^{2n - 1}}
    \end{align*}

    from each interval.

    For example, to create a set with measure $a = \frac{1}{2}$,
    remove $\frac{1}{4} + 2(\frac{1}{16}) + 4(\frac{1}{64}) + \cdots = \frac{1}{2}$.


\subsection{An open set can be written as a countable union of disjoint open intervals}

\begin{theorem}
  Let $G \subset \R$. Then $G$ can be written as a {\it countable} union of disjoint open intervals.
\end{theorem}

\begin{proof}

\end{proof}

\begin{example}[A non-measurable set]
  Consider the unit circle $S^1$. We would like to define a countably additive, translation invariant,
  function $\mu:\powerset(S^1) \to [0, \infty]$, which we will call a ``measure​''.

  Let $r$ be irrational and define the translation $\tau(x) = (x + r) \mod 1$.

  Recall that for all $x \in S_1$, the orbit of $x$ under $\tau$ is non-periodic and dense.

  (Note that non-periodicity of the orbit is quite a striking property: it never hits a point twice!)

  Define a set $A_0$ containing one point from every distinct orbit. (Uses some form of Axiom of Choice)

  Define $A_n = \tau^n(A_0)$.

  Note that $A_i$ and $A_j$ are disjoint for all $i \neq j$. (Suppose they had a point in common. Then that
  point would either be a member of two distinct orbits, or it would be a point that occurs twice in the same
  orbit. Neither is possible.)

  Therefore, if the $A_i$ are measurable, then
  \begin{align*}
    \mu\Big(\bigcup_{i=0}^\infty A_i\Big) = \sum_{i=0}^\infty \mu(A_i).
  \end{align*}
  But note that $\bigcup_{i=0}^\infty A_i = S^1$, therefore the LHS equals 1.

  However, $\tau$ is a translation, and therefore $\mu(A_i) = \mu(A_0)$ for all $i$, if $\mu$ is translation
  invariant.

  But this is a contradiction, since if $\mu(A_0) = 0$ then we have $1 = \sum_{i=0}^\infty 0 = 0$, and
  if $\mu(A_0) > 0$ then we have $1 = \infty$.

  Therefore no countably additive and translation invariant $\mu$ can be defined on the $A_i$.

  Counter-examples like this motivate the restriction of measure to $\sigma$-algebras.
\end{example}


\subsection{sigma-algebras, Borel sets}

\url{https://en.wikipedia.org/wiki/Outcome_(probability)}


An \defn{outcome} is an atomic, lowest-level, result of an experiment/process. Outcomes are mutually exclusive.

An \defn{event} is a set of \defn{outcomes} that we assign probability to. It is a subset of $\Omega$. Events are not mutually
exclusive: ``greater than 0.5?​'' and ``greater than 0.6?​'' are both events and, for a given outcome, both events
may ``occur​''.

An \defn{algebra} is a collection of events. So an algebra contains all the things we might assign probability to.
Furthermore, an algebra must be closed under complementation, union and intersection ( ``not​'', ``or​'', and ``and​'').

A $\sigma$-\defn{algebra} is an algebra that is closed under countably infinite unions and intersections.




\begin{definition*}[$\sigma$-algebra]
  An \defn{algebra} in $\Omega$ is a collection of subsets of $\Omega$ that
  \begin{enumerate}
  \item contains $\emptyset$ and $\Omega$
  \item is closed under complements
  \item is closed under {\it finite} unions and intersections
  \end{enumerate}

  It is a $\sigma$-\defn{algebra} if it is additionally closed under {\it countable} unions and intersections.
\end{definition*}

\begin{definition}
  A ($\sigma$-) algebra \defn{generated} by a collection of subsets is the smallest ($\sigma$-) algebra of which that
  collection is a subset.
\end{definition}


\begin{quote}
  the intersection of all fields in $\Omega$ containing $\ms A$.
\end{quote}

Here, ``containing​'' means subset inclusion.

An in fact, ``in​'' here means neither set membership nor subset inclusion. It is used in a more technical sense
to refer to fields whose sets are subsets of $\Omega$ (i.e. fields that are elements of the powerset
of $\Omega$). Bass uses ``on​'' for this: ``a field on $\Omega$​''.

\begin{definition}[Borel $\sigma$-algebra, Borel set]
  A \defn{Borel} $\sigma$-\defn{algebra} is the $\sigma$-algebra generated by the open sets.

  A \defn{Borel set} is a subset of $\Omega$ that is an element of a Borel $\sigma$-algebra.
\end{definition}

A Borel $\sigma$-algebra contains is generated by open sets (and so equivalently by closed sets). But
the $\sigma$-algebra contains singletons, half-closed intervals, etc.

\begin{theorem}
  The Borel $\sigma$-algebra on $\R$ can be generated by the following sets
  \begin{enumerate}
  \item $\ms I_1 = \{(a, b) ~:~ a, b \in \R\}$
  \item $\ms I_2 = \{[a, b] ~:~ a, b \in \R\}$
  \item $\ms I_3 = \{(a, b] ~:~ a, b \in \R\}$
  \item $\ms I_4 = \{[a, \infty) ~:~ a \in \R\}$
  \item $\ms I_5 = \{(p, q) ~:~ p, q \in \Q\}$
  \end{enumerate}
\end{theorem}

\begin{proof}
  Let $\ms O$ be the collection of open subsets of $\R$, so that $\ms B = \sigma(\ms O)$.
  \begin{enumerate}
  \item The key here is that every open subset of $\R$ is a countable union of open intervals. So the collection of
    open sets is automatically in the $\sigma$-algebra generated by the open intervals, so you can't ``get
    anything new​'' from them.

    Let $\ms I_1 = \{(a, b) ~:~ a, b \in \R\}$. We want to show that $\sigma(\ms I_1) = \sigma(\ms O)$.

    In one direction, every element of $\ms I_1$ is open, so clearly $\sigma(\ms I_1) \subseteq \sigma(\ms O)$.

    For the other direction, let $X \in \ms O$ be an open subset of $\R$. Then $X$ is a countable union of open
    intervals (i.e. finite intervals and open rays). Every finite interval is in $\ms I_1$. But open ray are
    also countable unions of finite intervals: $(-\infty, a) = \bigcup_n^\infty (a-n, a)$
    and $(a, \infty) = \bigcup_n^\infty (a, a + n)$. Therefore $X \in \sigma(\ms I_1)$, i.e. every open set is
    in the $\sigma$-algebra generated by open intervals. This is equivalent to the
    statement $\ms O \subseteq \sigma(\ms I_1)$, i.e. the collection of all open sets is a subset of
    that $\sigma$-algebra. Therefore $\sigma(\ms O) \subseteq \sigma(\sigma(\ms I_1)) = \sigma(\ms I_1)$.

  \item We reduce this to (1) by showing that we can make $(a, b)$ from $[a, b]$.

    Let $\ms I_2 = \{[a, b] ~:~ a, b \in \R\}$. We want to show that $\sigma(\ms I_2) = \sigma(\ms O)$.

    To show $\sigma(\ms I_2) \subseteq \sigma(\ms O)$, note that for $a < b$
    \begin{align*}
      [a, b] &= \bigcap_{n=1}^\infty (a - n^{-1}, b + n^{-1}).
    \end{align*}
    Therefore $[a, b] \in \sigma(\ms I_1)$ for all $a, b \in \R$, hence $\sigma(\ms I_2) \subseteq \sigma(\ms I_1) = \sigma(\ms O)$.

    To show $\sigma(\ms O) \subseteq \sigma(\ms I_2)$, note that for $a < b$ and $n_0 \geq 2/(b - a)$
    \begin{align*}
      (a, b) &= \bigcup_{n=N_0}^\infty [a + n^{-1}, b - n^{-1}].
    \end{align*}
    Therefore $(a, b) \in \sigma(\ms I_2)$ for all $a, b \in \R$ hence $\ms I_1 \subseteq \sigma(\ms I_2)$,
    hence $\sigma(\ms I_1) = \sigma(\ms O) \subseteq \sigma(\ms I_2)$. But we have already shown
    that$\sigma(\ms I_1) = \sigma(\ms O)$, therefore $\sigma(\ms O) \subseteq \sigma(\ms I_2)$.
  \end{enumerate}
\end{proof}

\subsection{Bass 3. Measures}

Let $\Omega$ be a set and $\mc A$ a $\sigma$-algebra on $\Omega$.

A \defn{measure} is a function $\mu:\mc A \to [0, \infty]$ that is \defn{countably additive} (CA; measure of disjoint union equals sum of measures).

CA has various implications which make $\mu$ behave in unsurprising ways:
\begin{enumerate}
\item It's finitely and countably \defn{subadditive} (measure of union does not exceed sum of measures)
\item Measure of limiting sets equals limit of measures (e.g. if $A_i \uparrow A$ then $\mu(A) = \lim_n \mu(A_i)$)
\end{enumerate}

\begin{intuition*}
  The function $\mu$ is a map from sets to reals. So in principle it could assign whatever real values it wants
  to whatever sets. E.g. for disjoint $A, B$ it could assign a value to $\mu(A \cup B)$ that is completely
  different from $\mu(A) + \mu(B)$.

  In fact, however, measures treat sets as an aggregate of points. I think that everything is perfectly
  intuitive except that countably infinite unions might not work as expected.

  In other words, $\mu$ acts exactly as one would expect: as if it's applying a uniform layer of paint to each
  subset: the total amount of paint used to paint a union of disjoint sets is the sum of the paint applied to
  each set in the union.

  But CA means the additivity is retained even when there are infinitely many sets in the union. An example of CA
  failing to hold is densities of finite subsets of the natural numbers: the density of the singleton $\{1\}$ as
  a proportion of the natural numbers is naturally defined to be $0$ (the limit of the density in a finite sample
  as the sample size tends to infinity). But the density of the countable union of all singletons is $1$, which is
  not the sum of the densities.
\end{intuition*}

A measure is \defn{finite} if $\mu(\Omega) < \infty$, and $\sigma$\defn{-finite} if there exists a countable partition
of $\Omega$ with each subset in the partition having finite measure.

$(\Omega, \mc A, \mu)$ is a \defn{measure space}.

A subset $A \subset \Omega$ (not necessarily in $\mc A$) is a \defn{null set} if $A$ is a subset of some element
of $\mc A$ which has zero measure. $(\Omega, \mc A, \mu)$ is a \defn{complete} measure space if all null sets are
in $\mc A$. The \defn{completion} of $\mc A$ is the smallest complete $\sigma$-algebra $\bar{\mc A}$ containing
$\mc A$ such that $(\Omega, \bar{\mc A}, \bar \mu)$ is a complete measure space, where $\bar \mu$ is an
extension of $\mu$ from $\mc A$ to $\bar{\mc A}$.

A \defn{probability measure} is a measure where $\mu(\Omega) = 1$.

\subsection{Bass 4.  Construction of measures}

\subsubsection{Overview}

\begin{enumerate}

\item We want a function that, in some appropriate sense, measures the {\it length} of an arbitrary subset of
  $\R$.

\item We're not going to get the ``sensible measure of length​'' property out of nowhere: we're going to inject a
  pre-existing sensible measure of length of tractable sets at a low level, and build on this.

\item That low-level ``sensible measure of length​'' is, when we're working with $\R$, going to be the length of an
  interval: $|(a, b)| = b - a$.

\item Clearly we want our measure of length to be additive over a finite collection of subsets. But we will also
  require it to be additive over a countably infinite collection of subsets, and this requirement is central to
  everything that follows.

\item So, more precisely, what we want is a {\it countably additive} set function (i.e. a \defn{measure}) defined on a large collection (as
  large as possible) of subsets of $\R$, that is a ``sensible measure of length​'' of those subsets.

\item A theorem tells us a way to make a {\it countably sub-additive} set function (i.e. an \defn{outer measure}) defined on {\it all} subsets:
  \begin{enumerate}
  \item Let $E$ be an arbitrary subset of $\R$ that we want to measure.
  \item Now, restrict attention to the collection of ``low-level​'' subsets of $\R$ for which we have the pre-existing
    sensible measure of length. In $\R$, these are open intervals, or perhaps half-open intervals \red{open sets?}.
  \item Sometimes, one of these low-level subsets will cover $E$. But if $E$ is not a simple interval, we will
    approximate the length of $E$ better with a collection of low-level subsets whose union covers $E$, while
    none of them do on their own. Whether it is a collection of one or many, we will refer to this as a
    ``covering collection of subsets​''.
  \item Note that the covering collection is built out of the low-level subsets, so we can assign a sensible
    measure of length to the covering collection: in $\R$, it is just the sum of lengths of the intervals involved.
  \item Create a set containing the measure of every covering collection. We define our outer measure on $E$ to be
    the infimum (greatest lower bound) of that set. Roughly speaking, we've defined $\mu^*(E)$ to be the total
    length of the collection of intervals that cover $E$ most efficiently (with least unnecessary overlap).
  \end{enumerate}

\item We will call our outer measure $\mu^*$. Clearly it is a reasonable measure of length for some sets.

\item Recall that it is defined on {\it all} subsets of $\R$ (its definition involved our restricted collection of
  ``low-level​'' subsets, but the resulting procedure can be applied to any subset).

\item Pause here: this definition of $\mu^*$ is fundamental. The outer measure that we assign to an arbitrary
  subset $E$ is obtained by using the intervals $I_i$ that we {\it can} measure. We look over all collections of
  the $I_i$ that cover $E$ and record the total length of each cover. The infimum of these cover lengths is the
  measure assigned to $E$:
  \begin{align*}
    \mu^*(E) = \inf\Big\{\sum_i \ell(I_i) : E \subseteq \bigcup_i I_i\Big\}.
  \end{align*}
\item Now, it's nice that it is defined on all subsets of $\R$, and it does have some sensible properties such as
  countable sub-additivity, and probably finite additivity, but it does {\it not} necessarily have the countable
  additivity property that we require.

\item We can get that though with an adjustment: we restrict the collection of subsets that we're allowed to
  measure, so that it's no longer {\it all} subsets.

\item There are two candidates we could restrict to. One is the \defn{Borel} $\sigma$-algebra. This is the
  $\sigma$-algebra generated by the open subsets.

\item However, there's a larger $\sigma$-algebra we can restrict to: the \defn{Lebesgue} $\sigma$-algebra. This is the class
  of $\mu^*$-measurable sets. A set $A \subset \R$ is $\mu^*$-measurable if finite additivity holds between $A$
  and every other subset of $\R$, that is $\mu^*(A \cap E) + \mu^*(A \cap E^c) = \mu^*(A)$ for
  all $E \subset \R$.

\item The Borel $\sigma$-algebra is contained within the Lebesgue $\sigma$-algebra. Restricting $\mu^*$ to either
  gives us what we want: countable additivity. The restriction of $\mu^*$ to the Lebesgue $\sigma$-algebra is
  \defn{Lebesgue measure} $\mu$.

\item The above involved using interval length as our measure of the low-level subsets:
  $\ell(I_i) = b_i - a_i$. There is an important generalization known as \defn{Lebesgue-Stieltjes measure}: we introduce a
  real-valued increasing function $\alpha: \R \to \R$ that distorts the measures we assign to each
  interval: $\ell(I_i) = \alpha(b_i) - \alpha(a_i)$. So an interval in a region in which $\alpha$ is increasing
  rapidly has larger measure. Other than that, the theory for Lebesgue-Stieltjes measure is the same: we define
  the outer measure using the low-level intervals on which $\ell$ is defined, and we restrict to the collection
  of measurable sets, which is a $\sigma$-algebra.

\item The Carath\'eodory extension theorem states:
  Suppose:
  \begin{enumerate}
  \item We have $\ell$ which behaves like a measure on some algebra (countable additivity does hold if the
    countable union happens to be in the algebra).
  \item We construct an outer measure $\mu^*(E)$ on \defn{any} subset $E$ using the standard outer measure construction
    (infimum of lengths of open covers, using $\ell$ to measure length).
  \end{enumerate}
  Then $\mu^*$ is countably additive on $\mu^*$



\end{enumerate}



Suppose we are defining a measure $m$ on $\Omega = \R$.

Since an open set $G$ is a countable union of disjoint open intervals, we want
\begin{align*}
  m(G) = \sum_{i=1}^\infty b_i - a_i.
\end{align*}
The basic idea is that, for a set $E$, we are going to define $m(E)$ to be the measure of the smallest open
cover of $E$:
\begin{align*}
  m(E) := \inf \{ m(G) ~:~ G \text{~open~}, E \subseteq G\}.
\end{align*}
However, the infimum has to be restricted to a $\sigma$-algebra that is smaller than all subsets of $\R$ (the
latter is a $\sigma$-algebra, but it's not possible to construct a measure with this as its domain [theorem]).

An \defn{outer measure} is defined similarly to a measure. The difference is
\begin{enumerate}
\item It is defined on {\it all} subsets of $\Omega$
\item It obeys \defn{countable subadditivity}, but {\it not} necessarily countable additivity.
\end{enumerate}

A common way to construct an outer measure is
\begin{enumerate}
\item Define a collection $\mc C$ of subsets of $\Om$ (the collection must contain a subset that partitions $\Om$.)
\item Define a cost function $\ell : \mc C \to [0, \infty]$
\item Define $\mu^*(E)$ to be the cost of the least-cost cover of $E$:
  \begin{align*}
    \mu^*(E) := \inf \Big\{\sum_{i=1}^\infty\ell(C_i) ~:~ E \subseteq \bigcup_{i=1}^\infty C_i\Big\}.
  \end{align*}
  Note what this does: it defines $\mu^*$ on {\it any} subset $E$, while the collection $\mc C$ typically comes from a
  restricted collection (e.g. open sets).
\end{enumerate}

It's obvious that this is FSA but one has to prove that it is CSA (using an epsilon-of-room technique).

\begin{question*}
  \red{TODO} Must an outer measure be finitely additive?

  I don't think so, FA doesn't follow from CS. But could one include FA in the definition of an outer measure?


Is an outer measure always FA? I.e. the issue is just getting CA?

=> Sort of but look at Vitali construction for a counter-example

  Is the (canonical) example given finitely additive?
\end{question*}

\begin{example}[Lebesgue measure on subsets of $\R$]
  \begin{enumerate}
  \item Consider the collection of all open intervals $O \subseteq \R$.
  \item Define $\ell(O)$ in the normal way (sum of $b_i - a_i$ over the disjoint open intervals).
  \item Define $\mu^*(E)$ to be the infimal cost of an open cover of $E$ (i.e. as above in the canonical
    construction of outer measure)
  \end{enumerate}
  So what we've just constructed is an outer measure: it assigns a measure to {\it any} subset $E$, using open sets
  for the cover.

  It must be countably sub-additive, because it used the canonical construction, and we have a theorem for that.

  Now, we would ideally like this to be a measure on all subsets of $\R$, i.e. countably additive.

  It isn't (theorem), but it {\it is} a measure on a restricted collection of subsets of $\R$. That collection is
  the $\sigma$-algebra generated by the open sets (the Borel algebra).

  This measure is \defn{Lebesgue measure}.

  To recap: it was constructed as follows:
  \begin{enumerate}
  \item We want to assign a value to some subset $E$
  \item We use open intervals to cover $E$
  \item The value assigned to $E$ is the measure of the smallest open cover (using length of open intervals to
    define measure here)
  \item This gives a countably subadditive function (an outer measure).
  \item But this is only countably additive for $E$ in the $\sigma$-algebra.
  \end{enumerate}
\end{example}

\begin{intuition*}
  Outer measures have that name because they approximate from above ( ``shrink-wrapping​'').

  When you approximate from above, you get something which is countably subadditive:

  \begin{proof}
    Suppose $\mu^*(A \cup B) > \mu^*(A) + \mu^*(B)$.

    Intuitively, take the most efficient cover of $A$ and the most efficient cover of $B$. Their combined cost
    is $\mu^*(A) + \mu^*(B)$. But together they are a cover of $A \cup B$. But that implies that the most
    efficient cover of $A \cup B$ is no greater than $\mu^*(A) + \mu^*(B)$; a contradiction.

    A real proof has to deal with the fact that we're dealing with infima, not minima.
  \end{proof}
\end{intuition*}

\begin{lemma*}[A continuous function is measurable]\label{lemma-continuous-function-is-measurable}
  Let $Y \subseteq \R$ and let $g: X \to Y$ be a continuous function.

  We must show that there exists a $\sigma$-algebra such that $\{x ~:~ g(x) > y\} \in \mc A$ for all $y \in Y$.

  Let $y \in Y$ and let $U = g^{-1}((y, \infty))$. Then $U$ is open in $X$ since it is the preimage of an open
  set under a continuous function. Therefore $U$ is in the Borel $\sigma$-algebra on $X$.

  Therefore the Borel $\sigma$-algebra satisfies our requirement and $g$ is measurable.
\end{lemma*}

\section*{Lim sup and lim inf}

\subsection{For a sequence of numbers}
\begin{align*}
  \liminf_{n\to\infty} x_n := \lim_{n\to\infty} \inf_{m \geq n} x_m
\end{align*}
This is the largest value below which the sequence never falls again. As we increment $n$, the tail of the
sequence beyond $n$ has some $\inf$. These $\inf$s form an increasing sequence which converge to some value
or $+\infty$. This value is the $\liminf$.

\red{TODO} $\liminf_{n \to \infty} x_n$ can also be described as the $\inf$ of the limits of all convergent subsequences.

\begin{align*}
  \limsup_{n\to\infty} x_n := \lim_{n\to\infty} \sup_{m \geq n} x_m
\end{align*}
This is the smallest value above which the sequence never rises again. The $sup$s form a decreasing sequence
which converge to some value or $-\infty$. This value is the $\limsup$.

If $x_n$ has a limit (either real, or $\pm\infty$) then $\liminf x_n = \limsup x_n = \lim x_n$.

\subsection*{For  a sequence of functions}

$\liminf_{n\to\infty} f_n$ is the function defined by $(\liminf_{n\to\infty} f_n)(x) = \liminf_{n\to\infty} f_n(x)$.

Thus for a sequence $f_n$ of well-behaved curves $\liminf_{n\to\infty} f_n$ describes a curve with the property
that, at each point $x$, there is a point in the sequence beyond which the sequence never falls
below $(\liminf_{n\to\infty} f_n)(x)$.




\subsection{For a sequence of values of a single function}
Let $f(x) = \sin(1/x)$. Then $\liminf_{x \to 0} f(x) = -1$ and $\limsup_{x\to 0} f(x) = +1$. The difference
between the two is called the \defn{oscillation} of $f$ at $0$.

\subsection{For a sequence of sets}

lim sup: elements that always will reappear again

lim inf: elements that eventually never disappear again

lim inf subset of lim sup


\section*{Expressions involving countable unions and intersections}

A singleton $\{x\}$ is a countable intersection of intervals:
    \begin{align*}
      \bigcap_{n=1}^\infty \Big(x -\frac{1}{n}, x\Big] = \{x\}.
    \end{align*}

Let $x, a \in \R$ and $n \in \N$. Sets of real numbers satisfying inequalities:
\begin{align*}
  \big\{x ~:~ x \geq a\big\} = \bigcap_{i=1}^\infty \big\{x ~:~ x > a - 1/n\big\}
\end{align*}

\section{Dynamical Systems and Ergodicity}

\begin{definition}
  A measure $\mu$ is $f$-invariant if $\mu(f^{-1}(A)) = \mu(A)$ for all $A$ in the $\sigma$-algebra.
\end{definition}

\begin{definition}
  A measure $\mu$ is $f$-ergodic if $f^{-1}(A) = A$ implies $\mu(A) = 0$ or $\mu(A) = 1$.
\end{definition}

\section{Lebesgue integral}

All functions are measurable unless stated otherwise.

\begin{theorem*}[MCT]
  Let $f_n$ be an {\it increasing} sequence of non-negative functions (i.e. $f_1 \leq f_2 \leq \cdots$). Then, if
  the $f_n$ converge pointwise, the sequence of integrals converges to the integral of the limiting function:
  \begin{align*}
    \lim \int f_n &= \int \limn f_n.
  \end{align*}
\end{theorem*}


\begin{proof}
  $f_n$ is increasing therefore, by monotonicity of the Lebesgue integral, $\int f_n$ is an increasing sequence
  of real numbers.

  Let $L = \lim \int f_n$. We must show $L \leq \int f$ and $L \geq \int f$.

  The first direction is easy: since $f_n \leq f$ for all $n$ we have $\int f_n \leq \int f$ (by monotonicity
  of Lebesgue integral) and therefore $\lim \int f_n \leq \int f$ (since taking a limit preserves an
  inequality).

  To show $\lim \int f_n \geq \int f$ we use a simple function.

  Let $0 \leq s \leq f$ be a simple function, where $s = \sum_{i=1}^k a_i\ind_{E_i}$.

  Let $\eps \in (0, 1)$ and define
  \begin{align*}
    A_n = \{x ~:~ f_n(x) \geq (1 - \eps)s(x)\},
  \end{align*}
  i.e. the set of points where $f_n$ is within $\eps$ of $s$.

  We will show that $\int f_n$ is always at least as big as a quantity that converges to $\int f$.

  Since the $f_n \to f$ we have $A_n \uparrow X$. Therefore
  \begin{align*}
    \int f_n
    \geq \int_{A_n} f_n
    &\geq \int_{A_n} (1 - \eps)s \\
  % &=    (1 - \eps)\int_{A_n} \sum_{i=1}^k a_i\ind_{E_i} \\
    &=    (1 - \eps)\int \sum_{i=1}^k a_i\ind_{E_i \cap A_n} \\
    &=    (1 - \eps)\sum_{i=1}^k a_i\mu(E_i \cap A_n) \\
  \end{align*}
  Now we let $n \to \infty$, obtaining
  \begin{align*}
    \limn \int f_n
    &\geq (1 - \eps)\sum_{i=1}^k a_i\mu(E_i) \\
    &= (1 - \eps)\int s.
  \end{align*}
  Since $\eps$ is arbitrary in $(0, 1)$ we have
  \begin{align*}
    \limn \int f_n \geq \int s.
  \end{align*}
  And since $s$ is an arbitrary simple function satisfying $0 \leq s \leq f$ we may take the supremum over all
  such $s$ yielding
  \begin{align*}
    \limn \int f_n
    &\geq \sup_{0 \leq s \leq f} \int s\\
    &=:    \int f.
  \end{align*}
\end{proof}

\begin{theorem*}[linearity of Lebesgue integral]
  If $f$ and $g$ are either
  \begin{enumerate}
  \item non-negative and measurable, or
  \item integrable,
  \end{enumerate}
  then
  \begin{align*}
    \int (f + g) = \int f + \int g.
  \end{align*}
\end{theorem*}

\begin{proof}
  First (\red{TODO}) we show that the result holds for non-negative simple functions $s$ and $t$.

  Next we suppose $f$ and $g$ are non-negative and consider sequences $(s_n)$ and $(t_n)$ of non-negative
  simple functions increasing to $f$ and $g$ respectively. By the MCT we have
  \begin{align*}
    \lim_{n\to\infty} \int (s_n + t_n)
    &= \int \lim_{n\to\infty} (s_n + t_n) \\
    &= \int (f + g).
  \end{align*}
  But, using linearity of non-negative simple functions, the LHS is
  \begin{align*}
    \lim_{n\to\infty} \int (s_n + t_n)
    &= \lim_{n\to\infty} \int s_n + \lim_{n\to\infty} \int t_n \\
    &= \int f + \int g.
  \end{align*}
  Finally we allow $f$ and $g$ to take both positive and negative values, while being integrable. We use the
  triangle inequality to prove that $f + g$ is integrable given that $f$ and $g$ are, then mess about with the
  decomposition of $f$ and $g$ into $f^+, f^-, g^+, g^-$.
\end{proof}

\begin{intuition*}
  If $(f_n)$ is a sequence of continuous curves, $\liminf_{n\to\infty} f_n$ is a curve that you see ``at the
  horizon​'' when viewing the $(f_n)$ sequence from underneath. At each
  point $(\liminf_{n\to\infty} f_n)(x) := \limn (\inf_{m\geq n}f_m(x))$ is the height that the curves
  eventually never fall below. If the $f_n$ have a pointwise limit then this will
  be $\liminf_{n\to\infty} f_n$. If they do not have a limit, the statement one can make is that there exists a
  point beyond which the $f_n$ curves are no smaller then the $\liminf_{n\to\infty} f_n$ curve.
\end{intuition*}

\begin{theorem}[Fatou's lemma]
  For a sequence of non-negative measurable functions $f_n$ the limit of the sequence of integrals is at least
  as big as the integral of the limit inferior function.
  \begin{align*}
    \liminf_{n\to\infty} \int f_n \geq \int \liminf_{n\to\infty} f_n.
  \end{align*}
\end{theorem}

\begin{mdframed}
\includegraphics[width=400pt]{img/analysis--berkeley-202a--billingsley-section-1--lebesgue-integral-a3de.png}
\end{mdframed}
\url{https://math.stackexchange.com/a/242930/397805}

\begin{mdframed}
\includegraphics[width=400pt]{img/analysis--berkeley-202a--billingsley-section-1--lebesgue-integral-70e1.png}
\end{mdframed}

% https://math.stackexchange.com/a/2748616/397805

\begin{proof}
  Let $g_n(x) = \inf_{m\geq n}f_m(x)$. Note that $(g_n)$ is an increasing sequence of functions.

  By definition,
  \begin{align*}
    \int \liminf_{n\to\infty} f_n
    &:= \int \limn \inf_{m\geq n}f_m \\
    &=: \int \limn g_n \\
    &= \limn \int g_n ~~~~~~~\text{(by the monotone convergence theorem)}\\
    &\leq \limn \int f_n ~~~~~~~\text{(by monotonicity of Lebesgue integral)}.
  \end{align*}
\end{proof}


Since $\limsupn f_n = -\liminfn (-f_n)$

A typical use of Fatou’s lemma is the following. Suppose we have $f_n \to f$ and $\sup_n \int |f_n| \leq K < \infty$.


\begin{theorem*}[dominated convergence theorem]
  Suppose $f_n$ are a sequence of functions and there exists an integrable dominating function $g$ such
  that$|f_n| \leq g$. Then, if the $f_n$ converge pointwise, the sequence of integrals converges to the
  integral of the limiting function:
  \begin{align*}
    \lim \int f_n &= \int \limn f_n.
  \end{align*}
  Furthermore the $f_n$ are integrable.
\end{theorem*}

\begin{proof}

  (This is the proof in Bass)

  Note that $g + f_n \geq 0$. Therefore by Fatou's lemma
  \begin{align*}
    \liminfn \int (g + f_n) \geq \int \liminfn (g + f_n),
  \end{align*}
  and by linearity of the integral, integrability of $g$, and convergence of the $f_n$,
  \begin{align*}
    \liminfn \int f_n
    \geq \int \liminfn f_n
    = \int f.
  \end{align*}

  Similarly, $g - f_n \geq 0$, hence
  \begin{align*}
    \liminfn \int (g - f_n) \geq \int \liminfn (g - f_n),
  \end{align*}
  and
  \begin{align*}
    \liminfn \Big(-\int f_n\Big) \geq -\int f,
  \end{align*}
  or equivalently,
  \begin{align*}
    \limsupn \int f_n \leq \int f.
  \end{align*}

  Thus we have
  \begin{align*}
    \int f \leq \liminfn \int f_n \leq \limsupn \int f_n \leq \int f,
  \end{align*}
  Therefore
  \begin{align*}
    \limn \int f_n = \int f,
  \end{align*}
  as required.
\end{proof}


This alternative proof uses triangle inequality arguments:

\begin{proof}

  (From Bright Side Of Mathematics video lectures; different from main proof in Bass.)
  % Note that from monotonicity of the Lebesgue integral we have $\int |f_n| \leq \int g < \infty$ and
  % also $\int |f| \leq \int g < \infty$.

  We will show that $\lim \int |f_n - f| = 0$.

  Note that $|f_n - f| \leq |f_n| + |f| \leq 2g$.

  Note that $2g - |f_n - f| \geq 0$. Therefore, by Fatou's lemma,
  \begin{align*}
    \liminf_{n\to\infty} \int (2g - |f_n - f|) &\geq \int \liminf_{n\to\infty} (2g - |f_n - f|).
  \end{align*}
  Since $\int g < \infty$, and since $f_n \to f$, we have
  \begin{align*}
    2\int g -  \limsupn \int |f_n - f| &\geq 2\int g,
  \end{align*}
  therefore
  \begin{align*}
    \limsupn \int |f_n - f| &\leq 0.
  \end{align*}
  But
  \begin{align*}
    0 \leq \liminf \int |f_n - f| \leq \limsupn \int |f_n - f| &\leq 0,
  \end{align*}
  Therefore $\lim \int |f_n - f| = 0$.

  But note that
  \begin{align*}
    0
    &\leq \Big|\int f_n - \int f\Big| \\
    &= \Big|\int (f_n - f)\Big| \\
    &\leq \int |f_n - f| \to 0.
  \end{align*}
  Therefore
  \begin{align*}
    \limn \Big|\int f_n - \int f\Big| = 0,
  \end{align*}
  and therefore
  \begin{align*}
    \limn \int f_n = \lim \int f,
  \end{align*}
  as required.
\end{proof}


\begin{theorem*}[MCT, Fatou's lemma, and DCT, summarised]
  Suppose $f_n$ are a sequence of functions that converge pointwise to some limiting function.

  Consider the sequence of integrals $\int f_n$.

  If either
  \begin{enumerate}
  \item $f_n$ are {\it non-negative} and $f_1 \leq f_2 \leq \cdots$ (MCT), or
  \item $|f_n| \leq g$ and $g$ is {\it integrable} (DCT),
  \end{enumerate}
  then the sequence of integrals converges to the integral of the limiting function:
  \begin{align*}
    \lim \int f_n &= \int \limn f_n.
  \end{align*}
  Alternatively, if all we know is that the $f_n$ are non-negative, then the limit of the sequence of integrals
  is no smaller than the integral of the limit inferior function (Fatou's lemma):
  \begin{align*}
  \limn \int f_n \geq \int \liminf_{n\to\infty} f_n.
  \end{align*}
\end{theorem*}

\begin{theorem*}[8.2 An approximation result on $\R$]
  Suppose $f: \R \to \R$ is a Borel-measurable function. Then for every $\eps > 0$ there exists a continuous
  function $g$ with compact support such that
  \begin{align*}
    \int (f - g) < \eps.
  \end{align*}
\end{theorem*}

\begin{proof}
  \red{TODO}
\end{proof}

\section{Lebesgue vs Riemann integrals}

We use $R(f)$ to denote the Riemann integral and $\int f$ to denote the Lebesgue integral.

\begin{definition*}[Riemann integral]
  Given a partition $P$, define the majorant approximation $U(P, f)$ to be the area under the step function
  comprising rectangles of the form
  \begin{align*}
    (\sup_{x_{i-1} \leq x \leq x_i} f(x))(x_i - x_{i-1}).
  \end{align*}
  The minorant approximation $L(P, f)$ is the analogous thing using $\inf$.

  Define
  \begin{align*}
    \overline{R}(f) = \inf \{ U(P, f) ~:~ P \text{~is a partition~}\}
  \end{align*}
  and
  \begin{align*}
    \underline{R}(f) = \sup \{ L(P, f) ~:~ P \text{~is a partition~}\}.
  \end{align*}
  The Riemann integral $R(f)$ exists if $\overline{R}(f) = \underline{R}(f)$ and is equal to the common value.
\end{definition*}

\begin{theorem*}
  A bounded Borel-measurable function $f:[a, b]\to\R$ is Riemann integrable if and only if it is continuous
  a.e. In that case the Lebesgue and Riemann integrals agree.
\end{theorem*}

\begin{proof}
  For the forwards direction we show that if $f$ is Riemann integrable then $f$ is continuous a.e.
  and $R(f) = \int f$.

  Given a partition $P$ define the simple functions corresponding to the majorant and minorant approximations:
  \begin{align*}
    T_P(x) = \sum_{i=1}^n \Big(\sup_{[x_{i-1}, x_i]} f\Big)\ind_{[x_{i-1}, x_i)}(x)
  \end{align*}
  and
  \begin{align*}
    S_P(x) = \sum_{i=1}^n \Big(\inf_{[x_{i-1}, x_i]} f\Big)\ind_{[x_{i-1}, x_i)}(x).
  \end{align*}
  Note that $\int T_P = U(P, f)$ and $\int S_P = L(P, f)$.

  We now argue (\red{TODO}), using sequences of partitions, that $T_P$ decreases at each point to, say, $T$, and
  $S_P$ increases at each point to, say, $S$, and that
  \begin{align*}
    T = S = F \ae
  \end{align*}
  The proof of this equality uses the DCT (recall that $f$ is bounded):
  \begin{align*}
    \int (T - S) = \lim_{i \to \infty} \int (T_{P_i} - S_{P_i}) = \lim_{i\to\infty} \big(U(P_i, f) - L(P_i, f)\big) = 0.
  \end{align*}
\end{proof}

\section{Ch 8. Properties of the Lebesgue integral}

\begin{theorem}[conditions for $f = 0$ a.e.]
  Let $f$ be real-valued and measurable. Then $f = 0$ a.e. if
  \begin{enumerate}
  \item $f$ is non-negative and $\int f = 0$,
  \item $\int_A f = 0$ on every measurable set $A$,
  \item $\int_0^x f(u) \du = 0$ for all $x$ under Lebesgue measure.
  \end{enumerate}
\end{theorem}

\begin{theorem}
  If $f$ is non-negative and $\int f = 0$ then $f = 0$ a.e.
\end{theorem}

\begin{proof}
  Define $A_n = \{x ~:~ f(x) > \frac{1}{n}\}$. If $f$ is not equal to zero a.e. then there exists $n$ such
  that $\mu(A_n) > 0$. But then
  \begin{align*}
    0 = \int f \geq \frac{1}{n} \mu(A_n),
  \end{align*}
  a contradiction.
\end{proof}


\begin{theorem}
  If $f$ is real-valued and $\int_A f = 0$ on every measurable set $A$ then $f = 0$ a.e.
\end{theorem}

\begin{proof}
  Let $\eps > 0$. We have
  \begin{align*}
    0 = \int_{\{x ~:~ f(x) > \eps\}} f \geq \eps\mu(\{x ~:~ f(x) > \eps\}),
  \end{align*}
  therefore $\mu(\{x ~:~ f(x) > \eps\}) = 0$. Therefore
  \begin{align*}
    \mu(\{x ~:~ f(x) > 0\})
    &= \mu(\bigcup_{n=1}^\infty \{x ~:~ f(x) > \frac{1}{n}\}) \\
    &\leq \sum_{n=1}^\infty \mu(\{x ~:~ f(x) > \frac{1}{n}\}) = 0.
  \end{align*}
  Similarly,
  \begin{align*}
    0 = \int_{\{x ~:~ f(x) < -\eps\}} f \geq \eps\mu(\{x ~:~ f(x) < -\eps\}),
  \end{align*}
  leading to
  \begin{align*}
    \mu(\{x ~:~ f(x) < 0\}) = 0.
  \end{align*}
\end{proof}



\begin{theorem}[approximation by continuous function]
  Let $f: \R\to \R$ be a Lebesgue-measurable integrable function. Let $\eps > 0$. Then there exists a
  continuous function $g$ with {\it compact support} such that
  \begin{align*}
    \int |f - g| < \eps.
  \end{align*}
\end{theorem}


\begin{theorem}[Egorov's theorem]

\end{theorem}

\begin{proof}
  Define
  \begin{align*}
    A_{nk} = \bigcup_n \{x ~:~ |f_n(x) - f(x)| \geq 1/k \}
  \end{align*}
  Thus $A_{nk}$ is the set of $x$ values for which, at some point beyond $n$, there is a value lying further than $1/k$ from $f$.

  Note that $A_{nk} \to \emptyset$ as $n \to \infty$ with fixed $k$, since $f_n \to f$.


\end{proof}

\section{10. Types of convergence}

\begin{definition}
 A sequence of functions $f_n \to f$ \defn{in measure} if for any $\eps$
 \begin{align*}
   \limn \mu\Big(\Big\{x ~:~ |f_n(x) - f(x)| > \eps\Big\}\Big) = 0.
 \end{align*}
 {\bf Intuition:} while there may not actually be convergence at any point (large values may always occur in the future),
 as time goes on the measure of discrepant points at any given point in time gets arbitrarily small.
\end{definition}

\begin{theorem}
  \begin{enumerate}
  \item If $f_n \to f$ a.e. then $f_n \to f$ in measure
  \item If $f_n \to f$ in measure then there exists a subsequence such that $f_{n_k} \to f$ a.e.
  \end{enumerate}
\end{theorem}

\begin{proof}
  \red{TODO}
\end{proof}

\begin{remark*}
  For a finite measure ($\mu(X) < \infty$) convergence in measure and convergence a.e. are equivalent. To prove
  that i.m. $\implies$ a.e. one can use the DCT on the indicator
  function $\ind_{\{x ~:~ |f_n(x) - f(x)| > \eps\}}$, since for a finite measure it is bounded above by $1$.
\end{remark*}

\begin{theorem}[Chebyshev's inequality]
  \begin{align*}
    \mu(\{x ~:~ |f(x)| \geq a\}) \leq \frac{\int |f|^p}{a^p}
  \end{align*}
    for $p \geq 1$.
\end{theorem}

\begin{proof}
  Let $A = \{x ~:~ |f(x)| \geq a\}$. Then
  \begin{align*}
    \mu(A)
    = \int \ind_A
    \leq \int \frac{|f|}{a},
  \end{align*}
  and this remains true when raising the integrand to a power $p > 1$.
\end{proof}

\section{12. Signed measures}

\begin{definition}
  A \defn{signed measure} $\mu:\mc A \to (-\infty, \infty]$ satisfies countable additivity over a union of disjoint
  sets $A_i \in \mc A$:
  \begin{align*}
    \mu(\bigcup_{n=1}^\infty A_i) = \sum_{n=1}^{\infty} \mu(A_i).
  \end{align*}
  If the measure of this union is finite then the series $\sum_{n=1}^{\infty} \mu(A_i)$ must converge
  absolutely. This implies that the summation is well-defined (it is a theorem that for a series in a complete
  topological space such as the reals, absolute convergence implies unconditional convergence).
\end{definition}

\begin{theorem}
  If $A_n \uparrow A$ then $\limn \mu(A_n) = \mu(A)$.
\end{theorem}

\begin{proof}

\end{proof}

\begin{theorem}
  Let $A = \bigcup_{i=1}^\infty A_i$. Then
  \begin{align*}
    \mu(\bigcup_{i=1}^\infty A_i) = \limn \mu(\bigcup_{i=1}^\infty A_i).
  \end{align*}
\end{theorem}

\begin{proof}
  (\red{TODO} the same as for unsigned measure)
  Let $A = \bigcup_{i=1}^\infty$

  \begin{align*}
    \mu(\bigcup_{i=1}^\infty A_i)
    &:= \mu(\lim_{n\to\infty} \bigcup_{i=1}^n A_i) .
  \end{align*}

\end{proof}

\begin{definition}
  $A$ is a \defn{positive set} if $\mu(B) \geq 0$ for all $B \subseteq A$ (where $A$ and $B$ are in the $\sigma$-algebra).

  $A$ is a \defn{negative set} if $\mu(B) \leq 0$ ...

  $A$ is a \defn{null set} if $\mu(B) = 0$ ...
\end{definition}

\begin{theorem}
  If $\mu(E) < 0$ then there exists a negative set $F \subseteq E$ with $\mu(F) < 0$.
\end{theorem}

\begin{proof}
  If $E$ is a negative set we are done.


  Alternatively, there is a measurable subset of $E$ with positive measure. Let $n_1$ be the smallest positive
  integer such that there exists measurable $E_1 \subset E$ with $\mu(E_1) \geq 1/n_1$
\end{proof}

\begin{theorem}[Hahn decomposition theorem]
  $X$ can be partitioned as $P \cup N$ where $P$ is a positive set and $N$ is a negative set.

  The decomposition is unique up to a null set. I.e. if $X = P' \cup N'$
  then $\mu(P \Delta P') = \mu(N \Delta N') = 0$.
\end{theorem}

\begin{proof}
  \red{TODO}
\end{proof}

\begin{definition}
  Measures $\mu$ and $\nu$ are \defn{mutually singular} if $X$ can be partitioned as $X = E \cup F$ with
  $\mu(E) = 0$ and $\nu(F) = 0$.

  This is written $\mu \perp \nu$.
\end{definition}

\begin{example}
  For example, let $X = [0, 1]$ and let $\mu$ and $\nu$ be Lebesgue measure restricted to $[0,\frac{1}{2}]$
  and $[\frac{1}{2} , 1]$ respectively. Then $E = (\frac{1}{2} , 1]$ and $F = [0,\frac{1}{2}]$ shows
  that $\mu \perp \nu$.
\end{example}

\begin{theorem}[Jordan decomposition theorem]
  If $\mu$ is a signed measure then there exist positive measures $\mu^+$ and $\mu^-$ such
  that $\mu = \mu^+ - \mu^-$ and $\mu^+ \perp \mu^-$. This decomposition is unique.
\end{theorem}

\begin{definition}
  The measure $|\mu| := \mu^+ + \mu^-$ is called the \defn{total variation measure} of $\mu$.

  $|\mu|(X)$ is called the total variation of $\mu$.
\end{definition}

The core example motivating these definitions and decomposition theorems is

\begin{example}
  Let $m$ be Lebesgue measure and define
  \begin{align*}
    \mu(A) = \int_A f \d m.
  \end{align*}
  Then $\mu$ is a measure. If $f$ takes positive and negative values then $\mu$ is a signed measure.

  The \defn{Hahn decomposition} theorem states that the measure space $X$ may be partitioned as $X = P \cup M$ where
  $P$ is a positive set (all measurable subsets have non-negative measure) and $M$ is a negative set (all
  measurable subsets have non-positive measure).

  The obvious decomposition is
  \begin{align*}
    M &= \{x ~:~ f(x) \leq 0\} \\
    P &= \{x ~:~ f(x) > 0\},
  \end{align*}
  and this decomposition is unique except of course that $N = \{x ~:~ f(x) = 0\}$ is a null set (all measurable
  subsets have measure zero) and different ways of apportioning $N$ to $P$ and $M$ do not affect their measure
  (and thus their status as positive and negative sets respectively).

  The \defn{Jordan decomposition} theorem states that $\mu$ can be written as $\mu = \mu^+ - \mu^-$, where
  \begin{align*}
    \mu^+(A) = \int_A f^+ ~~~~~~~ \text{and} ~~~~~~~ \mu^-(A) = \int_A f^-.
  \end{align*}
  Recall that $f^+ := \max(f, 0)$ and $f^- := \min(f, 0)$.
\end{example}

\begin{intuition}
  So in other words, we define a measure $\mu$ by using a function $f:X \to \R$ to specify how the measure
  weights different subsets of $X$.

  The construction is natural but in general it gives rise to a signed measure $\mu$. We then note that there
  are two natural decompositions associated with signed measures:

  The Hahn decomposition partitions the measure space into one subset with purely positive measure and one with
  purely negative measure.

  The Jordan decomposition states that the measure can be written as the difference between two positive
  measures: one reflecting weighting of the input space due to positive values of the weighting function, and
  one reflecting weighting due to negative values of the weighting function.
\end{intuition}
