\section{Billingsley Section 1}
[Berkeley 202a]
[Billingsley - Probability \& Measure]

Why does he say ``closed under countable unions and intersections.​''?

  Billingsley p.19:
  \begin{quote}
    ...require a collection that contains the intervals and is closed under countable unions and intersections.
    Note that a singleton $\{x\}$ is a countable intersection of intervals:
    \begin{align*}
      \bigcap_{n=1}^\infty \Big(x -\frac{1}{n}, x\Big] = \{x\}.
    \end{align*}
  \end{quote}


\begin{itemize}
\item $\Omega = [0, 1]$
\item $\om \in \Omega$
\item $d_n(\om) \in \{0, 1\} = $ $n$-th digit in binary expansion of $\om$
\item Rademacher function $r_n(\om) = 2d_n(\om) - 1 \in \{-1, 1\}$
\end{itemize}

\subsection{Weak Law of Large Numbers}

Define the partial sum $s_n(\om) = \sum_{i=1}^n r_i(\om)$, i.e. the number of $1$s minus the number of $0$s in
the first $n$ digits of the binary expansion of $\om$. (The displacement of the random walk after $n$ steps.)

\begin{lemma}
  \begin{align*}
    \int_0^1 s_n(\om)^2 \d\om = n
  \end{align*}
\end{lemma}

I.e., viewed as a sequence of $n$ coin tosses yielding $-1$ or $+1$, the variance (expected squared distance
from mean) of their sum is $n$.

\begin{proof}
  Note that $s_n(\om)^2 = \sum_{i=1}^n r_i(\om)^2 - 2\sum_{i<j}r_i(\om)r_j(\om)$. Integrating over $[0, 1]$ we have
  \begin{align*}
    \int_0^1 s_n(\om)^2 \d\om
    &= \sum_{i=1}^n \int_0^1 r_i(\om)^2 \d\om - 2\sum_{i<j}\int_0^1r_i(\om)r_j(\om) \d\om \\
    &= \sum_{i=1}^n \int_0^1 1 \d\om - 0 \\
    &= n.
  \end{align*}
  We used there the fact that $\int_0^1r_i(\om)r_j(\om) \d\om = 0$ for $i < j$, i.e that the Rademacher
  functions are orthogonal. An argument for this is that as we move through a rank $i$ dyadic
  interval, $r_i(\omega)$ is constant (either $-1$ or $+1$) while at rank $j$ below, $r_j(\omega)$ flickers
  between $-1$ and $+1$, spending an equal amount of time in each.
\end{proof}


\begin{lemma}[Markov's Inequality]
  Let $f: [0, 1] \to \R^+$ be a step function. Then
  \begin{align*}
    P\Big(\Big\{x: f(x) \geq \alpha\Big\}\Big) \leq \frac{1}{\alpha}\int_0^1 f(x) \dx.
  \end{align*}
\end{lemma}




\begin{intuition}
  Think of the statement in rearranged form:
  \begin{align*}
    \alpha P\Big(\Big\{x: f(x) \geq \alpha\Big\}\Big) \leq \int_0^1 f(x) \dx.
  \end{align*}



  \includegraphics[width=400pt]{img/analysis--real-analysis--measure-theory--weak-law-of-large-numbers-c9c2.png}

  If $X \sim \Unif(0, 1)$ then the RHS is $\E[X]$.

\end{intuition}


\begin{proof}
  [me]

  Clearly
  \begin{align*}
    \int_{f(x) \geq \alpha} f \leq \int_{[0, 1]} f.
  \end{align*}
  Therefore
  \begin{align*}
    \int_{f(x) \geq \alpha} \alpha \leq \int_{[0, 1]} f
  \end{align*},
  or equivalently
\begin{align*}
  \alpha \int \textbf{1}_{f(x) \geq \alpha} \leq \int_{[0, 1]} f,
\end{align*}
which is the same thing as
  \begin{align*}
    \alpha P\Big(\Big\{x: f(x) \geq \alpha\Big\}\Big) < \int_0^1 f(x) \dx.
  \end{align*}
\end{proof}

\begin{theorem}[Weak Law of Large Numbers]
  Fix an $\epsilon > 0$. Then
  \begin{align*}
    \lim_{n \to \infty}P\Big(\Big\{\om: \frac{1}{n}\big|\sum_{i=1}^n r_i(\om)\big| \geq \epsilon\Big\}\Big) = 0.
  \end{align*}
\end{theorem}

In other words: we move through all the $\om \in [0, 1]$. For a given $\om \in [0, 1]$, compare the number
of $0$s and $1$s in the first $n$ digits of the binary expansion, and record the excess as a proportion of $n$;
this is $\frac{1}{n}|s_n(\om)|$. The theorem states that for all $\epsilon > 0$ the probability measure
associated with the set of $\om$s for which $\frac{1}{n}|s_n(\om)| > \epsilon$ goes to $0$ as $n \to \infty$.

\begin{proof}
  Fix an $\epsilon > 0$. We square both sides of the inequality, instead of working with the absolute value. So
  what we want to show is that $P\big(\big\{\om: s_n^2(\om) \geq n^2\epsilon^2\big\}\big) \to 0$
  as $n \to \infty$.

  It would be nice to find an expression for this probability measure as a function of $n$. However, what we'll
  do is find an upper bound: that will suffice also.

  Note that $s_n(\om)$ is a step function (and so $s_n^2(\om)$ is also):

\includegraphics[width=200pt]{img/analysis--real-analysis--measure-theory--weak-law-of-large-numbers-c049.png}


By Markov's inequality / ``Shaded Area lemma​'' we have
\begin{align*}
  n^2\epsilon^2P\Big(\Big\{\om: s_n^2(\om) \geq n^2\epsilon^2\Big\}\Big) \leq \int_0^1 s_n^2(\om) \d\om = n
\end{align*}
and therefore
\begin{align*}
  P\Big(\Big\{\om: s_n^2(\om) \geq n^2\epsilon^2\Big\}\Big) \leq  \frac{1}{n\epsilon^2},
\end{align*}
which proves the desired result since it shows that the probability measure is bounded above by a quantity that
goes to $0$ as $n \to \infty$.
\end{proof}

\subsection{Strong Law of Large Numbers}

\red{TODO} Relation of Borel's normal number theorem to SLNN.

\begin{definition}[negligible, null set]
  A set $A$ is \defn{negligible} if, for any $\epsilon > 0$, it can be covered by a finite or countable
  union $\bigcup_k I_k$ of intervals with $\sum_k |I_k| < \epsilon$.
\end{definition}

Recall the weak law of large numbers:
  \begin{align*}
    \lim_{n \to \infty}P\Big(\Big\{\om: \frac{1}{n}\big|s_n(\om)\big| \geq \epsilon\Big\}\Big) = 0.
  \end{align*}

\begin{definition}[Normal numbers]
  Define the set of \defn{normal numbers} to be
  \begin{align*}
    N = \Big\{\om ~:~ \lim_{n\to\infty} \frac{1}{n}s_n(\om) = 0 \Big\}.
  \end{align*}
\end{definition}

\begin{theorem*}[Borel's normal number theorem]
  $N^c = \R \setminus N$ is negligible.
\end{theorem*}

\begin{intuition}
  Note that the set of normal $\om$ can be written as the set of $\om$ that

  ``eventually stay within $1$​'' AND
  ``eventually stay within $1/2$'' AND
  ``eventually stay within $1/3$'' AND
  ``eventually stay within $1/4$'' ...
  \begin{align*}
    N = \bigcap_{k=1}^\infty \bigcup_{m=1}^\infty\bigcap_{n \geq m} \big\{ \om ~:~ \big|\frac{1}{n}S_n(\om)\big| < \frac{1}{k} \big\}.
  \end{align*}

  Visualize the $s_n$ sequence of a non-normal number $\om$, stretching off to infinity. However far we’ve
  gone, there will always be another point further along at which an excursion of the random walk sticks out
  further than $\eps$. But despite the fact that this must always happen, it’s less and less likely the further
  we go. The fact that it must always happen again corresponds to the fact that we can write the event as a
  countable union: (happened by this generation) union with (happened at the next generation), etc. But at the
  same time, since it’s getting harder and harder, for any given $\gamma >0$ we can find some generation $m$
  beyond which the union sums to less than $\gamma$. nevertheless , the event is equal to the union beyond that
  point, since the departures must always keep occurring (otherwise the number would be normal). So the union
  doesn’t have to include earlier generations.

  This is why the complement of the normal numbers is negligible. Perhaps it’s typical of negligible sets that
  they correspond to an event that must always occur one more time, and yet get ever less and less likely?
\end{intuition}

\begin{proof}
  Let $(\eps_n)$ be a sequence that converges to zero, and define a sequence of sets $(A_n)$, where
  \begin{align*}
    A_n = \Big\{\om : \Big|\frac{1}{n} s_n(\om)\Big| \geq \eps_n\Big\}.
  \end{align*}
  (We can think of $A_n$ as the set of $\om$ whose binary expansions are ``not normal so far​''.)

  Note that, for any given $m$, we have the following: a number that stays inside $\eps_n$ for ever is normal:
  \begin{align*}
    \Big(\bigcap_{n=m}^\infty A_n^c\Big) \subset N.
  \end{align*}
  Equivalently, a non-normal number must stray outside $\eps_n$ at some point:
  \begin{align*}
    N^c \subset \Big(\bigcup_{n=m}^\infty A_n\Big).
  \end{align*}
  Recall that our aim is to cover $N^c$ with a countable union of intervals, where the total length of the
  intervals is arbitrarily small (an ``efficent covering​​''). If we can show that the $A_n$ meet that
  description then we are done.

  Recall that $s_n$ is a step function such that, if $\om \in A_n$ then $\om' \in A_n$ for every $\om'$ in the
  same rank-$n$ dyadic interval as $\om$. Therefore each set $A_n$ is a finite disjoint
  union $\bigcup_{k}I_{nk}$ of intervals, and $P(A_n) = \sum_k |I_{nk}|$.

  So what we need to do is show that, for any given $\gamma > 0$, there exists a sequence $(\eps_n)$ converging
  to zero, and an $m$, such that $\sum_{n=m}^\infty P(A_n) < \gamma$.

  At this point, we need to find an expression for an upper bound on $P(A_n)$ in terms of $n$ and $\eps_n$.
  From the lemma, we have
  \begin{align*}
    P(A_n) \leq \frac{3}{n^2\eps_n^4},
  \end{align*}
  so we would like to find $(\eps_n)$ and $m$ such that
  \begin{align*}
    \sum_{n=m}^\infty \frac{3}{n^2\eps_n^4} < \gamma.
  \end{align*}
  To do so, we need only choose $(\eps_n)$ so that the series $\sum_nn^{-2}\eps_n^{-4}$
  converges: $\eps_n = n^{-1/8}$ will do. Then, since the series converges, there exists an $m$ such that the
  tail sums to less than $\gamma$, as required.
\end{proof}

\begin{lemma}
  Let $A_n = \Big\{\om : \Big|\frac{1}{n} s_n(\om)\Big| \geq \eps\Big\}$.

  For all $n \in \N$, we have (by taking the 4th power of both sides of the inequality and applying Markov's
  inequality)
  \begin{align*}
    P(A_n) \leq \frac{1}{n^4\eps^4} \int_0^1 s_n^4(\om) \d\om,
  \end{align*}
  and (by considering integrals of products of four Rademacher functions)
  \begin{align*}
    \int_0^1 s_n^4(\om) \d\om \leq 3n^2.
  \end{align*}
  Therefore
  \begin{align*}
    P(A_n) \leq \frac{3}{n^2\eps^4}.
  \end{align*}
\end{lemma}

\newpage
\subsection{An interval of  positive length is not negligible}

\begin{definition*}[length of an interval]
  The \defn{length} of $(a, b)$ is $|(a, b)| = b - a$.
\end{definition*}

\begin{theorem*}
  Let $I$ be an interval of positive length and let $I_1, I_2, \ldots$ be intervals.
  \begin{enumerate}
  \item If $\bigcup_k I_k \subseteq I$ (disjoint) then $\sum_k |I_k| \leq |I|$
  \item If $\bigcup_k I_k \supseteq I$ then $\sum_k |I_k| \geq |I|$. I.e. no cover of $I$ is negligible.
  \end{enumerate}
  A corollary is that if $\bigcup_k I_k = I$ then $\sum_k |I_k| = |I|$.
\end{theorem*}

\begin{proof}
  Let $I = (a, b)$ and let $I_k = (a_k, b_k)$ for all $k$.

  First, we show that if $\bigcup_k I_k \subseteq I$ (with the $I_k$ disjoint) then $\sum_k |I_k| \leq |I|$.

  There are two cases:
  \begin{enumerate}
  \item {\bf Finite cover}:

    The claim is that for any collection of $n$ disjoint intervals, if $\bigcup_{k=1}^n I_k \subseteq I$
    then $\sum_{k=1}^n |I_k| \leq |I|$.

    This is clearly true for a collection of intervals of size $n = 1$.

    Assume it's true for any collection of intervals of size $n-1$, and consider a collection of $n$ disjoint
    intervals with $\bigcup_{k=1}^n I_k \subseteq I$.

    Label the intervals $I_1, \ldots, I_n$, sorted by their left endpoint in ascending order. Note that the
    union of the first $n-1$ intervals is contained within $(a, a_n)$ and that the $n$-th interval has
    length $b_n - a_n \leq b - a_n$. Thus we have
    \begin{align*}
      \sum_{k=1}^n |I_k|
      &= \sum_{k=1}^{n-1}|I_k| + |I_n| \\
      &\leq (a_n - a) + (b - a_n) \\
      &= b - a.
    \end{align*}
    Therefore it is true for all $n$ by induction.

  \item {\bf Infinite cover}:

    The claim is that for any countably infinite collection of $n$ disjoint intervals,
    if $\bigcup_{k=1}^\infty I_k \subseteq I$ then $\sum_{k=1}^\infty |I_k| \leq |I|$.

    Consider an infinite collection of intervals satisfying $\bigcup_{k=1}^\infty I_k \subseteq I$.

    Note that for every finite subcollection of size $n$ we have $\sum_{k=1}^n |I_k| \leq |I|$ by the finite case.

    Therefore $\sum_{k=1}^\infty |I_k| = \sup \sum_{k=1}^n |I_k| \leq |I|$ where the supremum is over the set
    of all finite subcollections. Since this set is non-empty, the supremum is a finite positive number.
  \end{enumerate}
  ~\\~\\
  Finally, we show that if $\bigcup_k I_k \supseteq I$ then $\sum_k |I_k| \geq |I|$.
  Again, there are two cases:
  \begin{enumerate}
  \item {\bf Finite cover}:

    The claim is that for any collection of $n$ intervals, if $\bigcup_{k=1}^n I_k \supseteq I$
    then $\sum_{k=1}^n |I_k| \geq |I|$.

    In other words, that the total length of a finite cover of $I$ is bounded below by $|I| > 0$.

    Again, it's obvious for a cover comprising a single interval ($n=1$).

    Assume it's true for any cover comprising $n-1$ intervals, and consider a cover comprising $n$ intervals.

    Again, label the intervals $I_1, \ldots, I_n$, sorted by their left endpoint in ascending order.

    Note that the first $n-1$ intervals cover the interval $(a, b_{n-1})$ and that $|I_n| \geq b - b_{n-1}$.
    Thus we have
    \begin{align*}
      \sum_{k=1}^n |I_k|
      &= \sum_{k=1}^{n-1}|I_k| + |I_n| \\
      &\geq (b_{n-1} - a) + (b - b_{n-1}) \\
      &= b - a.
    \end{align*}

  \item {\bf Infinite cover}\\

    The claim is that for any infinite cover $\bigcup_{k=1}^\infty I_k \supseteq I$ we have $\sum_{k=1}^\infty |I_k| \geq |I|$.

    Consider a countably infinite cover of $I$.

    We might think that we could make an argument analogous to the one above:

    Note that for every finite subcover of size $n$ we have $\sum_{k=1}^n |I_k| \geq |I|$.

    Therefore $\sum_{k=1}^\infty |I_k| = \inf \sum_{k=1}^n |I_k|  \geq |I|$ where the infimum is over the set of all finite subcovers.

    However, we have to show that a finite subcover exists; otherwise the infimum would be $+\infty$.

    So what we do is construct a closed interval $[a + \eps, b]$ which is covered by a countably infinite open
    cover. Since that closed interval is compact from Heine-Borel, there exists a finite open subcover.

    \begin{quote}
      I think your second statement is true except for an infinite open cover of (a, b] which has finite total
      measure, but no finite subcovers. Such a cover does exist, and it’s the the object I was (clumsily)
      trying to refer to. So I think your second statement is false, but not nonsense, as it fails for the same
      reason that the second proof is more difficult.
    \end{quote}
  \end{enumerate}
\end{proof}


\begin{theorem}
  Every proper open subset of $\R$ is a countable union of disjoint open intervals and open rays.
\end{theorem}
\begin{proof}
  HW2 Q1 (uses an equivalence relation to partition the open set), Billingsley Example 2.6 (uses an
  uncountable union of intervals with rational endpoints which must contain duplicates).
\end{proof}


\begin{proof}
  Let $\ms A$ be a finite class of $n$ sets. Taking complements gives $2n$ sets.

  In the first iteration, each set can be involved in $2n$ unions and $2n$ intersections, for a total of $4n$
  new sets, which becomes $8n$ on taking complements.

  So after $k$ generations, we


\end{proof}
\subsection{Cantor sets}

\subsubsection{Middle-thirds Cantor set}

\begin{definition*}
We start at generation $0$ with
\begin{align*}
  C_0 = \Big[0, 1\Big].
\end{align*}

At generation $1$ we have removed the middle-third open interval $(\frac{1}{3}, \frac{2}{3})$,
leaving
\begin{align*}
  C_1 = \Big[0, \frac{1}{3}\Big] \cup \Big[\frac{2}{3}, 1\Big].
\end{align*}

At generation $2$ we have removed the middle-third open interval from each remaining interval, leaving
\begin{align*}
  C_2 = \Big[0, \frac{1}{9}\Big] \cup \Big[\frac{2}{9}, \frac{3}{9}\Big] ~~\cup~~ \Big[\frac{6}{9}, \frac{7}{9}\Big] \cup \Big[\frac{8}{9}, 1\Big].
\end{align*}
The Cantor set is defined to be the set of all points that are never removed:
\begin{align*}
  C := \bigcap_{n=1}^\infty C_n.
\end{align*}
Since the sets form a chain $C_0 \supset C_1 \cdots$, we can also write
\begin{align*}
  C = \lim_{n\to\infty} C_n.
\end{align*}
\end{definition*}

\begin{theorem*}
  The middle-thirds Cantor set:
  \begin{enumerate}
  \item is uncountable;
  \item is compact;
  \item every point is a limit point;
  \item contains no intervals;
  \item has zero measure;
  \end{enumerate}
\end{theorem*}
\begin{proof}[zero measure]
At generation $n$ we have $2^n$ closed intervals each of length $(1/3)^n$. Therefore
\begin{align*}
  \mu(C_n) = (2/3)^n
\end{align*}
and therefore
\begin{align*}
  \mu(C) = 0.
\end{align*}
\end{proof}

\begin{intuition*}
  The closed intervals that exist at generation $n$ of the Cantor set constructions become singletons in the
  limit $n \to \infty$. For example $[0,\frac{1}{3}]$ becomes $[0,\frac{1}{9}]$, etc. In the limit this gives
  rise to a singleton:
  \begin{align*}
    \lim_{n\to\infty} \Big[0, \frac{1}{3^n}\Big] = \{0\}.
  \end{align*}
  However, this singleton is arbitrarily close to another singleton. In fact, every point in the Cantor set is
  a limit point.

  Somehow however, points other than endpoints are in the Cantor set. For example, $1/4$.
\end{intuition*}

\subsubsection{Generalized Cantor sets}
    Let $a \in (0, 1)$. The Cantor set of measure $a$ is formed as follows:

    Note that $\sum_{n=1}^\infty \frac{1 - a}{2^n} = 1 - a$. So we will design an algorithm that
    removes $\frac{1-a}{2^n}$ at each iteration, for $n=1, 2, \ldots$. Note that at the start of iteration $n$
    there are $2^{n-1}$ intervals. So we remove
    \begin{align*}
      \frac{1-a}{2^{n}}\cdot\frac{1}{2^{n-1}} = \frac{1 - a}{2^{2n - 1}}
    \end{align*}

    from each interval.

    For example, to create a set with measure $a = \frac{1}{2}$,
    remove $\frac{1}{4} + 2(\frac{1}{16}) + 4(\frac{1}{64}) + \cdots = \frac{1}{2}$.


\subsection{An open set can be written as a countable union of disjoint open intervals}

\begin{theorem}
  Let $G \subset \R$. Then $G$ can be written as a {\it countable} union of disjoint open intervals.
\end{theorem}

\begin{proof}

\end{proof}

\subsection{sigma-algebras, Borel sets}

\url{https://en.wikipedia.org/wiki/Outcome_(probability)}


An \defn{outcome} is an atomic, lowest-level, result of an experiment/process. Outcomes are mutually exclusive.

An \defn{event} is a set of \defn{outcomes} that we assign probability to. It is a subset of $\Omega$. Events are not mutually
exclusive: ``greater than 0.5?​'' and ``greater than 0.6?​'' are both events and, for a given outcome, both events
may ``occur​''.

An \defn{algebra} is a collection of events. So an algebra contains all the things we might assign probability to.
Furthermore, an algebra must be closed under complementation, union and intersection ( ``not​'', ``or​'', and ``and​'').

A $\sigma$-\defn{algebra} is an algebra that is closed under countably infinite unions and intersections.




\begin{definition*}[$\sigma$-algebra]
  An \defn{algebra} in $\Omega$ is a collection of subsets of $\Omega$ that
  \begin{enumerate}
  \item contains $\emptyset$ and $\Omega$
  \item is closed under complements
  \item is closed under {\it finite} unions and intersections
  \end{enumerate}

  It is a $\sigma$-\defn{algebra} if it is additionally closed under {\it countable} unions and intersections.
\end{definition*}

\begin{definition}
  A ($\sigma$-) algebra \defn{generated} by a collection of subsets is the smallest ($\sigma$-) algebra of which that
  collection is a subset.
\end{definition}


\begin{quote}
  the intersection of all fields in $\Omega$ containing $\ms A$.
\end{quote}

Here, ``containing​'' means subset inclusion.

An in fact, ``in​'' here means neither set membership nor subset inclusion. It is used in a more technical sense
to refer to fields whose sets are subsets of $\Omega$ (i.e. fields that are elements of the powerset
of $\Omega$). Bass uses ``on​'' for this: ``a field on $\Omega$​''.

\begin{definition}[Borel $\sigma$-algebra, Borel set]
  A \defn{Borel} $\sigma$-\defn{algebra} is the $\sigma$-algebra generated by the open sets.

  A \defn{Borel set} is a subset of $\Omega$ that is an element of a Borel $\sigma$-algebra.
\end{definition}

A Borel $\sigma$-algebra contains is generated by open sets (and so equivalently by closed sets). But
the $\sigma$-algebra contains singletons, half-closed intervals, etc.

\begin{theorem}
  The Borel $\sigma$-algebra on $\R$ can be generated by the following sets
  \begin{enumerate}
  \item $\ms I_1 = \{(a, b) ~:~ a, b \in \R\}$
  \item $\ms I_2 = \{[a, b] ~:~ a, b \in \R\}$
  \item $\ms I_3 = \{(a, b] ~:~ a, b \in \R\}$
  \item $\ms I_4 = \{[a, \infty) ~:~ a \in \R\}$
  \item $\ms I_5 = \{(p, q) ~:~ p, q \in \Q\}$
  \end{enumerate}
\end{theorem}

\begin{proof}
  Let $\ms O$ be the collection of open subsets of $\R$, so that $\ms B = \sigma(\ms O)$.
  \begin{enumerate}
  \item The key here is that every open subset of $\R$ is a countable union of open intervals. So the collection of
    open sets is automatically in the $\sigma$-algebra generated by the open intervals, so you can't ``get
    anything new​'' from them.

    Let $\ms I_1 = \{(a, b) ~:~ a, b \in \R\}$. We want to show that $\sigma(\ms I_1) = \sigma(\ms O)$.

    In one direction, every element of $\ms I_1$ is open, so clearly $\sigma(\ms I_1) \subseteq \sigma(\ms O)$.

    For the other direction, let $X \in \ms O$ be an open subset of $\R$. Then $X$ is a countable union of open
    intervals (i.e. finite intervals and open rays). Every finite interval is in $\ms I_1$. But open ray are
    also countable unions of finite intervals: $(-\infty, a) = \bigcup_n^\infty (a-n, a)$
    and $(a, \infty) = \bigcup_n^\infty (a, a + n)$. Therefore $X \in \sigma(\ms I_1)$, i.e. every open set is
    in the $\sigma$-algebra generated by open intervals. This is equivalent to the
    statement $\ms O \subseteq \sigma(\ms I_1)$, i.e. the collection of all open sets is a subset of
    that $\sigma$-algebra. Therefore $\sigma(\ms O) \subseteq \sigma(\sigma(\ms I_1)) = \sigma(\ms I_1)$.

  \item We reduce this to (1) by showing that we can make $(a, b)$ from $[a, b]$.

    Let $\ms I_2 = \{[a, b] ~:~ a, b \in \R\}$. We want to show that $\sigma(\ms I_2) = \sigma(\ms O)$.

    To show $\sigma(\ms I_2) \subseteq \sigma(\ms O)$, note that for $a < b$
    \begin{align*}
      [a, b] &= \bigcap_{n=1}^\infty (a - n^{-1}, b + n^{-1}).
    \end{align*}
    Therefore $[a, b] \in \sigma(\ms I_1)$ for all $a, b \in \R$, hence $\sigma(\ms I_2) \subseteq \sigma(\ms I_1) = \sigma(\ms O)$.

    To show $\sigma(\ms O) \subseteq \sigma(\ms I_2)$, note that for $a < b$ and $n_0 \geq 2/(b - a)$
    \begin{align*}
      (a, b) &= \bigcup_{n=N_0}^\infty [a + n^{-1}, b - n^{-1}].
    \end{align*}
    Therefore $(a, b) \in \sigma(\ms I_2)$ for all $a, b \in \R$ hence $\ms I_1 \subseteq \sigma(\ms I_2)$,
    hence $\sigma(\ms I_1) = \sigma(\ms O) \subseteq \sigma(\ms I_2)$. But we have already shown
    that$\sigma(\ms I_1) = \sigma(\ms O)$, therefore $\sigma(\ms O) \subseteq \sigma(\ms I_2)$.
  \end{enumerate}
\end{proof}

\subsection{Bass 3. Measures}

Let $\Omega$ be a set and $\mc A$ a $\sigma$-algebra on $\Omega$.

A \defn{measure} is a function $\mu:\mc A \to [0, \infty]$ that is \defn{countably additive} (CA; measure of disjoint union equals sum of measures).

CA has various implications which make $\mu$ behave in unsurprising ways:
\begin{enumerate}
\item It's finitely and countably \defn{subadditive} (measure of union does not exceed sum of measures)
\item Measure of limiting sets equals limit of measures (e.g. if $A_i \uparrow A$ then $\mu(A) = \lim_n \mu(A_i)$)
\end{enumerate}

\begin{intuition*}
  The function $\mu$ is a map from sets to reals. So in principle it could assign whatever real values it wants
  to whatever sets. E.g. for disjoint $A, B$ it could assign a value to $\mu(A \cup B)$ that is completely
  different from $\mu(A) + \mu(B)$.

  In fact, however, measures treat sets as an aggregate of points. I think that everything is perfectly
  intuitive except that countably infinite unions might not work as expected.

  In other words, $\mu$ acts exactly as one would expect: as if it's applying a uniform layer of paint to each
  subset: the total amount of paint used to paint a union of disjoint sets is the sum of the paint applied to
  each set in the union.

  But CA means the additivity is retained even when there are infinitely many sets in the union. An example of CA
  failing to hold is densities of finite subsets of the natural numbers: the density of the singleton $\{1\}$ as
  a proportion of the natural numbers is naturally defined to be $0$ (the limit of the density in a finite sample
  as the sample size tends to infinity). But the density of the countable union of all singletons is $1$, which is
  not the sum of the densities.
\end{intuition*}

A measure is \defn{finite} if $\mu(\Omega) < \infty$, and $\sigma$\defn{-finite} if there exists a countable partition
of $\Omega$ with each subset in the partition having finite measure.

$(\Omega, \mc A, \mu)$ is a \defn{measure space}.

A subset $A \subset \Omega$ (not necessarily in $\mc A$) is a \defn{null set} if $A$ is a subset of some element
of $\mc A$ which has zero measure. $(\Omega, \mc A, \mu)$ is a \defn{complete} measure space if all null sets are
in $\mc A$. The \defn{completion} of $\mc A$ is the smallest complete $\sigma$-algebra $\bar{\mc A}$ containing
$\mc A$ such that $(\Omega, \bar{\mc A}, \bar \mu)$ is a complete measure space, where $\bar \mu$ is an
extension of $\mu$ from $\mc A$ to $\bar{\mc A}$.

A \defn{probability measure} is a measure where $\mu(\Omega) = 1$.

\section{Non-measurable sets}

\newpage
\begin{example}[Random set]

  Suppose we can construct a set $T \subseteq [0, 1]$ by accepting each element $x \in [0, 1]$ independently
  with probability $0.5$.

  Suppose for a contradiction that $T$ is Lebesgue measurable. Let $\eps > 0$. Then there exists an open
  set $O \supseteq T$ such that $m(O \setminus T) < \eps$.

  Specifically the density of $T$ in $O$ is
  \begin{align*}
    \frac{m(T \cap O)}{m(O)} > \frac{m(T)}{m(T) + \eps},
  \end{align*}
  which can be made arbitrarily high, say 0.99, by taking $\eps$ small.

  We can also write $O$ as a countable union of open intervals $O = \bigcup_{i=1}^\infty I_i$, and thus write
  the density of $T$ in $O$ as a weighted average of local densities in each interval $I_i$:
  \begin{align*}
    \frac{m(T \cap O)}{m(O)} = \sum_{i=1}^\infty \frac{m(I_i)}{m(O)} \frac{m(T_i)}{m(I_i)},
  \end{align*}
  where $T_i = T \cap I_i$ are the points of $T$ ``covered​'' by $I_i$.

  But (presumably!) the density of $T$ is $0.5$ in every interval, or at least, every one of the countably many
  intervals with rational endpoints. Therefore $m(T_i)/m(I_i) = 0.5$ and we have
  \begin{align*}
    0.99 = \frac{m(T \cap O)}{m(O)} = 0.5\sum_{i=1}^\infty \frac{m(I_i)}{m(O)} = 0.5.
  \end{align*}
  This is a contradiction, proving that such a $T$ is not Lebesgue measurable.
\end{example}


\begin{example}[Irrational rotation orbits]
  Consider the unit circle $S^1$. We would like to define a countably additive, translation invariant,
  function $\mu:\powerset(S^1) \to [0, \infty]$, which we will call a ``measure​''.

  Let $r$ be irrational and define the translation $\tau(x) = (x + r) \mod 1$.

  Recall that for all $x \in S_1$, the orbit of $x$ under $\tau$ is non-periodic and dense.

  (Note that non-periodicity of the orbit is quite a striking property: it never hits a point twice!)

  Define a set $A_0$ containing one point from every distinct orbit. (Uses some form of Axiom of Choice)

  Define $A_n = \tau^n(A_0)$.

  Note that $A_i$ and $A_j$ are disjoint for all $i \neq j$. (Suppose they had a point in common. Then that
  point would either be a member of two distinct orbits, or it would be a point that occurs twice in the same
  orbit. Neither is possible.)

  Therefore, if the $A_i$ are measurable, then
  \begin{align*}
    \mu\Big(\bigcup_{i=0}^\infty A_i\Big) = \sum_{i=0}^\infty \mu(A_i).
  \end{align*}
  But note that $\bigcup_{i=0}^\infty A_i = S^1$, therefore the LHS equals 1.

  However, $\tau$ is a translation, and therefore $\mu(A_i) = \mu(A_0)$ for all $i$, if $\mu$ is translation
  invariant.

  But this is a contradiction, since if $\mu(A_0) = 0$ then we have $1 = \sum_{i=0}^\infty 0 = 0$, and
  if $\mu(A_0) > 0$ then we have $1 = \infty$.

  Therefore no countably additive and translation invariant $\mu$ can be defined on the $A_i$.

  Counter-examples like this motivate the restriction of measure to $\sigma$-algebras.
\end{example}


\section{Theorems covered}

\begin{enumerate}
\item Bass 3.5
\begin{mdframed}
\includegraphics[width=400pt]{img/analysis--berkeley-202a--billingsley-section-1--theorems-covered-0fb2.png}
\end{mdframed}
\item 4.6 Caratheodory's extension theorem
\item Littlewood's three principles
  \begin{enumerate}
  \item {\it Any measurable set is almost an open set / a finite union of open intervals} (Bass 4.14) \\
    Because inner and outer measurabe coincide for a measurable set
  \item {\it Any measurable/integrable function is almost continuous} (Bass 5.2) \\
    Lusin's theorem, density on $L^1$ of continuous fns.
  \item Every convergent sequence of fns is nearly uniformly convergent if $\mu(X) < \infty$. \\
    Egorov's thm
  \end{enumerate}
\item Bass 5.6: continuous function on metric space is measurable
\item measurable functions closed under various combinations and limits
\item {\it Any measurable function is almost continuous} (Bass 5.2), Lusin's theorem
\item MCT, DCT, Fatou's lemma
\item Vitali covering lemma
\item Bass 7.5: $\int \sumninf f_n = \sumninf \int f_n$ for $f_n \geq 0$
\item Bass 8 when is a function zero a.e.
\item Bass 8.4. Approximation result: for integrable $f$ there exists continuous $g$ with compact
  support $\int |f - g| < \eps$
\item Folland section 2.4 Egorov's theorem, modes of convergence, Cauchy in measure
\item Folland 2.32 If $f_n \to f$ in $L^1$ then there exists a subsequence which converges a.e.
\item Folland 2.33 Egorov's thm: finite measure space, sequence $f_n \to f$ a.e. then there $f_n \to f$ uniformly
  on an arbitrarily large strict subset.
\item Signed measures: Bass 12.4, 12.5 Hahn decomposition theorem, 12.8 Jordan decomposition thm
\item Bass 13: Radon-Nikodym: for two absolutely continuous measures, there exists a function $f$ such that
  $\nu$ can be written as $\nu(A) = \int_A f$.
\item Folland section 3.4
  \begin{enumerate}
  \item Covering lemma
  \item average of function, $A_r f$ is continuous for locally-integrable $L^1_{loc}$,
  \item maximal theorem: limit on measure of points with large maximal fn value
  \item 3.18 limit of average in balls is fn value at point
  \item measure of complement of Lebesgue set is zero
  \end{enumerate}
\item Topology
  \begin{enumerate}
  \item Equivalence of ($f$ is continuous) and (maps neighborhood into ndb)
  \item Equivalence of open in metric space concepts
  \item Any compact Hausdorff t.s. is normal
  \item Bass 20.31 Uryohn's lemma (normal spaces have plenty of cont fns): if $X$ is normal and $E$ and $F$ disjoint closed then there exists a continuous
    fn $f: X \to [0, 1]$ with $f|_E = 0$ and $f|_F = 1$.
  \item Tietze extension: normal t.s. $F \subset X$ closed, $f: F \to [a, b]$ continuous. There exists
    continuous $\bar f: X \to [a, b]$ s.t. $\bar f|_f = f$.
  \item uniform limit of continuous fns is continuous
  \item continuous image of compact set is compact
  \item Bass 20.23 compact subset iff complete and totally bounded
  \item Arzela-Ascoli thm: $X$ compact Hausdorff, subset $\mc F$ of continuous fns is compact iff $\mc F$ is closed
    and all have finite sup norm and $\mc F$ is equicontinuous.
  \item Stone-Weierstrass special case: polynomials are dense in continuous fns
  \end{enumerate}
\end{enumerate}






\subsection{Bass 4.  Construction of measures}

\subsubsection{Overview}

\begin{enumerate}

\item We want a function that, in some appropriate sense, measures the {\it length} of an arbitrary subset of
  $\R$.

\item We're not going to get the ``sensible measure of length​'' property out of nowhere: we're going to inject a
  pre-existing sensible measure of length of tractable sets at a low level, and build on this.

\item That low-level ``sensible measure of length​'' is, when we're working with $\R$, going to be the length of an
  interval: $|(a, b)| = b - a$.

\item Clearly we want our measure of length to be additive over a finite collection of subsets. But we will also
  require it to be additive over a countably infinite collection of subsets, and this requirement is central to
  everything that follows.

\item So, more precisely, what we want is a {\it countably additive} set function (i.e. a \defn{measure}) defined on a large collection (as
  large as possible) of subsets of $\R$, that is a ``sensible measure of length​'' of those subsets.

\item A theorem tells us a way to make a {\it countably sub-additive} set function (i.e. an \defn{outer measure}) defined on {\it all} subsets:
  \begin{enumerate}
  \item Let $E$ be an arbitrary subset of $\R$ that we want to measure.
  \item Now, restrict attention to the collection of ``low-level​'' subsets of $\R$ for which we have the pre-existing
    sensible measure of length. In $\R$, these are open intervals, or perhaps half-open intervals \red{open sets?}.
  \item Sometimes, one of these low-level subsets will cover $E$. But if $E$ is not a simple interval, we will
    approximate the length of $E$ better with a collection of low-level subsets whose union covers $E$, while
    none of them do on their own. Whether it is a collection of one or many, we will refer to this as a
    ``covering collection of subsets​''.
  \item Note that the covering collection is built out of the low-level subsets, so we can assign a sensible
    measure of length to the covering collection: in $\R$, it is just the sum of lengths of the intervals involved.
  \item Create a set containing the measure of every covering collection. We define our outer measure on $E$ to be
    the infimum (greatest lower bound) of that set. Roughly speaking, we've defined $\mu^*(E)$ to be the total
    length of the collection of intervals that cover $E$ most efficiently (with least unnecessary overlap).
  \end{enumerate}

\item We will call our outer measure $\mu^*$. Clearly it is a reasonable measure of length for some sets.

\item Recall that it is defined on {\it all} subsets of $\R$ (its definition involved our restricted collection of
  ``low-level​'' subsets, but the resulting procedure can be applied to any subset).

\item Pause here: this definition of $\mu^*$ is fundamental. The outer measure that we assign to an arbitrary
  subset $E$ is obtained by using the intervals $I_i$ that we {\it can} measure. We look over all collections of
  the $I_i$ that cover $E$ and record the total length of each cover. The infimum of these cover lengths is the
  measure assigned to $E$:
  \begin{align*}
    \mu^*(E) = \inf\Big\{\sum_i \ell(I_i) : E \subseteq \bigcup_i I_i\Big\}.
  \end{align*}
\item Now, it's nice that it is defined on all subsets of $\R$, and it does have some sensible properties such as
  countable sub-additivity, and probably finite additivity, but it does {\it not} necessarily have the countable
  additivity property that we require.

\item We can get that though with an adjustment: we restrict the collection of subsets that we're allowed to
  measure, so that it's no longer {\it all} subsets.

\item There are two candidates we could restrict to. One is the \defn{Borel} $\sigma$-algebra. This is the
  $\sigma$-algebra generated by the open subsets. Note that it contains the singletons since these are
  countable intersections.

\item However, there's a larger $\sigma$-algebra we can restrict to: the \defn{Lebesgue} $\sigma$-algebra. This is the class
  of $\mu^*$-measurable sets. A set $A \subset \R$ is $\mu^*$-measurable if finite additivity holds between $A$
  and every other subset of $\R$, that is $\mu^*(A \cap E) + \mu^*(A \cap E^c) = \mu^*(A)$ for
  all $E \subset \R$.

\item The Borel $\sigma$-algebra is contained within the Lebesgue $\sigma$-algebra. Restricting $\mu^*$ to either
  gives us what we want: countable additivity. The restriction of $\mu^*$ to the Lebesgue $\sigma$-algebra is
  \defn{Lebesgue measure} $\mu$.

\item The above involved using interval length as our measure of the low-level
  subsets: $\ell(I_i) = b_i - a_i$. There is an important generalization known as \defn{Lebesgue-Stieltjes measure}:
  we introduce a real-valued increasing function $\alpha: \R \to \R$ that distorts the measures we
  assign to each interval: $\ell(I_i) = \alpha(b_i) - \alpha(a_i)$. So an interval in a region in
  which $\alpha$ is increasing rapidly has larger measure. Other than that, the theory for
  Lebesgue-Stieltjes measure is the same: we define the outer measure using the low-level intervals
  on which $\ell$ is defined, and we restrict to the collection of measurable sets, which is
  a $\sigma$-algebra.

\item The Carath\'eodory extension theorem states:
  Suppose:
  \begin{enumerate}
  \item We have $\ell$ which behaves like a measure on some algebra (countable additivity does hold if the
    countable union happens to be in the algebra).
  \item We construct an outer measure $\mu^*(E)$ on \defn{any} subset $E$ using the standard outer measure construction
    (infimum of lengths of open covers, using $\ell$ to measure length).
  \end{enumerate}
  Then $\mu^*$ is countably additive on $\mu^*$



\end{enumerate}



Suppose we are defining a measure $m$ on $\Omega = \R$.

Since an open set $G$ is a countable union of disjoint open intervals, we want
\begin{align*}
  m(G) = \sum_{i=1}^\infty b_i - a_i.
\end{align*}
The basic idea is that, for a set $E$, we are going to define $m(E)$ to be the measure of the smallest open
cover of $E$:
\begin{align*}
  m(E) := \inf \{ m(G) ~:~ G \text{~open~}, E \subseteq G\}.
\end{align*}
However, the infimum has to be restricted to a $\sigma$-algebra that is smaller than all subsets of $\R$ (the
latter is a $\sigma$-algebra, but it's not possible to construct a measure with this as its domain [theorem]).

An \defn{outer measure} is defined similarly to a measure. The difference is
\begin{enumerate}
\item It is defined on {\it all} subsets of $\Omega$
\item It obeys \defn{countable subadditivity}, but {\it not} necessarily countable additivity.
\end{enumerate}

A common way to construct an outer measure is
\begin{enumerate}
\item Define a collection $\mc C$ of subsets of $\Om$ (the collection must contain a subset that partitions $\Om$.)
\item Define a cost function $\ell : \mc C \to [0, \infty]$
\item Define $\mu^*(E)$ to be the cost of the least-cost cover of $E$:
  \begin{align*}
    \mu^*(E) := \inf \Big\{\sum_{i=1}^\infty\ell(C_i) ~:~ E \subseteq \bigcup_{i=1}^\infty C_i\Big\}.
  \end{align*}
  Note what this does: it defines $\mu^*$ on {\it any} subset $E$, while the collection $\mc C$ typically comes from a
  restricted collection (e.g. open sets).
\end{enumerate}

It's obvious that this is FSA but one has to prove that it is CSA (using an epsilon-of-room technique).

\begin{question*}
  \red{TODO} Must an outer measure be finitely additive?

  I don't think so, FA doesn't follow from CS. But could one include FA in the definition of an outer measure?


Is an outer measure always FA? I.e. the issue is just getting CA?

=> Sort of but look at Vitali construction for a counter-example

  Is the (canonical) example given finitely additive?
\end{question*}

\begin{example}[Lebesgue measure on subsets of $\R$]
  \begin{enumerate}
  \item Consider the collection of all open intervals $O \subseteq \R$.
  \item Define $\ell(O)$ in the normal way (sum of $b_i - a_i$ over the disjoint open intervals).
  \item Define $\mu^*(E)$ to be the infimal cost of an open cover of $E$ (i.e. as above in the canonical
    construction of outer measure)
  \end{enumerate}
  So what we've just constructed is an outer measure: it assigns a measure to {\it any} subset $E$, using open sets
  for the cover.

  It must be countably sub-additive, because it used the canonical construction, and we have a theorem for that.

  Now, we would ideally like this to be a measure on all subsets of $\R$, i.e. countably additive.

  It isn't (theorem), but it {\it is} a measure on a restricted collection of subsets of $\R$. That collection is
  the $\sigma$-algebra generated by the open sets (the Borel algebra).

  This measure is \defn{Lebesgue measure}.

  To recap: it was constructed as follows:
  \begin{enumerate}
  \item We want to assign a value to some subset $E$
  \item We use open intervals to cover $E$
  \item The value assigned to $E$ is the measure of the smallest open cover (using length of open intervals to
    define measure here)
  \item This gives a countably subadditive function (an outer measure).
  \item But this is only countably additive for $E$ in the $\sigma$-algebra.
  \end{enumerate}
\end{example}

\begin{intuition*}
  Outer measures have that name because they approximate from above ( ``shrink-wrapping​'').

  When you approximate from above, you get something which is countably subadditive:

  \begin{proof}
    Suppose $\mu^*(A \cup B) > \mu^*(A) + \mu^*(B)$.

    Intuitively, take the most efficient cover of $A$ and the most efficient cover of $B$. Their combined cost
    is $\mu^*(A) + \mu^*(B)$. But together they are a cover of $A \cup B$. But that implies that the most
    efficient cover of $A \cup B$ is no greater than $\mu^*(A) + \mu^*(B)$; a contradiction.

    A real proof has to deal with the fact that we're dealing with infima, not minima.
  \end{proof}
\end{intuition*}

\begin{lemma*}[A continuous function is measurable]\label{lemma-continuous-function-is-measurable}
  Let $Y \subseteq \R$ and let $g: X \to Y$ be a continuous function.

  We must show that there exists a $\sigma$-algebra such that $\{x ~:~ g(x) > y\} \in \mc A$ for all $y \in Y$.

  Let $y \in Y$ and let $U = g^{-1}((y, \infty))$. Then $U$ is open in $X$ since it is the preimage of an open
  set under a continuous function. Therefore $U$ is in the Borel $\sigma$-algebra on $X$.

  Therefore the Borel $\sigma$-algebra satisfies our requirement and $g$ is measurable.
\end{lemma*}

\section*{Lim sup and lim inf}

\subsection{For a sequence of numbers}
\begin{align*}
  \liminf_{n\to\infty} x_n := \lim_{n\to\infty} \inf_{m \geq n} x_m
\end{align*}
This is the largest value below which the sequence never falls again. As we increment $n$, the tail of the
sequence beyond $n$ has some $\inf$. These $\inf$s form an increasing sequence which converge to some value
or $+\infty$. This value is the $\liminf$.

\red{TODO} $\liminf_{n \to \infty} x_n$ can also be described as the $\inf$ of the limits of all convergent subsequences.

\begin{align*}
  \limsup_{n\to\infty} x_n := \lim_{n\to\infty} \sup_{m \geq n} x_m
\end{align*}
This is the smallest value above which the sequence never rises again. The $sup$s form a decreasing sequence
which converge to some value or $-\infty$. This value is the $\limsup$.

If $x_n$ has a limit (either real, or $\pm\infty$) then $\liminf x_n = \limsup x_n = \lim x_n$.

\subsection*{For  a sequence of functions}

$\liminf_{n\to\infty} f_n$ is the function defined by $(\liminf_{n\to\infty} f_n)(x) = \liminf_{n\to\infty} f_n(x)$.

Thus for a sequence $f_n$ of well-behaved curves $\liminf_{n\to\infty} f_n$ describes a curve with the property
that, at each point $x$, there is a point in the sequence beyond which the sequence never falls
below $(\liminf_{n\to\infty} f_n)(x)$.

\subsection{For a sequence of values of a single function}
Let $f(x) = \sin(1/x)$. Then $\liminf_{x \to 0} f(x) = -1$ and $\limsup_{x\to 0} f(x) = +1$. The difference
between the two is called the \defn{oscillation} of $f$ at $0$.

\subsection{For a sequence of sets}

Let $A_1, A_2, \ldots$ be a sequence of sets.

$\liminfn A_n$ are the elements that eventually never disappear again:
\begin{align*}
  \liminfn A_n := \bigcup_{i=1}^\infty \bigcap_{j=i}^\infty A_j
\end{align*}
$\limsupn A_n$ are the elements that always will reappear again:
\begin{align*}
  \limsupn A_n := \bigcap_{i=1}^\infty \bigcup_{j=i}^\infty A_j.
\end{align*}

We have $\liminfn A_n \subseteq \limsupn A_n$.

\url{https://math.stackexchange.com/a/476171/397805}


\section*{Expressions involving countable unions and intersections}

A singleton $\{x\}$ is a countable intersection of intervals:
    \begin{align*}
      \bigcap_{n=1}^\infty \Big(x -\frac{1}{n}, x\Big] = \{x\}.
    \end{align*}

Let $x, a \in \R$ and $n \in \N$. Sets of real numbers satisfying inequalities:
\begin{align*}
  \big\{x ~:~ x \geq a\big\} = \bigcap_{i=1}^\infty \big\{x ~:~ x > a - 1/n\big\}
\end{align*}

\section{Dynamical Systems and Ergodicity}

\begin{definition}
  A measure $\mu$ is $f$-invariant if $\mu(f^{-1}(A)) = \mu(A)$ for all $A$ in the $\sigma$-algebra.
\end{definition}

\begin{definition}
  A measure $\mu$ is $f$-ergodic if $f^{-1}(A) = A$ implies $\mu(A) = 0$ or $\mu(A) = 1$.
\end{definition}


\section{Lebesgue integral}

All functions are measurable unless stated otherwise.

\begin{theorem*}[MCT]
  Let $f_n$ be an {\it increasing} sequence of non-negative functions (i.e. $f_1 \leq f_2 \leq \cdots$). Then, if
  the $f_n$ converge pointwise, the sequence of integrals converges to the integral of the limiting function:
  \begin{align*}
    \lim \int f_n &= \int \limn f_n.
  \end{align*}
\end{theorem*}


\begin{proof}
  $f_n$ is increasing therefore, by monotonicity of the Lebesgue integral, $\int f_n$ is an increasing sequence
  of real numbers.

  Let $L = \lim \int f_n$. We must show $L \leq \int f$ and $L \geq \int f$.

  The first direction is easy: since $f_n \leq f$ for all $n$ we have $\int f_n \leq \int f$ (by monotonicity
  of Lebesgue integral) and therefore $\lim \int f_n \leq \int f$ (since taking a limit preserves an
  inequality).

  To show $\lim \int f_n \geq \int f$ we use a simple function.

  Let $0 \leq s \leq f$ be a simple function, where $s = \sum_{i=1}^k a_i\ind_{E_i}$.

  Let $\eps \in (0, 1)$ and define
  \begin{align*}
    A_n = \{x ~:~ f_n(x) \geq (1 - \eps)s(x)\},
  \end{align*}
  i.e. the set of points where $f_n$ is within $\eps$ of $s$.

  We will show that $\int f_n$ is always at least as big as a quantity that converges to $\int f$.

  Since the $f_n \to f$ we have $A_n \uparrow X$. Therefore
  \begin{align*}
    \int f_n
    \geq \int_{A_n} f_n
    &\geq \int_{A_n} (1 - \eps)s \\
  % &=    (1 - \eps)\int_{A_n} \sum_{i=1}^k a_i\ind_{E_i} \\
    &=    (1 - \eps)\int \sum_{i=1}^k a_i\ind_{E_i \cap A_n} \\
    &=    (1 - \eps)\sum_{i=1}^k a_i\mu(E_i \cap A_n) \\
  \end{align*}
  Now we let $n \to \infty$, obtaining
  \begin{align*}
    \limn \int f_n
    &\geq (1 - \eps)\sum_{i=1}^k a_i\mu(E_i) \\
    &= (1 - \eps)\int s.
  \end{align*}
  Since $\eps$ is arbitrary in $(0, 1)$ we have
  \begin{align*}
    \limn \int f_n \geq \int s.
  \end{align*}
  And since $s$ is an arbitrary simple function satisfying $0 \leq s \leq f$ we may take the supremum over all
  such $s$ yielding
  \begin{align*}
    \limn \int f_n
    &\geq \sup_{0 \leq s \leq f} \int s\\
    &=:    \int f.
  \end{align*}
\end{proof}

\begin{theorem*}[linearity of Lebesgue integral]
  If $f$ and $g$ are either
  \begin{enumerate}
  \item non-negative and measurable, or
  \item integrable,
  \end{enumerate}
  then
  \begin{align*}
    \int (f + g) = \int f + \int g.
  \end{align*}
\end{theorem*}

\begin{proof}
  First (\red{TODO}) we show that the result holds for non-negative simple functions $s$ and $t$.

  Next we suppose $f$ and $g$ are non-negative and consider sequences $(s_n)$ and $(t_n)$ of non-negative
  simple functions increasing to $f$ and $g$ respectively. By the MCT we have
  \begin{align*}
    \lim_{n\to\infty} \int (s_n + t_n)
    &= \int \lim_{n\to\infty} (s_n + t_n) \\
    &= \int (f + g).
  \end{align*}
  But, using linearity of non-negative simple functions, the LHS is
  \begin{align*}
    \lim_{n\to\infty} \int (s_n + t_n)
    &= \lim_{n\to\infty} \int s_n + \lim_{n\to\infty} \int t_n \\
    &= \int f + \int g.
  \end{align*}
  Finally we allow $f$ and $g$ to take both positive and negative values, while being integrable. We use the
  triangle inequality to prove that $f + g$ is integrable given that $f$ and $g$ are, then mess about with the
  decomposition of $f$ and $g$ into $f^+, f^-, g^+, g^-$.
\end{proof}

\begin{intuition*}
  If $(f_n)$ is a sequence of continuous curves, $\liminf_{n\to\infty} f_n$ is a curve that you see ``at the
  horizon​'' when viewing the $(f_n)$ sequence from underneath. At each
  point $(\liminf_{n\to\infty} f_n)(x) := \limn (\inf_{m\geq n}f_m(x))$ is the height that the curves
  eventually never fall below. If the $f_n$ have a pointwise limit then this will
  be $\liminf_{n\to\infty} f_n$. If they do not have a limit, the statement one can make is that there exists a
  point beyond which the $f_n$ curves are no smaller then the $\liminf_{n\to\infty} f_n$ curve.
\end{intuition*}

\begin{theorem}[Fatou's lemma]
  For a sequence of non-negative measurable functions $f_n$ the limit of the sequence of integrals is at least
  as big as the integral of the limit inferior function.
  \begin{align*}
    \liminf_{n\to\infty} \int f_n \geq \int \liminf_{n\to\infty} f_n.
  \end{align*}
\end{theorem}

\begin{mdframed}
\includegraphics[width=400pt]{img/analysis--berkeley-202a--billingsley-section-1--lebesgue-integral-a3de.png}
\end{mdframed}
\url{https://math.stackexchange.com/a/242930/397805}

\begin{mdframed}
\includegraphics[width=400pt]{img/analysis--berkeley-202a--billingsley-section-1--lebesgue-integral-70e1.png}
\end{mdframed}


 https://math.stackexchange.com/a/2748616/397805

\begin{proof}
  Let $g_n(x) = \inf_{m\geq n}f_m(x)$. Note that $(g_n)$ is an increasing sequence of functions.

  By definition,
  \begin{align*}
    \int \liminf_{n\to\infty} f_n
    &:= \int \limn \inf_{m\geq n}f_m \\
    &=: \int \limn g_n \\
    &= \limn \int g_n ~~~~~~~\text{(by the monotone convergence theorem)}\\
    &\leq \limn \int f_n ~~~~~~~\text{(by monotonicity of Lebesgue integral)}.
  \end{align*}
\end{proof}


Since $\limsupn f_n = -\liminfn (-f_n)$

A typical use of Fatou’s lemma is the following. Suppose we have $f_n \to f$ and $\sup_n \int |f_n| \leq K < \infty$.


\begin{theorem*}[dominated convergence theorem]
  Suppose $f_n$ are a sequence of functions and there exists an integrable dominating function $g$ such
  that$|f_n| \leq g$. Then, if the $f_n$ converge pointwise, the sequence of integrals converges to the
  integral of the limiting function:
  \begin{align*}
    \lim \int f_n &= \int \limn f_n.
  \end{align*}
  Furthermore the $f_n$ are integrable.
\end{theorem*}

\begin{proof}

  (This is the proof in Bass)

  Note that $g + f_n \geq 0$. Therefore by Fatou's lemma
  \begin{align*}
    \liminfn \int (g + f_n) \geq \int \liminfn (g + f_n),
  \end{align*}
  and by linearity of the integral, integrability of $g$, and convergence of the $f_n$,
  \begin{align*}
    \liminfn \int f_n
    \geq \int \liminfn f_n
    = \int f.
  \end{align*}

  Similarly, $g - f_n \geq 0$, hence
  \begin{align*}
    \liminfn \int (g - f_n) \geq \int \liminfn (g - f_n),
  \end{align*}
  and
  \begin{align*}
    \liminfn \Big(-\int f_n\Big) \geq -\int f,
  \end{align*}
  or equivalently,
  \begin{align*}
    \limsupn \int f_n \leq \int f.
  \end{align*}

  Thus we have
  \begin{align*}
    \int f \leq \liminfn \int f_n \leq \limsupn \int f_n \leq \int f,
  \end{align*}
  Therefore
  \begin{align*}
    \limn \int f_n = \int f,
  \end{align*}
  as required.
\end{proof}


This alternative proof uses triangle inequality arguments:

\begin{proof}

  (From Bright Side Of Mathematics video lectures; different from main proof in Bass.)
  % Note that from monotonicity of the Lebesgue integral we have $\int |f_n| \leq \int g < \infty$ and
  % also $\int |f| \leq \int g < \infty$.

  We will show that $\lim \int |f_n - f| = 0$.

  Note that $|f_n - f| \leq |f_n| + |f| \leq 2g$.

  Note that $2g - |f_n - f| \geq 0$. Therefore, by Fatou's lemma,
  \begin{align*}
    \liminf_{n\to\infty} \int (2g - |f_n - f|) &\geq \int \liminf_{n\to\infty} (2g - |f_n - f|).
  \end{align*}
  Since $\int g < \infty$, and since $f_n \to f$, we have
  \begin{align*}
    2\int g -  \limsupn \int |f_n - f| &\geq 2\int g,
  \end{align*}
  therefore
  \begin{align*}
    \limsupn \int |f_n - f| &\leq 0.
  \end{align*}
  But
  \begin{align*}
    0 \leq \liminf \int |f_n - f| \leq \limsupn \int |f_n - f| &\leq 0,
  \end{align*}
  Therefore $\lim \int |f_n - f| = 0$.

  But note that
  \begin{align*}
    0
    &\leq \Big|\int f_n - \int f\Big| \\
    &= \Big|\int (f_n - f)\Big| \\
    &\leq \int |f_n - f| \to 0.
  \end{align*}
  Therefore
  \begin{align*}
    \limn \Big|\int f_n - \int f\Big| = 0,
  \end{align*}
  and therefore
  \begin{align*}
    \limn \int f_n = \lim \int f,
  \end{align*}
  as required.
\end{proof}


\begin{theorem*}[MCT, Fatou's lemma, and DCT, summarised]
  Suppose $f_n$ are a sequence of functions that converge pointwise to some limiting function.

  Consider the sequence of integrals $\int f_n$.

  If either
  \begin{enumerate}
  \item $f_n$ are {\it non-negative} and $f_1 \leq f_2 \leq \cdots$ (MCT), or
  \item $|f_n| \leq g$ and $g$ is {\it integrable} (DCT),
  \end{enumerate}
  then the sequence of integrals converges to the integral of the limiting function:
  \begin{align*}
    \lim \int f_n &= \int \limn f_n.
  \end{align*}
  Alternatively, if all we know is that the $f_n$ are non-negative, then the limit of the sequence of integrals
  is no smaller than the integral of the limit inferior function (Fatou's lemma):
  \begin{align*}
  \limn \int f_n \geq \int \liminf_{n\to\infty} f_n.
  \end{align*}
\end{theorem*}

\begin{lemma}[Disjoint closed and compact sets have positive separation]\label{closed-compact-separation-lemma}
 Let $(Z, d)$ be a metric space. Let $A, B \subseteq Z$ be disjoint with $A$ closed and $B$ compact. We define
 \begin{align*}
   d(A, B) := \inf \{d(a, b) ~:~ a \in A, b \in B\}.
 \end{align*}
 Then $d(A, B) > 0$.
\end{lemma}

\begin{remark*}
  This is not true if both sets are merely {\it closed}. But compact is a stronger statement than closed (compact
  implies closed but the converse is not true).
\end{remark*}

\begin{mdframed}
\includegraphics[width=400pt]{img/analysis--berkeley-202a--every-function-(in-l^p)-is-nearly-continuous-6fed.png}
\end{mdframed}

\begin{proof}
  $A$ and $B$ are disjoint so we have
  \begin{align*}
    d: A \times B \to (0, \infty).
  \end{align*}
  This is a continuous function. (\red{TODO} prove. Triangle ineq?)

  Suppose $A$ and $B$ are compact. Then the product $A \times B$ is also compact.

  Recall that a continuous function on a compact set attains its infimum and supremum. Therefore there
  exist $a, b$ such that $d(a, b) = d(A, B)$. Therefore $d(A, B) > 0$.

  Now suppose $A$ is merely closed but not compact. We need to identify a compact subset of $A$. We do this by
  picking some constant $c$ and defining
  \begin{align*}
    A_{\text{far}} &= \{x \in A ~:~ d(x, B) \geq c\} \\
    A_{\text{near}} &= \bar{A \setminus A_{\text{far}}},
  \end{align*}
  (where the bar indicates closure of a set).

  Then $A_{\text{near}}$ is compact (i.e. closed and bounded, since we are in $\R^n$) and the result follows from the
  result for two compact sets above.
\end{proof}


\section{Every function (in $L^p$) is nearly continuous}

\begin{theorem}[Lusin's theorem]
  Let $f:[0, 1] \to \R$ be Lebesgue-measurable. For every $\eps > 0$ there exists a closed
  set $F \subseteq [0, 1]$ with $m(F) > 1 - \eps$ such that $f|_F$ is continuous.
\end{theorem}

\begin{remark*}
  Note that $f|_F$ continuous means it's continuous when viewed as a function $F \to \R$: it might in fact be
  continuous nowhere on $[0, 1]$. An example is $f = \ind_{[0,1] \setminus \Q}$, which is discontinuous
  everywhere, since in every neighborhood both $0$ and $1$ occur as the function's value. However we can
  construct a restriction of $f$ which is continuous: take an enumeration of the rationals and cover rational $i$
  with an interval of length $\eps/2^i$. The union of those intervals has measure at most $\eps$ (not equal
  because there is a lot of overlap). Therefore the complement of the union of those intervals is a set of
  measure at least $1 - \eps$ and contains only irrationals. Hence $f = 1$ on that set and therefore the
  restriction of $f$ to that is continuous.
\end{remark*}

The proof has three parts: we prove it for $f$ a characteristic (indicator) function, then $f$ a simple
function, then $f$ measurable.

\begin{claim*}
  Let $f = \ind_A$ where $A \subseteq [0, 1]$. Then there exists closed $F$ of arbitrarily large measure such
  that $f|_F$ is continuous.
\end{claim*}

\begin{intuition}
  This is true because we can make an indicator function continuous by removing a null set. E.g. for an
  indicator function on intervals we remove the endpoints; for an indicator function on the irrationals we
  remove rationals.

  For a general indicator function, we take a closed approximating ``inner​'' subset $E$ and an open approximating
  ``outer​'' superset $G$ and restrict the indicator function to $E \cup G^c$.
\end{intuition}

\begin{proof}
  From a previous theorem there exist $E \subseteq A \subseteq G$ such that $E$ is closed and $G$ is open
  and $m(G \setminus A) < \eps/2$ and $m(A \setminus E) < \eps/2$.

  We will focus on a subset on which the indicator function is ``well-behaved​''. Specifically,
  consider $E \cup (G^c \cap [0, 1])$ (hereafter $E \cup G^c$). This has measure
  \begin{align*}
    \mu(E \cup G^c) = 1 - m(G \setminus A) - m(A \setminus E) = 1 - \eps.
  \end{align*}

  Since $E \subseteq [0, 1]$ it is compact. Also $G^c$ is closed and so from lemma
  \ref{closed-compact-separation-lemma} we have that the infimum of the separation distances between $E$
  and $G^c$ is positive:
  \begin{align*}
    \delta := d(E, G^c) > 0,
  \end{align*}
  where $d(A, B) = \inf \{ |b - a| ~:~ a \in A, b \in B\}$.

  Recall that we have $f = \ind_A$ hence $f = 1$ on $E$ and $f = 0$ on $G^c$. We now define a continuous
  function $g$ that agrees with $f$ on $E \cup G^c$.

  Specifically, define
  \begin{align*}
    g(x) = \(1 - \frac{d(x, E)}{\delta}\)^+,
  \end{align*}
  where $y^+ = \max(y, 0)$ and $d(x, E) = \inf \{ |e - x| ~:~ e \in E\}$.

  Note that $g$ is continuous and agrees with $f$ on $E \cup G^c$: for $x \in G^c$ we have $g(x) = 0 = f(x)$,
  since $d(x, E) \geq \delta$. And for $x \in E$ we have $g(x) = 1 = f(x)$.
\end{proof}

\begin{claim*}
  Let $f$ be a simple function. Then there exists closed $F$ of arbitrarily large measure such that $f|_F$ is
  continuous.
\end{claim*}

\begin{proof}
  Let $f = \sum_{i=1}^M a_i\ind_{A_i}$ where the $A_i$ are Lebesgue measurable and the $a_i \geq 0$.

  For each $i$ take $F_i$ closed such that $\ind_{A_i}|_F$ is continuous, and $m([0, 1] \setminus F_i) < \eps/M$.

  Let $F = \bigcap F_i$. Then $m([0, 1] \setminus F) < \eps$ and $f|_F$ is continuous.
\end{proof}

\begin{claim*}
  Let $f$ be non-negative and bounded by $K$. Then there exists closed $F$ of arbitrarily large measure such
  that $f|_F$ is continuous.
\end{claim*}

\begin{proof}
  \red{TODO}
  \begin{enumerate}
  \item Take a simple function approximation to $f$
  \item Set $F = \cap_{n=0}^\infty F_n$ where $F_n$ of large measure
  \item Use it to define a sequence of functions converging uniformly to $f$ on $F$
  \item Uniform limit of continuous functions is continuous
  \end{enumerate}
\end{proof}

\begin{claim*}
  Let $f$ be an arbitrary measurable function. Then there exists closed $F$ of arbitrarily large measure such
  that $f|_F$ is continuous.
\end{claim*}

\begin{proof}
  Write $f = f^+ - f^-$...
\end{proof}

\begin{theorem*}[8.2 An approximation result on $\R$]
  Suppose $f: \R \to \R$ is Lebesgue integrable. Then for every $\eps > 0$ there exists a continuous
  function $g$ {\it with compact support} such that
  \begin{align*}
    \int |f - g| < \eps.
  \end{align*}
\end{theorem*}

\begin{remark*}
  Continuous functions are dense in integrable functions when distance measured in a certain way.
\end{remark*}

\begin{proof}

  [Alan Hammond proof]

  \begin{enumerate}
  \item Prove it for $f = \ind_{(a, b)}$ \\

    Replace the discontinuities at endpoints with a sloping line. If regions of slope have total
    width $4\delta$, choose $\delta < \eps/4$.

  \item Prove it for $f$ indicator on finite union of intervals \\

    Do the same finitely many times.

  \item Prove it for $f$ indicator on any bounded open set (countable union of open sets) \\

    Let $O = \bigcup_i I_i$ be an open set written as a countable union of intervals. Arrange them so that the
    tail has total length $< \eps/2$. Use the above on the finite head of the sequence to get an approxmiation
    within $\eps/2$ for the head; together with the error on the tail that gives error $< \eps$.

  \item Prove it for $f$ indicator on any bounded Borel set \\

    Let $O$ be a bounded Borel set.

    Take $E \subseteq O$ with $m(O \setminus E) < \eps$


  \item Prove it for $f$ any bounded simple function (finite linear combination of indicators on Borel sets) \\

    We have a vector space of functions with a spanning set formed by indicators on Borel sets. If $g_1$
    approximates $f_1$ and $g_2$ approximates $f_2$ then $g_1 + g_2$ approximates $f_1 + f_2$, and
    similarly $ag$ approximates $af$.

  \item Prove it for $f$ non-negative measurable bounded (in $y$ axis) with compact support (i.e. bounded on $x$ axis) (use MCT) \\

    Take $s_n \uparrow f$ pointwise.

    From MCT



  \item Prove it for $f$ integrable $f = f^+ - f^-$
  \end{enumerate}
\end{proof}






\begin{proof}
  We may assume $f \geq 0$, for the following reason. Suppose $f$ takes some negative values. Then we
  write $f = f^+ - f^-$. If we can find continuous $g_1$ and $g_2$ such that $\int |f^+ - g_1| < \eps/2$
  and $\int |f^- - g_2| < \eps/2$, then we can define $g = g_1 - g_2$ and we have... \red{TODO}.

  By the MCT, $\limninf \int f\ind_{[-n, n]} = \int f$ and this is finite since $f$ is integrable. Hence we can
  take $N$ large enough that $\int (f - f\ind_{[-N, N]}) < \eps$.

  Then we find

\end{proof}




\section{Using a function to construct a measure }


\begin{lemma}\label{lemma-integral-over-countable-disjoint-sets}
  Let $f$ be non-negative and integrable. Then
  \begin{align*}
    \int_{\cupninf A_n} f  = \sumninf \int_{A_n} f.
  \end{align*}
\end{lemma}

\begin{proof}
  \begin{align*}
    \int_{\cupninf A_n} f
    &= \int f \ind_{\cupninf A_n}  \\
    &= \int \sumninf f \ind_{A_n}  \\
    &= \int \lim_{N \to \infty} \sum_{n=1}^N f \ind_{A_n}  \\
    &= \lim_{N \to \infty} \int \sum_{n=1}^N f \ind_{A_n}  & \text{by the monotone convergence theorem}\\
    &= \lim_{N \to \infty} \sum_{n=1}^N \int f \ind_{A_n}  & \text{by linearity of the integral}\\
    &= \sum_{n=1}^\infty \int_{A_n} f  \\
  \end{align*}
\end{proof}

\begin{theorem}
  Let $f$ be non-negative and integrable with respect to $\mu$. Define $\nu$ by
  \begin{align*}
  \nu(A) := \int_A f \d\mu.
  \end{align*}
  Then $\nu$ is a measure.
\end{theorem}

\begin{proof}
  {\bf Countable additivity:} Suppose $A_n$ are disjoint measurable sets. We have
  \begin{align*}
    \nu\(\bigcupn A_n\)
    &:= \int_{\bigcupn A_n} f \dmu \\
    &= \sumninf \int_{A_n} f \dmu & \text{by lemma \ref{lemma-integral-over-countable-disjoint-sets}} \\
    &= \sumninf \nu(A_n).
  \end{align*}
\end{proof}

\section{Lebesgue vs Riemann integrals}

We use $R(f)$ to denote the Riemann integral and $\int f$ to denote the Lebesgue integral.

\begin{definition*}[Riemann integral]
  Given a partition $P$, define the majorant approximation $U(P, f)$ to be the area under the step function
  comprising rectangles of the form
  \begin{align*}
    (\sup_{x_{i-1} \leq x \leq x_i} f(x))(x_i - x_{i-1}).
  \end{align*}
  The minorant approximation $L(P, f)$ is the analogous thing using $\inf$.

  Define
  \begin{align*}
    \overline{R}(f) = \inf \{ U(P, f) ~:~ P \text{~is a partition~}\}
  \end{align*}
  and
  \begin{align*}
    \underline{R}(f) = \sup \{ L(P, f) ~:~ P \text{~is a partition~}\}.
  \end{align*}
  The Riemann integral $R(f)$ exists if $\overline{R}(f) = \underline{R}(f)$ and is equal to the common value.
\end{definition*}

\begin{theorem*}
  A bounded Borel-measurable function $f:[a, b]\to\R$ is Riemann integrable if and only if it is continuous
  a.e. In that case the Lebesgue and Riemann integrals agree.
\end{theorem*}

\begin{proof}
  For the forwards direction we show that if $f$ is Riemann integrable then $f$ is continuous a.e.
  and $R(f) = \int f$.

  Given a partition $P$ define the simple functions corresponding to the majorant and minorant approximations:
  \begin{align*}
    T_P(x) = \sum_{i=1}^n \Big(\sup_{[x_{i-1}, x_i]} f\Big)\ind_{[x_{i-1}, x_i)}(x)
  \end{align*}
  and
  \begin{align*}
    S_P(x) = \sum_{i=1}^n \Big(\inf_{[x_{i-1}, x_i]} f\Big)\ind_{[x_{i-1}, x_i)}(x).
  \end{align*}
  Note that $\int T_P = U(P, f)$ and $\int S_P = L(P, f)$.

  We now argue (\red{TODO}), using sequences of partitions, that $T_P$ decreases at each point to, say, $T$, and
  $S_P$ increases at each point to, say, $S$, and that
  \begin{align*}
    T = S = F \ae
  \end{align*}
  The proof of this equality uses the DCT (recall that $f$ is bounded):
  \begin{align*}
    \int (T - S) = \lim_{i \to \infty} \int (T_{P_i} - S_{P_i}) = \lim_{i\to\infty} \big(U(P_i, f) - L(P_i, f)\big) = 0.
  \end{align*}
\end{proof}

\section{Ch 8. Properties of the Lebesgue integral}

\begin{theorem}[conditions for $f = 0$ a.e.]
  Let $f$ be real-valued and measurable. Then $f = 0$ a.e. if
  \begin{enumerate}
  \item $f$ is non-negative and $\int f = 0$,
  \item $\int_A f = 0$ on every measurable set $A$,
  \item $\int_0^x f(u) \du = 0$ for all $x$ under Lebesgue measure.
  \end{enumerate}
\end{theorem}

\begin{theorem}
  If $f$ is non-negative and $\int f = 0$ then $f = 0$ a.e.
\end{theorem}

\begin{proof}
  Define $A_n = \{x ~:~ f(x) > \frac{1}{n}\}$. If $f$ is not equal to zero a.e. then there exists $n$ such
  that $\mu(A_n) > 0$. But then
  \begin{align*}
    0 = \int f \geq \frac{1}{n} \mu(A_n),
  \end{align*}
  a contradiction.
\end{proof}


\begin{theorem}
  If $f$ is real-valued and $\int_A f = 0$ on every measurable set $A$ then $f = 0$ a.e.
\end{theorem}

\begin{proof}
  Let $\eps > 0$. We have
  \begin{align*}
    0 = \int_{\{x ~:~ f(x) > \eps\}} f \geq \eps\mu(\{x ~:~ f(x) > \eps\}),
  \end{align*}
  therefore $\mu(\{x ~:~ f(x) > \eps\}) = 0$. Therefore
  \begin{align*}
    \mu(\{x ~:~ f(x) > 0\})
    &= \mu(\bigcup_{n=1}^\infty \{x ~:~ f(x) > \frac{1}{n}\}) \\
    &\leq \sum_{n=1}^\infty \mu(\{x ~:~ f(x) > \frac{1}{n}\}) = 0.
  \end{align*}
  Similarly,
  \begin{align*}
    0 = \int_{\{x ~:~ f(x) < -\eps\}} f \geq \eps\mu(\{x ~:~ f(x) < -\eps\}),
  \end{align*}
  leading to
  \begin{align*}
    \mu(\{x ~:~ f(x) < 0\}) = 0.
  \end{align*}
\end{proof}



\begin{theorem}[approximation by continuous function]
  Let $f: \R\to \R$ be a Lebesgue-measurable integrable function. Let $\eps > 0$. Then there exists a
  continuous function $g$ with {\it compact support} such that
  \begin{align*}
    \int |f - g| < \eps.
  \end{align*}
\end{theorem}

% Therefore
% \begin{align*}
%   \int f - g < \int |f - g| < \eps.
% \end{align*}


\begin{theorem}[Egorov's theorem]

\end{theorem}

\begin{proof}
  Define
  \begin{align*}
    A_{nk} = \bigcup_n \{x ~:~ |f_n(x) - f(x)| \geq 1/k \}
  \end{align*}
  Thus $A_{nk}$ is the set of $x$ values for which, at some point beyond $n$, there is a value lying further than $1/k$ from $f$.

  Note that $A_{nk} \to \emptyset$ as $n \to \infty$ with fixed $k$, since $f_n \to f$.


\end{proof}

\section{10. Types of convergence}

\begin{definition}
 A sequence of functions $f_n \to f$ \defn{in measure} if for any $\eps$
 \begin{align*}
   \limn \mu\Big(\Big\{x ~:~ |f_n(x) - f(x)| > \eps\Big\}\Big) = 0.
 \end{align*}
 {\bf Intuition:} while there may not actually be convergence at any point (large values may always occur in the future),
 as time goes on the measure of discrepant points at any given point in time gets arbitrarily small.
\end{definition}

\begin{theorem}
  \begin{enumerate}
  \item If $f_n \to f$ a.e. then $f_n \to f$ in measure
  \item If $f_n \to f$ in measure then there exists a subsequence such that $f_{n_k} \to f$ a.e.
  \end{enumerate}
\end{theorem}

\begin{proof}
  \red{TODO}
\end{proof}

\begin{remark*}
  For a finite measure ($\mu(X) < \infty$) convergence in measure and convergence a.e. are equivalent. To prove
  that i.m. $\implies$ a.e. one can use the DCT on the indicator
  function $\ind_{\{x ~:~ |f_n(x) - f(x)| > \eps\}}$, since for a finite measure it is bounded above by $1$.
\end{remark*}

\begin{theorem}[Chebyshev's inequality]
  \begin{align*}
    \mu(\{x ~:~ |f(x)| \geq a\}) \leq \frac{\int |f|^p}{a^p}
  \end{align*}
    for $p \geq 1$.
\end{theorem}

\begin{proof}
  Let $A = \{x ~:~ |f(x)| \geq a\}$. Then
  \begin{align*}
    \mu(A)
    = \int \ind_A
    \leq \int \frac{|f|}{a},
  \end{align*}
  and this remains true when raising the integrand to a power $p > 1$.
\end{proof}

\section{12. Signed measures}

\begin{definition}
  A \defn{signed measure} $\mu:\mc A \to (-\infty, \infty]$ satisfies countable additivity over a union of disjoint
  sets $A_i \in \mc A$:
  \begin{align*}
    \mu(\bigcup_{n=1}^\infty A_i) = \sum_{n=1}^{\infty} \mu(A_i).
  \end{align*}
  If the measure of this union is finite then the series $\sum_{n=1}^{\infty} \mu(A_i)$ must converge
  absolutely. This implies that the summation is well-defined (it is a theorem that for a series in a complete
  topological space such as the reals, absolute convergence implies unconditional convergence).
\end{definition}

\begin{theorem}
  If $A_n \uparrow A$ then $\limn \mu(A_n) = \mu(A)$.
\end{theorem}

\begin{proof}
  \red{TODO}
\end{proof}

\begin{theorem}
  Let $A = \bigcup_{i=1}^\infty A_i$. Then
  \begin{align*}
    \mu(\bigcup_{i=1}^\infty A_i) = \limn \mu(\bigcup_{i=1}^\infty A_i).
  \end{align*}
\end{theorem}

\begin{proof}
  (\red{TODO} the same as for unsigned measure)
  Let $A = \bigcup_{i=1}^\infty$

  \begin{align*}
    \mu(\bigcup_{i=1}^\infty A_i)
    &:= \mu(\lim_{n\to\infty} \bigcup_{i=1}^n A_i) .
  \end{align*}

\end{proof}

\begin{definition}
  $A$ is a \defn{positive set} if $\mu(B) \geq 0$ for all $B \subseteq A$ (where $A$ and $B$ are in the $\sigma$-algebra).

  $A$ is a \defn{negative set} if $\mu(B) \leq 0$ ...

  $A$ is a \defn{null set} if $\mu(B) = 0$ ...
\end{definition}

\begin{theorem}
  If $\mu(E) < 0$ then there exists a negative set $F \subseteq E$ with $\mu(F) < 0$.
\end{theorem}

\begin{proof}
  If $E$ is a negative set we are done.


  Alternatively, there is a measurable subset of $E$ with positive measure. Let $n_1$ be the smallest positive
  integer such that there exists measurable $E_1 \subset E$ with $\mu(E_1) \geq 1/n_1$
\end{proof}

\begin{theorem}[Hahn decomposition theorem]
  $X$ can be partitioned as $P \cup N$ where $P$ is a positive set and $N$ is a negative set.

  The decomposition is unique up to a null set. I.e. if $X = P' \cup N'$
  then $\mu(P \Delta P') = \mu(N \Delta N') = 0$.
\end{theorem}

\begin{proof}
  \red{TODO}

  Let $L = \inf \{\mu(A) ~:~ A \text{~is a negative set}\}$ (note that $\emptyset$ is a negative set so there is at least one).

  Choose a sequence $A_n$ of negative sets such that $\mu(A_n) \to L$.

  Create a sequence $B_n$ by disjointifying the $A_n$.

\end{proof}

\begin{definition}
  Measures $\mu$ and $\nu$ are \defn{mutually singular} if $X$ can be partitioned as $X = E \cup F$ with
  $\mu(E) = 0$ and $\nu(F) = 0$.

  This is written $\mu \perp \nu$.
\end{definition}

\begin{intuition*}
  In other words, the \defn{support sets} of $\mu$ and $\nu$ are non-overlapping and partition $X$.

  If we sample one value from a probability distribution formed as the mixture of measures $\mu$ and $\nu$, we
  will know with certainty which measure generated the value.

  One could be in an intermediate situation, where there is some overlap in support. In that case,
  sometimes we will know which measure generated the value and sometimes not. But with mutual
  singularity the support sets are mutually exclusive and we always know.
\end{intuition*}

\begin{example}
  For example, let $X = [0, 1]$ and let $\mu$ and $\nu$ be Lebesgue measure restricted to $[0,\frac{1}{2}]$
  and $[\frac{1}{2} , 1]$ respectively. Then $E = (\frac{1}{2} , 1]$ and $F = [0,\frac{1}{2}]$ shows
  that $\mu \perp \nu$.
\end{example}

\begin{example}
  Define
  \begin{align*}
    f_1(x) = x
  \end{align*}
  \begin{align*}
      f_2(x) =
      \begin{cases}
        0                         & x < 0 \\
        \text{Cantor-Lebesgue}(x) & 0 \leq x < 1 \\
        1                         & x \geq 1.
      \end{cases}
    \end{align*}
    \begin{align*}
      f_3(x) = \text{piecewise constant with countably many jumps of size $a_i \geq 0$ at points $x_i$}
    \end{align*}

    Define measures (``in some sense encompass all possible behaviors for positive measures on $\R$​'')

    The notation $\mu_f$ means Lebesgue-Stieltjes w.r.t. $f$. ($f$ is a ``distribution function​'').
    So the measure of an interval is determined by the change in the function over that interval.

  \begin{itemize}
  \item $\mu_1 = \mu_{f_1} = m$, Lebesgue measure on $\R$
  \item $\mu_2 = \mu_{f_2}$
  \item $\mu_3 = \mu_{f_3} = \sum_{i=1}^\infty a_i \delta_{x_i}$ where $a_i \geq 0$ and $\sum_{i=1}^\infty a_i < \infty$
  \end{itemize}

  \begin{claim*}
    All three measures are mutually singular w.r.t. each other.
  \end{claim*}

  \begin{proof}
    For $\mu_1$ vs $\mu_2$, note that if we sampled a real number from a mixture of those
    distributions, we could examine its base-3 representation: if it has any 1s then it must have
    come from $\mu_1$.

    So the space $X = [0, 1]$ is partitioned into
    \begin{enumerate}
    \item the Cantor set (which is a null set, so has measure $0$ under $\mu_1$, but the C-L
      function is increasing on that null set so it has positive measure under $\mu_2$), and
    \item the complement of the Cantor set (on which the Cantor-Lebesgue function does not
      increase, so all subsets of that have measure zero under $\mu_2$, but they have positive
      measure under $\mu_1$)
    \end{enumerate}
    It's clear that $\mu_1 \perp \mu_3$ since $\mu_3$ assigns zero measure almost everywhere (so
    there it is zero and $\mu_1$ is non-zero) and the complement is null so has zero measure
    under $\mu_1$ and yet positive under $\mu_3$ since that's where the jumps are.

    Finally $\mu_2$ vs $\mu_3$.

  \end{proof}

\end{example}

\begin{theorem}[Jordan decomposition theorem]
  If $\mu$ is a signed measure then there exist positive measures $\mu^+$ and $\mu^-$ such
  that $\mu = \mu^+ - \mu^-$ and $\mu^+ \perp \mu^-$. This decomposition is unique.
\end{theorem}

\begin{intuition}
  Note that in the mutual singularity statement $\mu^+ \perp \mu^-$, the decomposition being
  referred to comes from a Hahn decomposition into a positive and negative set.
\end{intuition}

\begin{mdframed}
\includegraphics[width=400pt]{img/analysis--berkeley-202a--billingsley-section-1--12-signed-measures-b561.png}
\end{mdframed}

\begin{proof}
  $\mu$ is signed so we can decompose into $M, P$ negative and positive. Thus for some set $A \in \mc A$
  \begin{align*}
    \mu^+(A) &= \mu(A \cap P) \\
    \mu^-(A) &= -\mu(A \cap M).
  \end{align*}

  \red{TODO} uniqueness
\end{proof}

\begin{definition}
  The measure $|\mu| := \mu^+ + \mu^-$ is called the \defn{total variation measure} of $\mu$.

  $|\mu|(X)$ is called the total variation of $\mu$.
\end{definition}

The core example motivating these definitions and decomposition theorems is

\begin{example}
  Let $m$ be Lebesgue measure and define
  \begin{align*}
    \mu(A) = \int_A f \d m.
  \end{align*}
  Then $\mu$ is a measure. If $f$ takes positive and negative values then $\mu$ is a signed measure.

  The \defn{Hahn decomposition} theorem states that the measure space $X$ may be partitioned as $X = P \cup N$ where
  $P$ is a positive set (all measurable subsets have non-negative measure) and $N$ is a negative set (all
  measurable subsets have non-positive measure).

  The obvious decomposition is
  \begin{align*}
    N &= \{x ~:~ f(x) \leq 0\} \\
    P &= \{x ~:~ f(x) > 0\},
  \end{align*}
  and this decomposition is unique except of course that $Z = \{x ~:~ f(x) = 0\}$ is a null set (all measurable
  subsets have measure zero) and different ways of apportioning $Z$ to $P$ and $N$ do not affect their measure
  (and thus their status as positive and negative sets respectively).

  The \defn{Jordan decomposition} theorem states that $\mu$ can be written as $\mu = \mu^+ - \mu^-$, where
  \begin{align*}
    \mu^+(A) = \int_A f^+ ~~~~~~~ \text{and} ~~~~~~~ \mu^-(A) = \int_A f^-.
  \end{align*}
  Recall that $f^+ := \max(f, 0)$ and $f^- := \min(f, 0)$.
\end{example}

\begin{intuition}
  So in other words, we define a measure $\mu$ by using a function $f:X \to \R$ to specify how the measure
  weights different subsets of $X$.

  The construction is natural but in general it gives rise to a signed measure $\mu$. We then note that there
  are two natural decompositions associated with signed measures:

  The Hahn decomposition partitions the measure space into one subset with purely positive measure
  and one with purely negative measure.

  The Jordan decomposition states that the measure can be written as the difference between two positive
  measures: one reflecting weighting of the input space due to positive values of the weighting function, and
  one reflecting weighting due to negative values of the weighting function.
\end{intuition}


\section{13. Radon-Nikodym theorem}

Analogy/example referring to notation at the beginning of Bass ch. 13:

$\mu$ is a measure reporting areas of US counties.

$\nu$ is a measure reporting population count of US counties.

$A$ is Illinois. The definition of $\nu$ is

\begin{align*}
  \nu(A) = \int_A f \d\mu.
\end{align*}
Thus $f$ is the density of people/area, and $f\d\mu$ can be thought of as an infinitisemal area
converted to population by multiplying by the local density.

\begin{definition}[absolute continuity]
  $\nu$ is absolutely continuous w.r.t. $\mu$ if $\mu(A) = 0 \implies \nu(A) = 0$.

  We write $\nu \ll \mu$.
\end{definition}

\begin{intuition}
  $\nu$ doesn't see anything that $\mu$ doesn't see​. $\mu$ might report non-zero values where
  $\nu$ reports zero; in this sense $\nu$ is less than $\mu$, as suggested by the notation.
\end{intuition}

\begin{theorem}[Radon-Nikodym]
  Suppose $\mu$ and $\nu$ are {\it positive} measures and $\nu \ll \mu$.

  Suppose $\mu$ is $\sigma$-finite and $\nu$ is finite, on a measurable space $(X, \mc A)$.

  Then there exists a {\it non-negative} function $f$ such that
  \begin{align*}
  \nu(A) = \int_A f \d\mu
  \end{align*}
  for all $A \in \mc A$.

  ($f$ is $\mu$-integrable and measurable w.r.t. $\mc A$.)

  If $g$ is another such function then $f = g$ $\mu$-almost everywhere.

  $f$ is called the \defn{Radon-Nikodym derivative} of $\nu$ w.r.t. $\mu$ and one may write
  $\d\nu = f\dmu$ or $f = \frac{\d\nu}{\dmu}$.
\end{theorem}

The idea of the proof is to look at the set of functions $f$ that
satisfy $\int_A f \d\mu \leq \nu(A)$ on all subsets $A$, and choose the one that
maximises $\int_X f \dmu$. This will turn out to turn the inequality into equality on all subsets.


Here's my understanding: by definition a measure $\nu$ has R-N derivative $f$ w.r.t. a measure $\mu$ if $\nu(A) = \int_A f d\mu$ for every measurable set $A$.

The notation $\frac{d \nu}{d \mu}$ is also used for such an $f$, thus $\nu(A) = \int_A \frac{d \nu}{d \mu} d\mu$.




The notation $$\frac{}{}$$


\begin{proof}~\\

  \subsubsection*{Step 2: definition of f}

  \red{TODO}

  Define a class of measurable functions
  \begin{align*}
    \mc F = \Big\{g ~:~ g \geq 0 \text{~and~} \int_A g \d\mu \leq \nu(A) ~\text{~for all~} A\Big\}
  \end{align*}
  Consider the integrals of these functions across the full space $X$.

  Let $L$ be the $\sup$ of those integrals. Note that  $L < \infty$ since $\nu$ is finite.

  Choose a sequence $g_n$ such that the sequence of integrals $\to L$.

  Set $h_n = \max\{g_1, \ldots g_n\}$.

  Prove that $h_2 = \max(g_1, g_2)$ is in $\mc F$. This is fairly straightforward: split into a subset
  where $g_1$ is greater, and the complement where $g_2$ is greater.

  Therefore $h_n \in \mc F$ by induction.

  The $h_n$ increase. Let $f$ be the limiting function (may be infinite, but this will have to be on a null set).

  Note: we haven't used abs. cont. yet.

  \subsubsection*{Step 3: Prove that f is the desired function}

  We need to prove that $\nu(A) = \int_A f \dmu$ for all $A \in \mc A$.

  Define a measure $\lambda$ by
  \begin{align*}
    \lambda(A) = \nu(A) - \int_A f \dmu.
  \end{align*}
  Thus we need to show that $\lambda(A) = 0$ for all $A$.

  Note that $\lambda$ is a positive measure, since $f \in \mc F$.

  Suppose for a contradiction that $\lambda$ is not mutually singular w.r.t. $\mu$.

  By the lemma, there exists $\eps$ and $G \in \mc A$ such that $\mu(G) > 0$ and $G$ is a positive set
  for $\lambda - \eps\mu$.

  Then for any $A \in \mc A$ we have
  \begin{align*}
    \nu(A) - \int_A f \dmu
    &= \lambda(A) \\
    &\geq \lambda(A \cap G) \\
    &\geq \eps\mu(A \cap G) &\text{since $G$ is a positive set for $\lambda - \eps\mu$} \\
    &=\int_A \eps\ind_{G} \dmu, &\text{~an "extra piece"!}
  \end{align*}
  or equivalently
  \begin{align*}
    \nu(A) = \int_A (f + \eps\ind_G) \dmu.
  \end{align*}

  Therefore $f + \eps\ind_G \in \mc F$. But
  \begin{align*}
    \int_X (f + \eps\ind_G) \dmu = L + \int_X\eps\ind_G \dmu > L.
  \end{align*}
  Yet $L := \sup \{\int_X g \dmu ~:~ g \in \mc F\}$, so this is a contradiction.

  Therefore $\lambda \perp \mu$.

  Therefore $\lambda(A) = 0$ whenever $\mu(A) > 0$.

  (Note again: we haven't used abs. cont. yet. So at this point we are close to the proof of Lebesgue
  decomposition theorem: we have decomposed $\nu$ into an absolutely cont piece $\int_A f \dmu$ and a mutually
  singular piece $\lambda$)

  But $0 \leq \lambda < \nu \ll \mu$, so $\lambda(A) = 0$ whenever $\mu(A) = 0$.

  Therefore $\nu(A) = \int_A f \dmu$ for all $A \in \mc A$, as required.
\end{proof}




\begin{claim*}
  If $g$ is another such function then $f = g$ $\mu$-almost everywhere.
\end{claim*}

\begin{proof}
    Suppose $f$ and $g$ are two such functions. Then for all $A \in \mc A$ we have
  \begin{align*}
    \nu(A) = \int_A f \d\mu = \int_A g \d\mu
  \end{align*}
  therefore
  \begin{align*}
    \int_A (f - g) \dmu = 0.
  \end{align*}
  Since this holds on every measurable set $A$, we have $f = g$ $\mu$-almost everywhere.
\end{proof}


\section{Differentiation}

Bass for Radon Nik then
Folland section 3.4

The Radon-Nikodym theorem gives a derivative for one measure $\nu$ with respect to another measure $\mu$.
Specifically, as long as $\nu \ll \mu$, it says that there exists $f$ such that
\begin{align*}
  \nu(A) = \int_A f \dmu.
\end{align*}
These measures are of course set-functions, in an abstract setting. We now focus on $\R^n$ and Lebesgue
measure. The statement above involving the derivative function $f$ is analogous (let $A = [a, b]$) to
\begin{align*}
  y(b) - y(a) = \int_a^b \dydx \dx.
\end{align*}
Here the measure $\nu$ has been replaced by a function $y$ which determines a new length for the
interval $[a, b]$.

The $f$ from Radon-Nikodym is referred to as $\frac{\d\nu}{\dmu}$. (And as a mnemonic at least, perhaps we can
think of the requirement for $\nu \ll \mu$ as not wanting this fraction to be zero below and non-zero above.)

$m$ is Lebesgue measure.

In the context of $\R$ under Lebesgue measure we can define a {\it pointwise} derivative

which in $\R$ would be like
\begin{align*}
  f^* = \lim_{r \to 0}\frac{\nu([x,x + r])}{r}.
\end{align*}

If $\nu \ll m$ then the Radon-Nikodym derivative $f$ exists such that
\begin{align*}
  \nu([x,x + r] = \int_{x}^{x+r} f \dm.
\end{align*}
In this case $\frac{\nu([x,x + r])}{r}$ is the average value of $f$ on the interval, and we would hope
that $f^* = f$ $m$-a.e, which is true as long as $\nu$ assigns a finite measure to all intervals (theorem
\ref{folland-3-18-limit-of-average}; the locally integrable requirement is stronger than ``assigns finite
measure to all intervals​'' since if $\int |f|$ is finite then so is $\int f$).

\begin{quote}
  From the point of view of $f$ this may be regarded as a generalisation of the FTC: the derivative of the
  indefinite integral of $f$ (namely, $\nu$) is $f$.
\end{quote}

In other words,
\begin{align*}
  \lim_{r \to 0}\frac{\nu([x,x + r])}{r} = f,
\end{align*}
is an FTC-like statement since the numerator -- the measure of an interval -- is the change in value of an
accumulation function:
\begin{align*}
  \nu([x, x + r]) = \int_x^{x + r} f \dm = \int_0^{x + r} f \dm - \int_0^{x} f \dm,
\end{align*}
and therefore what we are saying is that the derivative of the $f$-accumulation function is $f$ itself.


So what we are saying is
\begin{enumerate}
\item If you define the accumulation function $\int_0^x f \dm$ where $f$ is the Radon-Nikodym derivative, then the
  derivative of the accumulation function is $f$.
\item ... I'm not sure that's quite the right statement
\end{enumerate}

In $\R^n$ these become

\begin{align*}
  f^* = \lim_{r \to 0} \frac{\nu(B(x, r))}{m(B(x, r))},
\end{align*}
and $\nu(B(x, r)) = \int_{B(x, r)} f \dm$.

So the theorem below (\ref{folland-3-18-limit-of-average}) tells us that
\begin{align*}
  \lim_{r \to 0} \frac{\nu(B(x, r))}{m(B(x, r))} = f.
\end{align*}

(The connection to theorem \ref{folland-3-18-limit-of-average} is
that $\frac{\nu(B(x, r))}{m(B(x, r))} = (A_r f)(x)$.)

Now, in the one-dimensional case we could interpret $\nu([x, x + r])$ as a difference between accumulation
function values at two points $x$ and $x + r$, which allows us to
interpret $\lim_{r \to 0}\frac{\nu([x,x + r])}{r}$ as a derivative of an accumulation function.

But what about $\lim_{r \to 0} \frac{\nu(B(x, r))}{m(B(x, r))} = \lim_{r \to 0} (A_r f)(x)$? Can we interpret this as
\begin{align*}
  \nu(B(x, r)) = \int_{B(x, r)} f \dm
\end{align*}

\begin{question*}
  I think I understand the connection in the one-dimensional case, but not in higher dimensions.

  In one dimension ($\mathbb{R}$) under Lebesgue measure $m$, the connection is this:

  Suppose we have a measure $\nu$ and the Radon-Nikodym derivative w.r.t. $m$ exists. Then for an interval $[x, x + r]$ we have
  \begin{align*}
    \nu([x,x + r] = \int_{x}^{x+r} f dm.
  \end{align*}
  But this integral can be regarded as the difference between two integrals:
  \begin{align*}
    \nu([x, x + r]) = \int_0^{x + r} f dm - \int_0^{x} f dm,
  \end{align*}
  and thus $f^* = \lim_{r \to 0}\frac{\nu([x,x + r])}{r}$ is a derivative of the function $F(x) = \int_0^x f dm$.

  We also note that $\nu([x, x + r])$ is the average value of $f$ on the interval, so then the point of Folland
  theorem 3.18 is that it tells us that $f^* = f$ a.e. which is an FTC-like statement: the derivative of the
  ``area-so-far​'' function $F$ is $f$ itself.

  Now, how does this extend to the higher-dimensional context in which the Folland chapter is written? We have
  \begin{align*}
    f^* = \lim_{r \to 0} \frac{\nu(B(x, r))}{m(B(x, r))},
  \end{align*}
  and $\nu(B(x, r)) = \int_{B(x, r)} f dm$, but what I don't see is how we can make a statement analogous to
  \begin{align*}
    \nu([x, x + r]) = \int_0^{x + r} f dm - \int_0^{x} f dm,
  \end{align*}
  regarding the ball $B(x, r)$ in $\mathbb{R}^n$ for $n > 1$.

  (In standard multivariable calculus I gather that Green's / Stokes' theorems are relevant? But that's not something I've studied yet.)
\end{question*}


\begin{lemma}[A covering lemma]
  If $E \subset \R^n$ is covered by a collection $\mc B$ of open balls and the diameter of every ball is
  bounded by $R < \infty$, then there exists a disjoint collection of balls $B_1, \ldots$ such
  that $m(E) \leq 3^n \sum_i m(B_i)$.
\end{lemma}

If you have a collection of covering balls, you can extract a disjoint set that still covers a certain
fraction.

\begin{remark*}
  In the following, it is worth considering that the function $f$ could be a Radon-Nikodym derivative of a
  measure $\nu$ with respect to $\mu$. If it is, then the statement ``$f$ is locally integrable​'' implies
  that $\nu$ assigns a finite measure to bounded compact sets. And ``$f$ is integrable​'' implies that $\nu$ is a
  finite measure.
\end{remark*}

\begin{definition*}[Locally integrable]
  $f: \R^n \to \R$ is \defn{locally integrable} if $\int_K |f| \dmu < \infty$ for all bounded (compact) sets $K$.

  The set of locally integrable functions is $L^1_{loc}$.
\end{definition*}

\begin{definition}[Average value of a function]
  If $f \in L^1_{loc}$ then the \defn{average value} of $f$ on a ball $B(x, r)$ is
  \begin{align*}
    (A_r f)(x) := \frac{1}{m(B(x, r))} \int_{B(x, r)} f \dm
\end{align*}
\end{definition}


\begin{definition}[Maximal function]
  If $f \in L^1_{loc}$ the \defn{maximal function} of $f$ is
  \begin{align*}
    M f(x) := \sup_r (A_r |f|)(x).
  \end{align*}
  Informally, $M f(x)$ is the supremum of the values you get when comparing regions centred at $x$ according to
  the average absolute value of $f$ in that region.
\end{definition}

\begin{lemma}
  If $f \in L^1_{loc}$ the average value $A_r f(x)$ is jointly continuous in $r$ and $x$.
\end{lemma}

\begin{proof}
  Let $c = m(B(0, 1))$ and define $S(x, r) = \{y ~:~ |y - x| = r\}$ to be the sphere at $x$. We have $m(S(x, r)) = 0$.

  From (polar coordinates section of Folland) we know that $m(B(x, r)) = cr^n$.

  $\ind_{B(x, r)} \to \ind_{B(x_0, r_0)}$ a.e. Note that $|\ind_|$
\end{proof}

\begin{theorem}[Maximal theorem]
  For all $f \in L^1$ there exists a constant $C > 0$ such that for all $\alpha > 0$
  \begin{align*}
    m(\{x ~:~ M f(x) > \alpha\}) \leq \frac{C}{\alpha}\int |f|.
  \end{align*}
\end{theorem}

\begin{intuition*}
  So, this is an upper bound on how many places the average value of $|f|$ (over a local ball of any size) can
  exceed a value $\alpha$, and the bound is proportional to $\frac{\int |f|}{\alpha}$.

  Does this make sense? So there's a total amount $\int |f|$ of stuff, and there's a bound on how often the
  average value of stuff seen from a given point can exceed some amount.

  Say $f = c$ is constant and positive. Then $M f = c$ everywhere. But such an $f$ is not Lebesgue integrable,
  i.e. not in $L^1$.

  OK, so say $f = c$ in some ball of measure $b$ and $0$ elsewhere. Then $Mf = c$ everywhere in the ball and decreases away
  from the ball. The measure of places where $Mf > c$ is zero, so yes, that is bounded above by whatever
  positive number. The measure of places where $Mf \geq c$ should be $b$, and we have $\int |f| = cb$ So the theorem says that
  \begin{align*}
    b \leq \frac{C}{c} cb,
  \end{align*}
  which is true.
\end{intuition*}

\begin{proof}
  Let $E_\alpha = \{x ~:~ M f(x) > \alpha\}$.

  For every $x \in E_\alpha$ there exists $r_x$ such that $A_{r_x} |f|(x) > \alpha$.

  The balls $B(x, r_x)$ cover $E_\alpha$.


\end{proof}


\subsection{Lebesgue differentiation theorems}

\begin{theorem}\label{folland-3-18-limit-of-average}

  \begin{align*}
    \lim_{r \to 0} (A_r f)(x) = f(x) \ae
  \end{align*}
\end{theorem}
The point here is that if $f = \frac{\d\nu}{\dm}$ then
\begin{align*}
  (A_r f)(x)
  &= \frac{1}{m(B(x, r))} \int_{B(x, r)} f \dm \\
  &= \frac{\nu(B(x, r))}{m(B(x, r))},
\end{align*}
and so the theorem is actually saying that the derivative of some sort of function based on $\int_{B(x, r)} f \dm$ is $f$ a.e.

The theorem can be written differently:
\begin{align*}
  \lim_{r \to 0} \frac{1}{m(B(x, r))} \int_{B(x, r)} (f(y) - f(x)) \dy = 0 \ae
\end{align*}
and in fact a stronger statement is true with the integrand replaced by its absolute value. We define the \defn{Lebesgue set}
of $f$ to be
\begin{align*}
  L_f := \Big\{ x ~:~  \lim_{r \to 0} \frac{1}{m(B(x, r))} \int_{B(x, r)} |f(y) - f(x)| \dy = 0\Big\}.
\end{align*}
Then
\begin{theorem*}
  $m(L_f^c) = 0$.
\end{theorem*}

the Lebesgue set of $f$ are the points at which the derivative of $F(x) = \int_0^x f dm$ equals $f$, or rather,
the equivalent statement for balls in $R^n$, and I'm not sure quite what that statement is.

Anyway, the point is that this FTC-like statement is true a.e.

\section{Arzeli-Ascola}

$X$ is compact Hausdorff.

Consider continuous functions under supremum norm.
\begin{align*}
  d(f, g) = \origsup_{x \in [0, 1]} |f(x) - g(x)|.
\end{align*}
It is complete: every Cauchy sequence converges. (Work pointwise, complete it in the real line,
check the resulting fn is a unif limit of the sequence in question)

Question: when is a collection of continuous functions compact?

Compactness in a metric space:

$\R$ is not compact because it is too big: can't cover it with finite subcover.

$\Q \isect (0, 1)$ problem is at finest scale: it is not complete, you can leave the set at $1/\sqrt(2)$.

We will see that a subspace of a metric space is compact if two conditions hold: (1) not too big and (2)
complete.

(1) is total boundedness:

$(X, d)$ is a metric space. An $\eps$-net for $A$ is a countable set $\{x_i ~:~ i \in \N\} \subseteq X$ such
that $A \subset \union B(x_i, \eps)$: everyone in $A$ is within $\eps$ of one of the points.

$A$ is \defn{totally bounded} if for all $\eps > 0$ there exists a finite $\eps$-net for $A$.

Bass 20.23

A subset $A$ of a metric space is compact iff
\begin{enumerate}
\item it is complete, and
\item it is totally bounded.
\end{enumerate}


How might a space of continuous functions not be compact?

\begin{enumerate}
\item The functions may ``go off to infinity​'' - not totally bounded.
\item
  Consider $f_n(x) = \sin(nx)$. (``weakly convergent​'') but doesn't converge to a valid function
\item or a point could be removed, e.g. $f_n = 1/n$ is not compact because does not contain $0$.
\end{enumerate}

We need a notion of uniformity of convergence that doesn't rely on a $\delta$ in the domain (because we are
considering topological spaces hence the domain is not necessarily a metric space.) I.e. ``simultaneous​'' or
``uniform​'' continuity for the whole system of functions.

\begin{definition*}
  A subset $\mc F \subseteq \mc C(X)$ is \defn{equicontinuous} if $\forall~ \eps > 0$ and
  $\forall~ x \in X$ there exists an open set $G$ containing $x$ such
  that $|f(x) - f(y)| < \eps$ $\forall~ f \in \mc F$ and $\forall~ y \in G$.
\end{definition*}

Note that the same $G$ works for all $f$.

So this is the defn of continuity for a single function $f$. The question is whether $\delta$ can
be chosen uniformly for all functions in the collection. Or rather, since this is a topological
space, can the open set $G$ be chosen uniformly for all $f$? This means for a given $x$, does the
same $G$ work for all $f$?

Consider $\sin(nx)$. This collection is not equicontinuous. The reason is that for any {\it given}
$\delta$ (open interval $G$), we can always find a function further along in the sequence whose
vertical movement is so fast that it moves more than $\eps$. So, for any {\it given} one of those
functions, for any $\eps$, we will be able to find a $\delta$ within which the movement is
constrained to stay within $\eps$. However, there'll always be another function in the family where
that's not true. Thus the family as a whole fails the equicontinuity criterion.



\begin{theorem}[Arzeli-Ascoli]
  $X$ is compact Hausdorff. A subset $\mc F \subseteq \mc C(X)$ is compact iff all the following hold
  \begin{enumerate}
  \item $\mc F$ is closed
  \item for all $x \in X$, we have $\origsup_{f \in \mc F} |f(x)| < \infty$
  \item $\mc F$ is equicontinuous
  \end{enumerate}
\end{theorem}

More examples:

Consider functions $\mc C \to \R$ ($\mc C$ under supremum norm topology and $\R$ under standard Euclidean
metric topology).

Consider the pointwise evaluation map for a point $x$ i.e. $f \mapsto f(x)$. This is continuous:
domain space is under supremum norm; if two fns are close under sup norm then they are close at
every point. This continuity is the reason why in A-A the compactness of $\mc F$ implies (2) above
(continuous image of compact set is compact).



\section{Questions}

- For the purposes of applied mathematics, is a theory based on the infinitely divisible reals necessary? Or
would a theory based on an arbitrarily fine finite mesh suffice?
