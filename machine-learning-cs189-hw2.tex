\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{soul,color}
\usepackage{amsfonts, amsmath}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage[nobreak=true]{mdframed}

\usepackage{notes}
\newcommand{\solution}{\textbf{Solution: }}
\renewcommand{\P}{\Pr}
\newcommand{\p}{\text{P}}

\title{CS189, HW2}
\author{\vspace{-6ex} \\\\Dan Davison\\\texttt{ddavison@berkeley.edu}\\\\}
% \date{January 2017}
\date{\vspace{-6ex}}

\begin{document}

\maketitle

\subsection*{1. Conditional Probability}
In the following questions, \textbf{show your work}, not just the final answer.
\begin{enumerate}[label=(\alph*)]
\item The probability that an archer hits her target when it is windy is 0.4;
  when it is not windy, her probability of hitting the target is 0.7. On any
  shot, the probability of a gust of wind is 0.3. Find the probability that
  \begin{mdframed}
    Let the random variables involved be $W \in \{0, 1\}$ (wind no/yes) and
    $H \in \{0, 1\}$ (hit no/yes).
  \end{mdframed}
    \begin{enumerate}[label=(\roman*)]
        \item on a given shot there is a gust of wind and she hits her target.
          \begin{mdframed}
            $
            \Pr(W=1, H=1) = \Pr(W=1)\Pr(H=1|W=1) = 0.3 \cdot 0.4 = 0.12
            $
          \end{mdframed}
        \item she hits the target with her first shot.
          \begin{mdframed}
            $
            \Pr(H=1) = \sum_{w \in \{0, 1\}} \Pr(W=w) \Pr(H=1|W=w) = 0.7 \cdot 0.7 + 0.3 \cdot 0.4 = 0.61
            $
          \end{mdframed}
        \item she hits the target exactly once in two shots.
          \begin{mdframed}
            Each shot may be viewed as an independent draw of $(W,
            H)$. Therefore we use $\Pr(H=1)$ from part (ii) as the success
            probability in a binomial distribution:
            $$
            \Pr(\text{one hit in two trials}) = {2 \choose 1} \Pr(H=1)^1 \left(1 - \Pr(H=1)\right)^1 = 2 \cdot 0.61 \cdot 0.39 = 0.4758.
            $$
          \end{mdframed}
        \item there was no gust of wind on an occasion when she missed.
          \begin{mdframed}
            \begin{align*}
              \Pr(W=0|H=0)
              &= \frac{\Pr(W=0, H=0)}{\Pr(H=0)} \\\\
              &= \frac{\Pr(W=0) \Pr(H=0|W=0)}{\sum_{w \in \{0, 1\}} \Pr(W=w) \Pr(H=0|W=w)} \\\\
              &= \frac{0.7 \cdot 0.3}{0.7 \cdot 0.3 + 0.3 \cdot 0.6} \\\\
              &= 0.5385 ~~~\text{(4 d.p.)}
            \end{align*}
          \end{mdframed}
    \end{enumerate}

  \item Let $A, B, C$ be events. Show that if $$P(A|B, C) > P(A|B)$$
    then $$P(A|B, C^c) < P(A|B),$$ where $C^c$ denotes the complement of
    $C$. Assume that each event on which we are conditioning has positive
    probability.



    \begin{mdframed}
      % Intuitively, the fact we are given is the following: if you tell me that
      % both $B$ and $C$ occurred, then my belief that $A$ occurred is greater
      % than if you told me that $B$ occurred and gave me no information about
      % $C$. It is therefore intuitively reasonable that, telling me that $B$
      % occurred but $C$ did not occur decreases my belief that $A$ occurred,
      % compared to knowledge that $B$ occurred and ignorance about $C$.

      First, we expand the conditional probabilities involved in the given inequality:
      \begin{align*}
        \Pr(A|B,C) = \frac{\Pr(A,B)\Pr(C|A,B)}{\Pr(B)\Pr(C|B)} > \Pr(A|B) = \frac{\Pr(A,B)}{\Pr(B)}.
      \end{align*}
      Multiplying both sides by $\frac{\Pr(B)}{\Pr(A,B)}$ shows that
      \begin{align*}
        \frac{\Pr(C|A,B)}{\Pr(C|B)} > 1,
      \end{align*}
      i.e. $\Pr(C|A,B) > \Pr(C|B)$.

      We can transform that into a statement about $C^c$ by subtracting both sides from 1:
      \begin{align*}
        \Pr(C^c|A,B) = 1 - \Pr(C|A,B) < 1 - \Pr(C|B) = \Pr(C^c|B),
      \end{align*}
      i.e.
      \begin{align*}
        \frac{\Pr(C^c|A,B)}{\Pr(C^c|B)} < 1.
      \end{align*}

      Now, we want to show that $\Pr(A|B, C^c) < \Pr(A|B)$. The left hand side is
      \begin{align*}
        \P(A|B, C^c)
        &= \frac{\P(A,B)\P(C^c|A,B)}{\P(B)\P(C^c|B)}
        < \frac{\P(A,B)}{\P(B)} = \P(A|B),
      \end{align*}
      as required.

    \end{mdframed}

\end{enumerate}

\newpage

\subsection*{Positive Definiteness (2016)}

3.

(a) Give an explicit formula for $x^\T Ax$. Write your answer as a sum
involving the elements of $A$ and $x$.

\begin{mdframed}
  \begin{align*}
    x^\T A x = \sum_{i=1}^n \sum_{j=1}^n a_{ij}x_ix_j
  \end{align*}
\end{mdframed}

(b) Show that if $A$ is positive definite, then the entries on the diagonal of
$A$ are positive (that is, $a_{ii} > 0$ for all $1 \leq i \leq n$.)

\begin{mdframed}
  We prove the contrapositive: suppose $a_{ii} \leq 0$ for some
  $1 \leq i \leq n$. Now consider a particular $x$ containing zeros everywhere
  except for $x_i = 1$. Then $x^\T A x = a_{ii}x_i^2 = a_{ii} \leq 0$, so $A$
  is not positive definite.
\end{mdframed}


4.

(b) Let $A$ be positive definite. Prove that all eigenvalues of $A$ are greater than zero.
\begin{mdframed}
  Let $\lambda$ be an eigenvalue of $A$ and let $v \neq \vec 0$ be an eigenvector for
  this eigenvalue, so that $Av = \lambda v$. Since $A$ is positive
  definite, we have $v^\T Av = \lambda |v|^2 > 0$. Since
  $|v|^2 > 0$, we conclude $\lambda > 0$.
\end{mdframed}

(c) Let $A$ be positive definite. Prove that $A$ is invertible.
\begin{mdframed}
  $\det A$ is equal to the product of the eigenvalues. Since these are all
  positive $\det A > 0$ and so $A$ is invertible.
\end{mdframed}

(d) Let $A$ be positive definite. Prove that there exist $n$ linearly independent
vectors $x_1,x_2,...,x_n$ such that $A_{ij} = x_i^\T x_j$. (Hint: Use the spectral theorem
and what you proved in (b) to find a matrix $B$ such that $A = B^\T B$.)
\begin{mdframed}
  The spectral theorem states that
\end{mdframed}
\newpage

\subsection*{2. Positive Definiteness}
\textbf{Definition.} Let $A \in \mathbb{R}^{n \times n}$ be a symmetric matrix.
\begin{itemize}
    \item We say that $A$ is \textbf{positive definite} if $\forall x \in \mathbb{R}^n - \{0\}$, $x^{\top}Ax > 0$. We denote this with $A \succ 0$.
    \item Similarly, we say that $A$ is \textbf{positive semidefinite} if $\forall x \in \R^n$, $x^{\top}Ax \geq 0$. We denote this with $A \succeq 0$.
\end{itemize}
\begin{enumerate}[label=(\alph*)]
    \item For a symmetric matrix $A \in \R^{n\times n}$, prove that all of the following are equivalent.
    \begin{enumerate}[label=(\roman*)]
        \item $A \succeq 0$.
        \item $B^{\top} AB \succeq 0$, for some invertible matrix $B \in \R^{n\times n}$.
        \item All the eigenvalues of $A$ are nonnegative.
        \item There exists a matrix $U \in \R^{n\times n}$ such that $A = U U^{\top}$.
    \end{enumerate}

    (Suggested road map: (i) $\Leftrightarrow$ (ii), (i) $\Rightarrow$ (iii) $\Rightarrow$ (iv)$ \Rightarrow$ (i). For the implication (iii) $\Rightarrow$ (iv) use the \emph{Spectral Theorem for Symmetric Matrices}.

    \begin{mdframed}
      \textbf{(i) $\iff$ (ii)}

      Let $B = A^\T = A^\1$. Then $B$ is invertible and $B^\T AB =
      A$. Therefore $A \succeq 0 \iff B^\T AB \succeq 0$.

      \textbf{(i) $\implies$ (iii)}

      Let $\lambda$ be an eigenvalue of $A$ and let $v \neq \vec 0$ be an eigenvector for
      this eigenvalue, so that $Av = \lambda v$. Since $A$ is positive
      semidefinite, we have $v^\T Av = \lambda |v|^2 \geq 0$. Since
      $|v|^2 > 0$, we conclude $\lambda \geq 0$.

      % (Note that $\det A$ is the product of the eigenvalues, so if $A$ were
      % positive definite then it would be invertible. However as $A$ is positive
      % semidefinite it may not be.)

      \textbf{(iii) $\implies$ (iv)}

      We're asked to show that there exists a matrix $U$ such that $A = UU^\T$.

      Since $A$ is symmetric, by the Spectral Theorem for Symmetric Matrices
      its eigenvectors are orthonormal and it can be ``diagonalized'' as
      $A = U^*\Lambda U^{*\1}$ where the columns of $U^*$ are the eigenvectors of $A$
      and $\Lambda$ is a diagonal matrix containing the eigenvalues. Since the
      inverse of an orthogonal matrix is its transpose, we have
      $$
      A = U^*\Lambda U^{*\1} = U^*\Lambda U^{*\T}.
      $$
      Now define $U = U^*\Lambda^{1/2}$, where $\Lambda^{1/2}$ is a diagonal
      matrix containing the square roots of the eigenvalues:
      $\(\Lambda^{1/2}\)_{jj} = \sqrt{\lambda_j}$. Note that
      $U^\T = (U^*\Lambda^{1/2})^\T = \Lambda^{1/2}U^{*\T}$. Then
      \begin{align*}
        A = U^*\Lambda U^{*\T} = U^*\Lambda^{1/2}\Lambda^{1/2} U^{*\T} = UU^\T.
      \end{align*}

      \textbf{(iv) $\implies$ (i)}

      $A = UU^\T$ implies that $A$ is symmetric, since the following quantities are the same:
      \begin{enumerate}
      \item $i,j$-th element of $UU^\T$
      \item dot product of $U$ row $i$ and $U^T$ column $j$
      \item dot product of $U$ row $i$ and $U$ row $j$
      \item dot product of $U$ row $j$ and $U$ row $i$
      \item dot product of $U$ row $j$ and $U^\T$ column $i$
      \item $j,i$-th element of $UU^\T$.
      \end{enumerate}

      So we just need to show that a symmetric matrix is positive definite.
    \end{mdframed}

    \item For a symmetric positive definite matrix $A \succ 0 \in \R^{n\times n}$, prove the following.
    \begin{enumerate}[label=(\roman*)]
        \item For every $\lambda > 0$, we have that $A + \lambda I \succ 0$.
          \begin{mdframed}
            We want to show that $x^\T(A + \lambda I) x > 0$ for all $x \in \R^n$. We have
            \begin{align*}
              x^\T(A + \lambda I) x
              &= x^\T(Ax + \lambda Ix) \\
              &= x^\T Ax + \lambda x^\T x > 0
            \end{align*}
            where the inequality is true because $x^\T Ax > 0$ due to the
            positive definiteness of $A$, and $\lambda x^\T x > 0$ because
            $\lambda > 0$ and $x^\T x > 0$ because it is the square of the
            2-norm of $x$.
          \end{mdframed}
        \item There exists a $\gamma > 0$ such that $A - \gamma I \succ 0$.
          \begin{mdframed}
            We want to show that a $\gamma > 0$ exists such that
            \begin{align*}
              x^\T(A - \gamma I) x
              &= x^\T(Ax - \gamma Ix) \\
              &= x^\T Ax - \gamma x^\T x > 0
            \end{align*}
            for all non-zero $x \in \R^n$. To satisfy this, we can choose any
            $\gamma < \frac{x^\T Ax}{x^\T x}$. Both the numerator and
            denominator here are strictly positive (due to positive
            definiteness of $A$ and positivity of squared norm), so such a
            $\gamma > 0$ does exist.
            % For the case $n=2$, let the input variables be $x$ and $y$ and let
            % $A = \mat{a}{b}{b}{c}$. Since $A$ is positive definite, we have
            % $\cvec{x}{y}^\T A\cvec{x}{y} > 0$, which is a statement that a quadratic form in $x$
            % and $y$ has no real roots and is concave-up:
            % $$
            % ax^2 + 2bxy + cy^2 > 0.
            % $$
            % If we let $y = y_0 \neq 0$ be constant then we have a quadratic in $x$, the roots of which are
            % \begin{align*}
            %   x
            %   = \frac{-2by_0 \pm \sqrt{4b^2y_0^2 - 4acy_0^2}}{2a}
            %   = y_0\frac{-b \pm \sqrt{b^2 - ac}}{a}.
            % \end{align*}
            % Since we know that there are no real roots, we have
            % $$
            % ac - b^2 > 0,
            % $$
            % and therefore $ac > 0$, i.e. $a$ and $c$ agree in sign. And since
            % the quadratic surface is concave-up, we have $a > 0$ and $c > 0$, as required.
          \end{mdframed}
        \item $\sum_{i=1}^n \sum_{j=1}^n A_{ij} > 0$, where $A_{ij}$ is the element at the $i$-th row and $j$-th column of $A$.
    \end{enumerate}
\end{enumerate}

\newpage
\subsection*{3. Derivatives and Norms}
In the following questions, \textbf{show your work}, not just the final answer.
\begin{enumerate}[label=(\alph*)]
    \item Let $\x, \vec a \in \R^n$. Compute $\nabla_\x(\vec a^{\top}\x)$.
    \begin{mdframed}
      We view $\vec a$ as a constant vector and $\vec a^\T \x$ as a function
      $f:\R^n \rightarrow \R$ with $f(\x) = \vec a^\T \x = \sum_{i=1}^n a_ix_i$. The
      requested gradient is the column vector of first partial derivatives
      \begin{align*}
        \nabla_\x(\vec a^\T \x)
        = \begin{bmatrix}f_{x_1}\\f_{x_2}\\ \vdots\\f_{x_n}\end{bmatrix}
        = \begin{bmatrix}\frac{\partial a^\T x}{\partial x_1}\\ \frac{\partial a^\T x}{\partial x_2}\\ \vdots\\\frac{\partial a^\T x}{\partial x_n}\\\end{bmatrix}
        = \begin{bmatrix}a_1\\a_2\\ \vdots\\a_n\end{bmatrix}
        = \vec a.
      \end{align*}
    \end{mdframed}

  \item Let $A \in \R^{n\times n}$, $\x \in \R^n$. Compute $\nabla_\x(\x^{\top} A\x)$. \\
    How does the expression you derived simplify in the case that $A$ is symmetric? \\

    (Hint: to get a feeling for the problem, explicitly write down a $2 \times 2$ or $3 \times 3$ matrix $A$ with components $A_{11}$, $A_{12}$, etc., explicitly expand $x^{\top}Ax$ as a polynomial without matrix notation, calculate the gradient in the usual way, and put the result back into matrix form. Then generalize the result to the $n \times n$ case.)
    \begin{mdframed}
      \textbf{$2 \times 2$ symmetric}
      \begin{align*}
        \x^\T A\x &= A_{11}x_1^2 + 2A_{12}x_1x_2 + A_{22}x_2^2 \\
                  &= \sum_{jk}A_{jk}x_jx_k \\
        \\
        \nabla_\x(\x^{\top} A\x) &=
        \cvec
        {2A_{11}x_1 + 2A_{12}x_2}
        {2A_{12}x_1 + 2A_{22}x_2} = 2A\x
      \end{align*}
      \textbf{$2 \times 2$}
      \begin{align*}
        x^\T Ax &= A_{11}x_1^2 + (A_{12} + A_{21})x_1x_2 + A_{22}x_2^2 \\\\
        \nabla_x(x^{\top} Ax) &=
        \cvec
        {2A_{11}x_1 + (A_{12} + A_{21})x_2}
        {(A_{12} + A_{21})x_1 + 2A_{22}x_2} = (A + A^\T)\x
      \end{align*}
    \end{mdframed}

    \item Let $A, X \in \R^{n \times n}$. Compute $\nabla_X (\text{trace}(A^{\top}X))$.
    \begin{mdframed}
      We view $A$ as a constant matrix and $\trace A^\T X$ as a function
      $f:\R^{n \times n} \rightarrow \R$ with
      $$
      f(X)
      = \trace A^\T X
      = \sum_{j=1}^n A_{\cdot j} \cdot X_{\cdot j}
      = \sum_{j=1}^n \sum_{i=1}^n A_{ij}X_{ij},
      $$
      where $B_{\cdot j}$ represents the $j$-th column of the matrix $B$.

      The requested gradient is the matrix of first partial derivatives
      \begin{align*}
        \nabla_X (\text{trace}(A^{\top}X)) =
        \begin{bmatrix}
          \frac{\partial f}{\partial X_{11}} & \frac{\partial f}{\partial X_{12}} & \cdots & \frac{\partial f}{\partial X_{1n}} \\
          \frac{\partial f}{\partial X_{21}} & \frac{\partial f}{\partial X_{22}} & \cdots & \frac{\partial f}{\partial X_{2n}} \\
          \vdots                            & \vdots                             &        & \vdots                             \\
          \frac{\partial f}{\partial X_{n1}} & \frac{\partial f}{\partial X_{n2}} & \cdots & \frac{\partial f}{\partial X_{nn}} \\
        \end{bmatrix}
        =
        \begin{bmatrix}
          A_{11}  & A_{12} & \cdots & A_{1n} \\
          A_{21}  & A_{22} & \cdots & A_{2n} \\
          \vdots & \vdots &        & \vdots \\
          A_{n1}  & A_{n2} & \cdots & A_{nn} \\
        \end{bmatrix}
        = A.
      \end{align*}
    \end{mdframed}

    \item For a function $f: \R^d \rightarrow \R$ to be a norm, the distance metric $\delta(x, y) = f(x-y)$  must satisfy the triangle inequality. Is the function $f(x) = (\sqrt{|x_1|} + \sqrt{|x_2|})^2$ a norm for vectors $x \in \R^2$? Prove it or give a counterexample.
    \begin{mdframed} \solution
    % Solution here
    \end{mdframed}

    \item Let $x \in \R^n$. Prove that $\lVert x \rVert_{\infty} \leq \lVert x\rVert_2 \leq \sqrt{n} \lVert x \rVert_{\infty}$.
    \begin{mdframed} \solution
    % Solution here
    \end{mdframed}

    \item Let $x \in \R^n$. Prove that $\lVert x \rVert_2 \leq \lVert x \rVert_1 \leq \sqrt{n} \lVert x \rVert_2$. \\
    (Hint: The Cauchyâ€“Schwarz inequality may come in handy.)
    \begin{mdframed} \solution
    % Solution here
    \end{mdframed}

\end{enumerate}

\newpage
\subsection*{4. Eigenvalues}
Let $A \in \R^{n\times n}$ be a symmetric matrix with $A \succeq 0$.
\begin{enumerate}[label=(\alph*)]
    \item Prove that the largest eigenvalue of $A$ is $$\lambda_{\max}(A) = \max_{\lVert x \rVert_2 = 1} x^{\top} Ax.$$ \\
    (Hint: Use the \emph{Spectral Theorem for Symmetric Matrices} to reduce the problem to the diagonal case.)
    \begin{mdframed} \solution
    % Solution here
    \end{mdframed}

    \item Similarly, prove that the smallest eigenvalue of $A$ is $$\lambda_{\min}(A) = \min_{\lVert x\rVert_2 = 1} x^{\top} Ax.$$
    \begin{mdframed}
    \solution %Solution here
    \end{mdframed}

    \item Is either of the optimization problems described in parts (a) and (b) a convex program? Justify your answer.
    \begin{mdframed}
    \solution % Solution here
    \end{mdframed}

    \item Show that if $\lambda$ is an eigenvalue of $A$ then $\lambda^2$ is an eigenvalue of $A^2$, and deduce that $$\lambda_{\max}(A^2) = \lambda_{\max}(A)^2 \text{ and } \lambda_{\min}(A^2) = \lambda_{\min}(A)^2.$$
    \begin{mdframed}
    \solution % Solution here
    \end{mdframed}

    \item From parts (a), (b), and (d), show that for any vector $x \in \R^n$ such that $\lVert x \rVert_2 = 1$, $$\lambda_{\min}(A) \leq \lVert Ax \rVert_2 \leq \lambda_{\max}(A).$$
    \begin{mdframed}
    \solution % Solution here
    \end{mdframed}

    \item From part (e), deduce that for any vector $x \in \R^n$, $$\lambda_{\min}(A) \lVert x \rVert_2 \leq \lVert Ax \rVert_2 \leq \lambda_{\max}(A)\lVert x \rVert_2.$$
    \begin{mdframed}
    \solution % Solution here
    \end{mdframed}
\end{enumerate}

\newpage
\subsection*{5. Gradient Descent}
Consider the optimization problem $\min_{x \in \R^n} \frac{1}{2} x^{\top} Ax - b^{\top}x$, where $A$ is a symmetric matrix with $0 < \lambda_{\min}(A)$ and $\lambda_{\max} (A) < 1$.
\begin{enumerate}[label=(\alph*)]
\item Using the first order optimality conditions, derive a closed-form
  solution for the minimum possible value of $x$, which we denote $x^*$.
  \begin{mdframed}
    Let $f(\x) = \frac{1}{2} x^\T Ax - b^{\top}x$. Since
    $x^\T Ax = \sum_{j,k}A_{jk}x_jx_k$, the gradient in the $x_j$ direction is
    \begin{align*}
      (\nabla_x f)_j = \sum_k A_{jk} x_k - b_j
    \end{align*}
    (the factor of $1/2$ cancels the 2s deriving from differentiating $x_j^2$
    and $2x_jx_k$).

    In other words,
    \begin{align*}
      \nabla_x f = A\x - \vec b.
    \end{align*}
    Setting this equal to zero gives $x^* = A^\1b$.


    Compare the 1D version:
    $f(x) = \frac{1}{2}ax^2 - bx \implies f'(x) = ax - b \implies x^* = b/a$.
  \end{mdframed}

  \item Solving a linear system directly using Gaussian elimination takes
    $O(n^3)$ time, which may be wasteful if the matrix $A$ is sparse. For this
    reason, we will use gradient descent to compute an approximation to the
    optimal point $x^*$. Write down the update rule for gradient descent with a
    step size of 1.
    \begin{mdframed}
      \begin{align*}
        &\text{for $j$ in $1 \ldots d$}\\
        &~~~~~~~~x^{(i)}_j \leftarrow x^{(i-1)}_j - \sum_k A_{jk} x^{(i-1)}_k + b_j
      \end{align*}
      Or in other words,
      \begin{align*}
        x^{(i)}
        &\leftarrow x^{(i-1)} - A x^{(i-1)} + b \\
        &= (I - A)x^{(i-1)} + b
      \end{align*}

    \end{mdframed}

  \item Show that the iterates $x^{(i)}$ satisfy the recursion
    $$x^{(i)} - x^* = (I-A)(x^{(i-1)} - x^*).$$
    \begin{mdframed}
      \begin{align*}
        x^{(i)} - x^*
        &= (I - A)x^{(i-1)} + b - x^* \\
        &= (I - A)x^{(i-1)} + Ax^* - x^* \\
        &= (I - A)x^{(i-1)} + (A - I)x^* \\
        &= (I - A)(x^{(i-1)} - x^*) \\
      \end{align*}
    \end{mdframed}

  \item Show that for some $0 < \rho < 1$,
    $$\lVert x^{(i)} - x^* \rVert_2 \leq \rho \lVert x^{(i-1)} - x^*
    \rVert_2.$$
    \begin{mdframed}
    \solution % Solution here
    \end{mdframed}

  \item Let $x^{(0)} \in \R^n$ be a starting value for our gradient descent
    iterations. If we want our solution $x^{(i)}$ to be $\epsilon > 0$ close to
    $x^*$, i.e. $\lVert x^{(i)} - x^* \rVert_2 \leq \epsilon$, then how many
    iterations of gradient descent should we perform? In other words, how large
    should $k$ be? Give your answer in terms of
    $\rho, \lVert x^{(0)} - x^*\rVert_2, $ and $\epsilon$. Note that
    $0 < \rho < 1$, so $\log \rho < 0$.
    \begin{mdframed}
    \solution % Solution here
    \end{mdframed}

  \item Observe that the running time of each iteration of gradient descent is
    dominated by a matrix-vector product. What is the overall running time of
    gradient descent to achieve a solution $x^{(i)}$ which is $\epsilon$-close
    to $x^*$? Give your answer in terms of
    $\rho, \lVert x^{(0)} - x^*\rVert_2, \epsilon,$ and $n$.
    \begin{mdframed}
    \solution % Solution here
    \end{mdframed}
\end{enumerate}

\newpage
\subsection*{6. Classification}
Suppose we have a classification problem with classes labeled $1, \ldots, c$ and an additional "doubt" category labeled $c+1$. Let $f: \R^d \rightarrow \{1, \ldots, c+1\}$ be a decision rule. Define the loss function
$$ R(f(x) = i|x) =
\begin{cases}
0 & \text{if }i = j \quad i, j \in \{1, \ldots, c\} \\
\lambda_r & \text{if } i=c + 1 \\
\lambda_s & \text{otherwise}
\end{cases} $$
where $\lambda_r \geq 0$ is the loss incurred for choosing doubt and $\lambda_s \geq 0$ is the loss incurred for making a misclassification. Hence the risk of classifying a new data point $x$ as class $i \in \{1, 2, \ldots, c+1\}$ is $$R(f(x) = i|x) = \sum_{j=1}^c L(f(x) = i, y = j) P(Y = j|x).$$
\begin{enumerate}[label=(\alph*)]
    \item Show that the following policy obtains the minimum risk. (1) Choose class $i$ if $P(Y = i|x) \geq P(Y = j|x)$ for all $j$ and $P(Y=i|x) \geq 1 - \lambda_r / \lambda_s$; (2) choose doubt otherwise.
    \begin{mdframed}
    \solution % Solution here
    \end{mdframed}

    \item What happens if $\lambda_r = 0$? What happens if $\lambda_r > \lambda_s$?  Explain why this is consistent with what one would expect intuitively.
    \begin{mdframed}
    \solution % Solution here
    \end{mdframed}
\end{enumerate}

\newpage
\subsection*{7. Gaussian Classification}
Let $P(x | \omega_i) \sim N(\mu_i, \sigma^2)$ for a two-category,
one-dimensional classification problem with classes $\omega_1$ and $\omega_2$,
$P(\omega_1) = P(\omega_2) = 1/2$, and $\mu_2 > \mu_1$.

\begin{enumerate}[label=(\alph*)]

\item Find the Bayes optimal decision boundary and the corresponding Bayes decision rule.
    \begin{mdframed}
      A Bayes optimal decision boundary for a one-dimensional, two-class
      problem is a point $x^*$ at which the two class posterior probabilities
      are equal. Since the variances and priors are equal the problem is
      symmetric and it seems intuitively clear that the decision boundary must
      be $x^* = \frac{\mu_1 + \mu_2}{2}$, with rule
      \begin{align*}
        f(x) =
        \begin{cases}
          \omega_1, &x < x^* \\
          \omega_2, &x > x^*
        \end{cases}
      \end{align*}
      (undefined classification exactly at the boundary).

      To prove this, first note that the posterior probability of membership of
      a point $x$ in class $\omega_i$ is
      \begin{align*}
        % \p(\omega_i|x) = \frac{\p(\omega_i)\p(\omega_i|x)}{\p(\omega_1)\p(\omega_1|x) + \p(\omega_2)\p(\omega_2|x)}.
        \p(\omega_i|x)
        &= \frac{\p(\omega_i)\p(\omega_i|x)}{\p(x)} \\
        &= \frac{1}{2\p(x)} \frac{1}{(\sqrt{2\pi}\sigma)}\exp(-\frac{(x - \mu_i)^2}{2\sigma^2})
      \end{align*}
      Viewed as a function of $\omega_i$, the log posterior is
      \begin{align*}
        \log \p(\omega_i|x) = -\frac{(x - \mu_i)^2}{2\sigma^2} + \constant,
      \end{align*}
      so the decision boundary $x^*$ satisfies
      \begin{align*}
        &-\frac{(x^* - \mu_1)^2}{2\sigma^2} = -\frac{(x^* - \mu_2)^2}{2\sigma^2} \\
        \implies&(x^* - \mu_1)^2 = (x^* - \mu_2)^2 \\
        % \implies&x^*(-2\mu_1 + 2\mu_2) = \mu_2^2 - \mu_1^2 \\
        \implies&x^* = \frac{\mu_2^2 - \mu_1^2}{2(\mu_2 - \mu_1)} = \frac{\mu_2 + \mu_1}{2}. \\
      \end{align*}

    \end{mdframed}

  \item The Bayes error is the probability of misclassification,
    $$ P_e = P((\text{misclassified as }\omega_1) | \omega_2)P(\omega_2) +
    P((\text{misclassified as }\omega_2)|\omega_1)P(\omega_1).$$ Show that the
    Bayes error associated with this decision rule is
    $$ P_e = \frac{1}{\sqrt{2\pi}} \int_a^{\infty} e^{-z^2/2} dz$$ where $a = \frac{\mu_2 - \mu_1}{2\sigma}$.
    \begin{mdframed}
      Let the random variables $X$ and $Y$ represent the sample point and its
      class respectively.  The probability of misclassification is
      \begin{align*}
        P_e
        &= P((\text{misclassified as }\omega_1) | Y=\omega_2)P(Y=\omega_2) +
           P((\text{misclassified as }\omega_2) | Y=\omega_1)P(Y=\omega_1) \\
        &= \frac{1}{2}\(\p(X < x^*|Y=\omega_2) + \p(X > x^*|Y=\omega_1)\).
      \end{align*}
      These two probability distributions are 1D Gaussians with variance
      $\sigma^2$ and means $\mu_2$ and $\mu_1$ respectively. Now change the
      parameterization of these Gaussians so that they both have variance 1 and
      mean 0. The above probability becomes
      \begin{align*}
        P_e
        &= \frac{1}{2}\(
          \p(X < \frac{x^* - \mu_2}{\sigma}|Y=\omega_2) +
          \p(X > \frac{x^* - \mu_1}{\sigma}|Y=\omega_1)
        \) \\
        &= \frac{1}{2\sqrt{2\pi}}\(
          \int_{-\infty}^{(x^* - \mu_2)/\sigma} e^{-z^2} \d z +
          \int_{(x^* - \mu_1)/\sigma}^{\infty} e^{-z^2} \d z
          \)
      \end{align*}

    \end{mdframed}
\end{enumerate}

\newpage
\subsection*{8. Maximum Likelihood Estimation}
Let $X$ be a discrete random variable which takes values in $\{1, 2, 3\}$ with probabilities $P(X = 1) = p_1, P(X=2) = p_2,$ and $P(X = 3) = p_3$, where $p_1 + p_2 + p_3 = 1$.  Show how to use the method of maximum likelihood to estimate $p_1, p_2,$ and $p_3$ from $n$ observations of $X: x_1, \ldots, x_n$. Express your answer in terms of the counts $$k_1 = \sum_{i=1}^n \mathbbm{1}(x_i = 1), k_2 = \sum_{i=1}^n \mathbbm{1}(x_i = 2), \text{ and }k_3 = \sum_{i=1}^n \mathbbm{1}(x_i = 3),$$ where
$$\mathbbm{1}(x = a) =
\begin{cases}
1 & \text{if } x = a \\
0 & \text{if } x \neq a.
\end{cases}$$
\begin{mdframed}
  Let the observed data vector be $\vec k = [k_1,k_2,k_3]^\T$ and the parameter
  vector be $\vec p = [p_1, p_2, p_3]^\T$. The sampling model is
  $\vec k \sim \text{Multinomial}(\vec p)$, so the probability of the observed
  data vector is
  \begin{align*}
    \Pr(\vec k | \vec p) = \frac{n!}{k_1!k_2!k_3!}\prod_{j=1}^3 p_j^{k_j},
  \end{align*}
  giving the following log-likelihood function:
  \begin{align*}
    l(\vec p) = \log \Pr(\vec k | \vec p) = \sum_{j=1}^3 k_j \log p_j.
  \end{align*}
  We want to maximize this log-likelihood subject to the constraint that
  $\sum_j p_j = 1$. To do so, we maximize the Lagrangian
  \begin{align*}
    \Lagr(\vec p, \lambda) = \sum_{j=1}^3 k_j \log p_j - \lambda\left(\sum_{j=1}^3 p_j - 1\right).
  \end{align*}
  The gradient of the Lagrangian is
  \begin{align*}
    \nabla_{\Lagr} =
    \begin{bmatrix}
      \partial \Lagr / \partial p_1\\
      \partial \Lagr / \partial p_2\\
      \partial \Lagr / \partial p_3\\
      \partial \Lagr / \partial \lambda\\
    \end{bmatrix} =
    \begin{bmatrix}
      k_1/p_1 - \lambda\\
      k_2/p_2 - \lambda\\
      k_3/p_3 - \lambda\\
      1 - \sum_{j=1}^3 p_j\\
    \end{bmatrix}.
  \end{align*}
  Solving $\nabla_{\Lagr} = 0$ yields $k_j = \widehat \lambda \widehat p_j$ and
  $\sum_j \widehat p_j = 1$. Therefore
  $n = \sum_j k_j = \widehat \lambda \sum_j \widehat p_j = \widehat \lambda$,
  giving the maximum likelihood parameter estimates
  $$
  \widehat p_j = \frac{k_j}{n}.
  $$
  (Confirm that this point is a maximum.)
\end{mdframed}


\end{document}
