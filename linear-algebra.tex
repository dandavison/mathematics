\section{Definitions}
\subsection{Subspace}
A subspace $U$ of $V$ is a subset of $V$ for which
\begin{enumerate}
\item $0 \in U$
\item For any finite subset $U^* \subset U$, the set of all linear combinations
  of $U^*$ is also a subset of $U$.
\end{enumerate}

\subsection{Complex numbers}
When viewed as a real vector space (i.e. with real scalars), $\C$ is
two-dimensional, e.g. $\{1, i\}$ is a basis.

When viewed as a complex vector space (i.e. with complex scalars), $\C$ has a
more one-dimensional character: $\{1, i\}$ are no longer linearly independent.

\footnotetext{
  \href{https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab}{Essence of Linear Algebra} video series by \href{http://www.3blue1brown.com/}{Grant Sanderson / 3blue1brown}
}

\section{Examples of vector spaces}
\begin{enumerate}
\item The set $\R^n$ of $n-$tuples of real numbers, under componentwise
  addition and multiplication by real scalars.
\item The set $M_{m \times n}(\R)$ of $m\times n$ matrices, under componentwise
  addition and multiplication by real scalars.
\item The field $\C$ with multiplication by real scalars is equivalent to $\R^2$.
\item The set of solutions of a system of \textit{homogeneous} linear equations
  in $n$ variables is a subspace $V$ of $\R^n$. (Let $A$ be the matrix
  representing the system and let $u$ and $v$ be solutions. Then $Au = Av = 0$
  and $V$ is a subspace since $A(u + v) = Au + Av = 0$, and
  $A(\lambda u) = \lambda Au = 0$.)
\item The set $\R_n[x]$ of polynomials of degree $\leq n$ is a real vector
  space.
\item The set $\R^X$ of real-valued functions on any set $X$ is a real vector
  space. Examples:
  \begin{enumerate}
  \item Let $[n] = \{1, 2, \ldots, n\}$. Note that the function space $\R^{[n]}$ is
    the same as $\R^n$ (both are sets of $n-$tuples of reals).
  \item Similarly, $\R^{[m]\times[n]}$ is the same as $M_{m \times n}(\R)$.
  \item $\R^\R$, the set of all functions $\R\to\R$.
  \item The set of continuous functions $\R \to \R$, and differentiable functions
    $\R \to \R$, under pointwise addition and pointwise scalar multiplication
    (from any field?).
  \item Set of solutions of a homogeneous linear ODE
  \end{enumerate}
\item Sequences $(a_n)$ of real numbers, under term-wise addition and term-wise
  scalar multiplication, form a vector space, identifiable with the function
  space $\R^\N$. Examples:
  \begin{enumerate}
  \item Set of convergent sequences
  \end{enumerate}
\end{enumerate}
\section{Span, basis, dimension}
\begin{theorem}
  Every basis has the same size.
\end{theorem}

\begin{proof} Let $v_1, \ldots, v_n$ be a basis for a vector space $V$.


\end{proof}

\begin{theorem}
  A spanning set that is the same size as a basis is also a basis.
\end{theorem}

\begin{proof}
  Let $v_1, \ldots, v_n$ be a basis for a vector space $V$, and let
  $u_1, \ldots, u_n$ span $V$.

  We know that $v_1, \ldots, v_n$ are linearly independent and that if we
  remove any one of them they will cease to span.

  We want to show that $u_1, \ldots, u_n$ are linearly independent.

  Suppose, that the $u_i$ are not linearly independent and that
  $u_2, \ldots, u_n$ span $V$. Thus there are $n-1$ vectors in this spanning
  set. But the Steinitz Exchange Lemma states that if $v_1, \ldots, v_n$ are
  linearly independent and $u_1, \ldots, u_m$ span, then $n \leq m$. This
  contradiction proves that the $u_i$ are linearly independent.
\end{proof}
\section{Linear transformations and matrices}

A linear transformation is completely specified by

\begin{enumerate}
\item Some basis vectors $i$ and $j$
\item Where those basis vectors are taken to by the transformation.
\end{enumerate}

How the transformation affects any other point follows from those two pieces of
information.

So $i$ might be taken to $ai + bj$, and $j$ might be taken to $ci + dj$.
In this case we would use the following matrix to describe the
transformation:

$$
\mat{a}{c}
    {b}{d}
$$

Some examples are

$$
\begin{array}{ll}
\text{stretch by a in the i-direction} & \mat{a}{0}
                                             {0}{1}
\\\\
\text{stretch by a in the i-direction and shear right} & \mat{a}{b}
                                                             {0}{1}
\\\\
\text{rotate anticlockwise 90°} & \mat{0}{-1}
                                      {1}{ 0}
\end{array}
$$

Note that we haven't said what $i$ and $j$ are yet; they \textit{define} the
2-dimensional space that we're considering. But, we can think of them for now
as the usual orthogonal unit vectors in 2D space.

So the matrix tells us where the basis vectors have been taken to. Any other
vector $fi + gj$ is taken to wherever that is using the transformed basis
vectors:

$$
fi + gj \longrightarrow f\cvec{a}{b} + g\cvec{c}{d} = \cvec{fa + gc}{fb + gd}
$$


And that's how matrix multiplication is defined:

$$
\mat{a}{c}
    {b}{d} \cvec{f}{g} = \cvec{fa + gc}{fb + gd}
$$


A matrix represents a linear transformation by showing where the basis vector
are taken to.


\section{Change of basis}

Suppose person B uses some other basis vectors to describe locations in
space. Specifically, in our coordinates, their basis vectors are
$\scvec{2}{1}$ and $\scvec{-1}{1}$.


\textbf{When they state a vector, what is it in our coordinates?}

If they say $\scvec{-1}{2}$, what is that in our coordinates?

Well, if they say $\scvec{1}{0}$, that's $\scvec{2}{1}$ in our coordinates. And
if they say $\scvec{0}{1}$, that's $\scvec{-1}{1}$ in our coordinates. So the
matrix containing \textit{their basis vectors expressed using our coordinate system}
transforms a point expressed in their coordinate system into one expressed in
ours. That last sentence is critical, so hopefully it makes sense! So, the answer is

$$
\mat{2}{-1}
    {1}{ 1} \cvec{-1}{2} = \cvec{-4}{1}.
$$


\textbf{When we state a vector, what is it in their coordinates?}

We give the vector $\scvec{3}{2}$. What is that in their coordinate system? By
definition, the answer is the weights that scales their basis vectors to hit
$\scvec{3}{2}$. So, the solution to

$$
\mat{2}{-1}
    {1}{1} \cvec{a}{b} = \cvec{3}{2}.
$$


Computationally, we can see that we can get the solution by multiplying both
sides by the inverse:

$$
\cvec{a}{b} = \mat{2}{-1}
                  {1}{1}^{-1} \cvec{3}{2}.
$$

Conceptually, we have

$$
\mat{2}{-1}
    {1}{1} =
\begin{bmatrix}\text{matrix converting their}\\\text{representation to ours} \\ \end{bmatrix}
$$

where "their representation" means the vector expressed using their coordinate
system. So the role played by the inverse is

$$
\cvec{a}{b} =
\begin{bmatrix}\text{matrix converting our}\\\text{representation to theirs} \\ \end{bmatrix}
\cvec{3}{2}.
$$

\textbf{When we state a transformation, what is it in their coordinates?}

We state a 90° anticlockwise rotation of 2D space:

$$
\mat{0}{-1}
    {1}{0}
$$

what is that transformation in their coordinates? The answer is

$$
\begin{bmatrix}\text{matrix converting our}\\\text{representation to theirs} \\ \end{bmatrix}
\mat{0}{-1}
    {1}{0}
\begin{bmatrix}\text{matrix converting their}\\\text{representation to ours} \\ \end{bmatrix}
$$

since the composition of those three transformations defines a single
transformation that takes in a vector expressed in their coordinate system,
converts it to our coordinate system, transforms it as requested, and then
converts back to theirs.


\section{Symmetric matrices}

\textbf{Spectral theorem for symmetric matrices}

Symmetric $n \by n$ matrix $A$ (real).

$A^\1 = A^\T$

$n$ orthogonal eigenvectors with real eigenvalues.

Orthonormal matrix $U$ containing normalized eigenvectors.

$A = U\Lambda U^\1 = U\Lambda U^\T$

(Eigenvalues are uniquely determined by matrix. Eigenvalues can be repeated, in which case any linear combination of their
eigenvalues is also an eigenvalue.)



\section{Linear and quadratic approximations to a function}
\footnote{
  \href{https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions/a/reasoning-behind-the-second-partial-derivative-test}{khanacademy - Grant Sanderson - second partial derivative test}
}




We construct first- and second-order approximations to a differentiable
function $f: \R^2 \rightarrow \R$. The approximation is made at some point
$(x_0, y_0) = \vec x_0 \in \R^2$; we demand that the value of the approximation, and the
first and second derivatives, match those of $f$ exactly at that point.

\subsection{Linear approximation to a function $f(x, y)$ near $(x_0, y_0)$:}

\begin{align*}
L(x, y) &=
~
f(x_0, y_0) ~+
(x - x_0)f_x(x_0,y_0) +
(y - y_0)f_y(x_0,y_0)
\\\\
&= f(\vec x) + (\vec x - \vec x_0) \cdot \nabla_f(\vec x_0)
\end{align*}

Note that, at $(x_0, y_0)$, the first partial derivatives of $L$ are equal to
those of $f$, as they must be. (In fact, we could say that the coefficients are
determined by this requirement; see the quadratic case below. But the linear
case is obvious without ``deriving'' the coefficients.)


\subsection{Quadratic approximation to a function $f(x, y)$ near $(x_0, y_0)$:}

First note that the ``quadratic form'' $ax^2 + 2bxy + cy^2$ can be written as
\begin{align*}
\x^\T A \x = \cvec{x}{y}^\T \mat{a}{b}
                                {b}{c} \cvec{x}{y}.
\end{align*}
This is a scalar. In general, a quadratic form for symmetric matric $A$ is
\begin{align*}
\x^\T A \y = \sum_{jk}A_{jk}x_jy_k.
\end{align*}

The $j$-th component of the gradient of $q(\x) = \x^\T A \x$ is
$\partiald{q}{x_j} = 2\sum_k A_{jk}x_k$, so
\begin{align*}
  \nabla ~\x^\T \vec A \x = 2\vec A \x.
\end{align*}

\begin{align*}
Q(x, y) &=
f(\vec x_0) + (x - x_0)f_x(\vec x_0) +
(y - y_0)f_y(\vec x_0) ~+ \\
&~~~~~~~\frac{1}{2} f_{xx}(\vec x_0)(x - x_0)^2 +
f_{xy}(\vec x_0)(x - x_0)(y - y_0) +
\frac{1}{2} f_{yy}(\vec x_0)(y - y_0)^2 \\\\
&= f(\vec x_0) +
(\vec x - \vec x_0) \cdot \nabla f(\vec x_0) +
\frac{1}{2}(\vec x - \vec x_0)^\T \nabla^2 f(\vec x_0)(\vec x - \vec x_0),
\end{align*}
where $\nabla^2 f(\vec x_0)$ is the Hessian matrix $\mat{f_{xx}}{f_{xy}}
                                                        {f_{yx}}{f_{yy}}$ evaluated at $\vec x_0$.

\subsection{Second partial derivative test and positive definiteness of Hessian}

The second partial derivative test for a function of two variables states that
we examine the determinant of the Hessian evaluated at the critical point:
$$
D = \det \nabla^2 f(\vec x_0) = f_{xx}(\vec x_0)f_{yy}(\vec x_0) - f_{xy}(\vec x_0)^2.
$$

Notice that $D \geq 0$ implies that the sign of $f_{xx}$ and $f_{yy}$ agree
(because we're subtracting the square of the mixed partial $f_{xy}$, i.e. a
positive number).

\begin{tabular}{ l l l l l }
  $D$    & roots          & $f_{xx}$ &  & Hessian \\
  \hline
  $+$    & no real roots  & $+$     & minimum        & positive definite \\
  $+$    & no real roots  & $-$     & maximum        & negative definite \\
  $0$    & one real root  & $+$     & minimum        & positive semidefinite \\
  $0$    & one real root  & $-$     & maximum        & negative semidefinite \\
  $-$    & two real roots & n/a     & saddle point   & - \\
\end{tabular}

\subsubsection*{Explanation}
At a critical point $\vec x_0$, the gradient is zero and the quadratic approximation is therefore
$$
Q(x, y) = f(\vec x_0) + \frac{1}{2}(\vec x - \vec x_0)^\T \nabla^2 f(\vec x_0)(\vec x - \vec x_0).
$$
So if this is a minimum (concave-up paraboloid) then this quadratic form is
positive for all $\vec x \neq \vec x_0$ (and if it's a maximum then it's
negative for all $\vec x \neq \vec x_0$).

Basically the argument is that, instead of analyzing the function $f$ itself,
we analyze its quadratic approximation at the critical point. So the question
comes down to: how do we determine whether a quadratic form is always positive,
always negative, or takes positive and negative values?

To answer that, consider a generic quadratic form $ax^2 + 2bxy + cy^2$. Let $y$
be constant at $y_0$; then we have a quadratic in $x$, the roots of which are
\begin{align*}
  x
  = \frac{-2by_0 \pm \sqrt{4b^2y_0^2 - 4acy_0^2}}{2a}
  = y_0\frac{-b \pm \sqrt{b^2 - ac}}{a}.
\end{align*}
So, whether this is a saddle point or a minimum/maximum depends on whether the
quadratic form has real roots. If there are no real roots, then whether it's a
minimum or a maximum depends on the sign of $f_{xx}$ (this sign will be the
same as that of $f_{yy}$ in the no real roots case).

\subsection{Derivation of quadratic approximation coefficients}
\begin{align*}
Q(x, y) =
&f(\vec x_0) + (x - x_0)f_x(\vec x_0) +
(y - y_0)f_y(\vec x_0) ~+ \\
&a(x - x_0)^2 +
b(x - x_0)(y - y_0) +
c(y - y_0)^2
\end{align*}

What are the coefficients $a,b,c$? They are determined by the requirement that
the second partial derivatives are identical at the point of approximation
$\vec x_0$.

First look at the first partial derivatives:

\begin{align*}
  Q_x &= f_x(\vec x_0) + 2a(x - x_0) + b(y - y_0)\\
  Q_y &= f_y(\vec x_0) + b(x - x_0) + 2c(y - y_0)\\
\end{align*}
so the quadratic approximation is an exact first-order approximation at $\vec x_0$, as required:
\begin{align*}
  Q_x(\vec x_0) &= f_x(\vec x_0) \\
  Q_y(\vec x_0) &= f_y(\vec x_0),
\end{align*}

Now look at the second derivatives:
\begin{align*}
  Q_{xx} &= 0 + 2a + 0 \\
  Q_{xy} &= 0 + 0 + b \\
  Q_{yx} &= 0 + b + 0 \\
  Q_{yy} &= 0 + 0 + 2c \\
\end{align*}
Since we require that these match those of $f$ exactly at $\vec x_0$, we have
\begin{align*}
  a &= \frac{1}{2} f_{xx}(\vec x_0) \\
  b &= f_{xy}(\vec x_0) = f_{yx}(\vec x_0) \\
  c &= \frac{1}{2} f_{yy}(\vec x_0),
\end{align*}
so the quadratic approximation is
\begin{align*}
Q(x, y) =
&f(\vec x_0) + (x - x_0)f_x(\vec x_0) +
(y - y_0)f_y(\vec x_0) ~+ \\
&\frac{1}{2} f_{xx}(\vec x_0)(x - x_0)^2 +
f_{xy}(\vec x_0)(x - x_0)(y - y_0) +
\frac{1}{2} f_{yy}(\vec x_0)(y - y_0)^2
\end{align*}
\section{Finding the nth Fibonacci number via an eigenvector change of basis}


This is the problem given at the end of the eigenvectors video in the
Essence of Linear Algebra\footnote{\url{https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab}}
series by 3blue1brown\footnote{\url{http://www.3blue1brown.com/}}.


\subsection*{Introduction}

Consider the matrix

$$
A = \mat{0}{1}
        {1}{1}
$$

The first few powers are

\begin{align*}
&A^{1} &= \mat{0}{1}
              {1}{1}
\\
&A^{2} = \mat{0}{1}
             {1}{1} \mat{0}{1}
                        {1}{1} &= \mat{1}{1}
                                      {1}{2}
\\
&A^{3} = \mat{0}{1}
             {1}{1} \mat{1}{1}
                        {1}{2} &= \mat{1}{2}
                                      {2}{3}
\\
&A^{4} = \mat{0}{1}
             {1}{1} \mat{1}{2}
                        {2}{3} &= \mat{2}{3}
                                      {3}{5}
\end{align*}

The Fibonacci sequence is the sequence you get by starting with $0,
1$ and after that always forming the next number by adding the two previous ones:
$F_0, F_1, F_2, F_3, F_4, F_5, F_6, F_7, ...$ = $0, 1, 1, 2, 3, 5, 8, 13, ...$.

The matrix powers are generating the Fibonacci sequence:

$$
A^{n} = \mat{F_{n-1} }{F_n      }
            {F_n     }{F_{n+1} }
$$

So if there were a way to compute the $\nth$ power of that matrix "directly",
that would also be a way to compute the $\nth$ Fibonacci number "directly",
i.e. without computing all the preceding Fibonacci numbers \textit{en route}.

How can we do this? To state the problem in a different way, we need to
construct a new matrix that performs exactly the same transformation as $A^n$,
but which somehow does the exponentiation step "in one go" rather than by
multiplying $A$ with itself $n$ times.

\subsection*{Solution outline}

Matrices represent transformations, so we can talk about them as taking in some
vector and producing some other vector. The approach we're going to take is to
re-express the $A^n$ transformation as follows:

1. Convert the input vector to its representation in an alternative basis which
   uses the eigenvectors as the basis vectors (it's called an "eigenbasis").
2. In this alternative basis, compute the new position of the vector after
   carrying out the $A^n$ transformation.
3. Convert the resulting vector back to its representation in our original
   basis.

I.e., we're going to compute the overall transformation as this product of
matrices (remember that one reads these things right-to-left):

$$
\begin{bmatrix}\text{matrix converting their}\\\text{representation to ours} \\ \end{bmatrix}
\begin{bmatrix}\text{matrix that does the A transformation}\\\text{in the alternative basis} \\ \end{bmatrix}^n
\begin{bmatrix}\text{matrix converting our}\\\text{representation to theirs} \\ \end{bmatrix}
$$

The crux of all this is that the exponentiation is efficient in the
eigenbasis. That's because, in the eigenbasis, the transformation is just
stretching space in the directions of the two basis vectors. So to do the
transformation $n$ times in the eigenbasis, you just stretch by the
stretch-factor raised to the $\nth$ power, rather than doing $n$ matrix
multiplications.

\subsection*{Solution details}

Let's suppose we've already found the eigenvectors, and that there are two of
them, and that we've arranged them as the two columns of a matrix $V$. $V$ holds
the basis vectors of the alternative basis, and therefore we know from the
[change of basis](\url{./linear-algebra.html#change-of-basis}) notes that $V$ is the
matrix that takes as input a vector expressed in the alternative basis and
outputs its representation in our basis.

So, step (3) is done by $V$, and step (1) is done by $V^{-1}$, and the matrix
performing all three steps is going to look like

$$
V
\begin{bmatrix}\text{matrix that does the A transformation}\\\text{in the alternative basis} \\ \end{bmatrix}^n
V^{-1}
$$

OK, so what is the matrix in the middle? The
[change of basis](\url{./linear-algebra.html#change-of-basis}) notes tell us that we
can compute it as

$$
\begin{bmatrix}\text{matrix converting our}\\\text{representation to theirs} \\ \end{bmatrix}
A
\begin{bmatrix}\text{matrix converting their}\\\text{representation to ours} \\ \end{bmatrix}
$$

In other words the matrix in the middle is

$$
V^{-1}AV
$$

and the entire transformation is

$$
V
\Big(V^{-1}AV\Big)^n
V^{-1}
$$

Put back into words, that's

$$
\begin{bmatrix}\text{matrix converting their}\\\text{representation to ours} \\ \end{bmatrix}
\Bigg(
    \begin{bmatrix}\text{matrix converting our}\\\text{representation to theirs} \\ \end{bmatrix}
    A
    \begin{bmatrix}\text{matrix converting their}\\\text{representation to ours} \\ \end{bmatrix}
\Bigg)^n
\begin{bmatrix}\text{matrix converting our}\\\text{representation to theirs} \\ \end{bmatrix}
$$




Recall that above we observed that the $\nth$ power of $A$ is a matrix with the
nth Fibonacci number in its bottom left and top right entries. So the following
tasks remain:

1. Find the eigenvectors and put them in a matrix $V$.
2. Find the inverse of $V$.
3. Compute the matrix product $V^{-1}AV$.
4. Compute the result of raising that to the $\nth$ power.
5. Plug the result of that into the overall expression.
6. Take the entry in the bottom left or top right (they should be the same!).

The result should be an expression giving the $\nth$ Fibonacci number as a
function of $n$. It should be possible to give as input to that function the
number one million, and have it output the one millionth Fibonacci number
directly, without it having to go through the preceding 999,999 Fibonacci
numbers.


\subsection*{The answer without showing the calculations}

\begin{align*}
&\text{The eigenvectors are}
\\\\
&V &= \mat{2          }{2          }
        {1 + \sqrt 5}{1 - \sqrt 5}
\\\\
&\text{which has inverse}
\\\\
&V^{-1} &= \frac{-1}{4\sqrt 5} \mat{1 - \sqrt 5 }{-2}
                                 {-1 - \sqrt 5}{2}
\\\\
&\text{Therefore}
\\\\
&V^{-1}AV &= \frac{1}{2} \mat{1 + \sqrt 5}{0          }
                            {0          }{1 - \sqrt 5}
\\\\
&\text{and}
\\\\
&(V^{-1}AV)^n &= \frac{1}{2^n} \mat{(1 + \sqrt 5)^n}{0          }
                                   {0                }{(1 - \sqrt 5)^n}
\\\\
&\text{and}
\\\\
&V \Big(V^{-1}AV\Big)^n V^{-1} &=
\mat{\frac{\big((1 + \sqrt 5)^{n-1} - (1 - \sqrt 5)^{n-1}\big)}{2^{n-1}\sqrt 5}}{\frac{\big((1 + \sqrt 5)^n     - (1 - \sqrt 5)^n    \big)}{2^n    \sqrt 5}}
    {\frac{\big((1 + \sqrt 5)^n     - (1 - \sqrt 5)^n    \big)}{2^n    \sqrt 5}}{\frac{\big((1 + \sqrt 5)^{n+1} - (1 - \sqrt 5)^{n+1}\big)}{2^{n+1}\sqrt 5}}
\\\\
&\text{Therefore the nth Fibonacci number is}
\\\\
&F_n &= \frac{(1 + \sqrt 5)^n     - (1 - \sqrt 5)^n}
             {2^n    \sqrt 5}
\end{align*}

\subsection*{Does this actually work?}

Yes.

    :::python
    from math import sqrt

    def fib(n):
        return (
            ( (1 + sqrt(5))**n - (1 - sqrt(5))**n )
            /
            float(2**n * sqrt(5)))

    for i in range(10):
        print i, fib(i)

    0 0.0
    1 1.0
    2 1.0
    3 2.0
    4 3.0
    5 5.0
    6 8.0
    7 13.0
    8 21.0
    9 34.0

\subsection*{History}

The formula is known as
Binet's formula (\url{https://en.wikipedia.org/wiki/Fibonacci_number#Closed-form_expression})
(1843) but was apparently known to Euler, Daniel Bernoulli and de Moivre more
than a century earlier. It can be derived without using linear algebra
techniques; I don't know when the style of proof attempted here would first
have been done. The result can be written as

$$
F_n = \frac{\phi^n - (1-\phi)^n}{\sqrt{5}}
$$

where $\phi = \frac{1+\sqrt{5}}{2}$ is the
golden ratio (\url{https://en.wikipedia.org/wiki/Golden_ratio}).


\subsection*{Calculations}

\subsubsection{1. Find the eigenvectors}
We follow the textbook approach: We have
$$
A = \mat{0}{1}
        {1}{1}
$$

An eigenvector $v$ satisfies $Av = \lambda v$ for some scalar $\lambda$. That
equation can be rearranged as follows

\begin{align*}
A\vec v &= \lambda I\vec v
\\
A\vec v - \lambda I\vec v &= \vec 0
\\
(A - \lambda I)\vec v &= \vec 0
\end{align*}

which means that the matrix $A - \lambda I$ is a transformation that takes some
non-zero vector $\vec v$ to the zero vector (i.e. it has a non-empty "null
space"). This means that the transformation cannot be reversed, i.e. the matrix
has no inverse, i.e. its determinant is zero. So, use that last fact to find
the eigenvectors $\lambda$:

\begin{align*}
\det (A - \lambda I) &= 0
\\
\\
\det \mat{-\lambda}{1}
         {1          }{1 - \lambda} &= 0
% \\
% \\
% (-\lambda)(1 - \lambda) - 1 &= 0
\\
\\
\lambda^2 - \lambda - 1 = 0
\end{align*}

Using the quadratic formula we have $a=1, b=-1, c=-1$ and

\begin{align*}
\lambda
= \frac{-b ± \sqrt{b^2 - 4ac}}{2a}
= \frac{1 ± \sqrt{5}}{2}
\end{align*}

which are the two eigenvalues.

To find eigenvectors associated with the eigenvalues, go back to the equations

\begin{align*}
(A - \lambda I)\vec v &= \vec 0
\\
\\
\mat{-\lambda}{1}
    {1       }{1 - \lambda} \vec v &= \vec 0
\end{align*}

Let an eigenvector $v$ be $\scvec{v_1}{v_2}$. The matrix equation corresponds
to this system of equations:

$$
\begin{cases}
-\lambda v_1 + v_2               &= 0\\
v_1          + (1 - \lambda) v_2 &= 0
\end{cases}
$$

From the first equation we have $v_2 = \lambda v_1$. There are infinitely many
eigenvectors (a line of them) associated with any given eigenvalue, so we can
pick an arbitrary value for $v_1$. If we choose $v_1=2$ then we have
eigenvectors $\scvec{2}{1+\sqrt 5}$ and $\scvec{2}{1-\sqrt 5}$. The matrix
containing the eigenvectors is

$$
V = \mat{2          }{2          }
        {1 + \sqrt 5}{1 - \sqrt 5}
$$


\subsubsection{2. Find inverse of $V$}

The inverse of a 2x2 matrix is given by

$$
\mat{a}{c}
    {b}{d} ^ {-1}
=
\frac{1}{\text{det}} \mat{d}{-c}
                         {-b}{a}
$$

where $\text{det} = ad - cb$. Therefore

\begin{align*}
V^{-1}
&= \frac{1}{2(1 - \sqrt 5) - 2(1 + \sqrt 5)} \mat{1 - \sqrt 5 }{-2}
                                                 {-(1 + \sqrt 5)}{2}
\\\\
&= \frac{-1}{4\sqrt 5} \mat{1 - \sqrt 5 }{-2}
                           {-(1 + \sqrt 5)}{2}
\end{align*}


\subsubsection{3. Find the matrix product $V^{-1}AV$}

Before we get lost in the calculation, let's remember what this is. It's a
matrix that does the $A$ transformation, but \textit{in the coordinate system defined
by $A$'s eigenvectors}. So, the resulting matrix \textit{must} do nothing other than
stretch space in the direction of one or both basis vectors in that coordinate
system. That's because (1) we represent a transformation with a matrix saying
where each of the basis vectors are taken to, (2) the definition of an
eigenvector of a transformation is that it is a vector which is simply
stretched by the transformation with no change in direction, therefore (3) if
the eigenvectors are the basis vectors, then the matrix representing the
transformation must just stretch space in the two directions. A matrix which
stretches space in the direction of the basis vectors looks like
$\smat{a}{0}{0}{b}$, i.e. it is diagonal. Therefore, $V^{-1}AV$ \textit{must} be
diagonal.

\begin{align*}
V^{-1}AV &=
\frac{-1}{4\sqrt 5}
\mat{1 - \sqrt 5 }{-2}
    {-(1 + \sqrt 5)}{2}
\mat{0}{1}
    {1}{1}
\mat{2          }{2          }
    {1 + \sqrt 5}{1 - \sqrt 5}
\\\\
&=
\frac{-1}{4\sqrt 5}
\mat{1 - \sqrt 5 }{-2}
    {-(1 + \sqrt 5)}{2}
\mat{1 + \sqrt 5}{1 - \sqrt 5}
    {3 + \sqrt 5}{3 - \sqrt 5}
\\\\
&=
\frac{-1}{4\sqrt 5}
\mat{-4 - 2(3 + \sqrt 5)             }{6 - 2\sqrt 5 - 2(3 - \sqrt 5)}
    {-(6 + 2\sqrt 5) + 2(3 + \sqrt 5)}{4 + 2(3 - \sqrt 5)}
\\\\
&=
\frac{-1}{2\sqrt 5}
\mat{-2 - 3 - \sqrt 5}{3 - \sqrt 5 - 3 + \sqrt 5}
    {-3 - \sqrt 5 + 3 + \sqrt 5}{2 + 3 - \sqrt 5}
\\\\
&=
\frac{-1}{2\sqrt 5}
\mat{-5 - \sqrt 5}{0          }
    {0           }{5 - \sqrt 5}
\\\\
&=
\frac{1}{2}
\mat{1 + \sqrt 5}{0          }
    {0          }{1 - \sqrt 5}
\end{align*}

\subsubsection{4. Compute $(V^{-1}AV)^n$}

The matrix is diagonal so this is straightforward. Note that this is the whole
point of converting to the eigenbasis: the exponentiation at this step just
involves the usual operations of raising scalar numbers to a power; no need to
multiply matrices together. A computer will be able to compute the $\nth$ power
of a diagonal matrix much faster than that of a non-diagonal matrix.

$$
(V^{-1}AV)^n = \frac{1}{2^n} \mat{(1 + \sqrt 5)^n}{0          }
                                 {0              }{(1 - \sqrt 5)^n}
$$

\subsubsection{5. Plug the $\nth$ power into the overall expression}

\begin{align*}
V \Big(V^{-1}AV\Big)^n V^{-1}
&=
\frac{-1}{4\sqrt 5}
\frac{1}{2^n}
\mat{2          }{2          }
    {1 + \sqrt 5}{1 - \sqrt 5}
\mat{(1 + \sqrt 5)^n}{0              }
    {0              }{(1 - \sqrt 5)^n}
\mat{1 - \sqrt 5 }{-2}
    {-(1 + \sqrt 5)}{2}
\\\\
&=
\frac{-1}{4\sqrt 5}
\frac{1}{2^n}
\mat{2          }{2          }
    {1 + \sqrt 5}{1 - \sqrt 5}
\mat{(1 - \sqrt 5)(1 + \sqrt 5)^n}{-2(1 + \sqrt 5)^n}
    {-(1 + \sqrt 5)(1 - \sqrt 5)^n}{2(1 - \sqrt 5)^n}
\\\\
&=
\frac{-1}{4\sqrt 5}
\frac{1}{2^n}
\mat{2(-4)\big((1 + \sqrt 5)^{n-1} - (1 - \sqrt 5)^{n-1}\big)}{-4\big((1 + \sqrt 5)^n     - (1 - \sqrt 5)^n    \big)}
    {   -4\big((1 + \sqrt 5)^n     - (1 - \sqrt 5)^n    \big)}{-2\big((1 + \sqrt 5)^{n+1} - (1 - \sqrt 5)^{n+1}\big)}
\\\\
&=
\frac{1}{4\sqrt 5}
\mat{4\frac{\big((1 + \sqrt 5)^{n-1} - (1 - \sqrt 5)^{n-1}\big)}{2^{n-1}}}{4\frac{\big((1 + \sqrt 5)^n     - (1 - \sqrt 5)^n    \big)}{2^n    }}
    {4\frac{\big((1 + \sqrt 5)^n     - (1 - \sqrt 5)^n    \big)}{2^n    }}{ \frac{\big((1 + \sqrt 5)^{n+1} - (1 - \sqrt 5)^{n+1}\big)}{2^{n-1}}}
\\\\
&=
\mat{\frac{\big((1 + \sqrt 5)^{n-1} - (1 - \sqrt 5)^{n-1}\big)}{2^{n-1}\sqrt 5}}{\frac{\big((1 + \sqrt 5)^n     - (1 - \sqrt 5)^n    \big)}{2^n    \sqrt 5}}
    {\frac{\big((1 + \sqrt 5)^n     - (1 - \sqrt 5)^n    \big)}{2^n    \sqrt 5}}{\frac{\big((1 + \sqrt 5)^{n+1} - (1 - \sqrt 5)^{n+1}\big)}{2^{n+1}\sqrt 5}}
\end{align*}
