\footnotetext{Notes from Classical Mechanics by John R. Taylor, ch. 6}

\section{Two example problems}
The calculus of variations can be used to find a function $y(x)$ that minimizes a scalar quantity
that is expressed as an integral $\int_{x_0}^{x_1} f[y(x), y'(x), x] \dx$. Here are two such
problems:

\begin{question*}
  What is the shortest path between two points in a plane?
\end{question*}

\begin{proof}
  Let the points be $(x_0, y_0)$ and $(x_1, y_1)$ and let them be joined by some path $y(x)$ of
  length $S$. Consider a short section of the path of length $\Delta s$ above a section of the
  $x$-axis of length $\Delta x$, and make a linear approximation to the path in this region. The
  length of the hypotenuse is
  \begin{align*}
    \Delta s = \sqrt{(\Delta x)^2 + (y'(x)\Delta x)^2} = \sqrt{1 + y'(x)^2} \Delta x.
  \end{align*}
  Therefore a shortest path is a function $y(x)$ that minimizes
  \begin{align*}
    S = \int_{x_0}^{x_1} \d s = \int_{x_0}^{x_1} \sqrt{1 + y'(x)^2} \dx,
  \end{align*}
  with the constraint that the endpoints are fixed at $y(x_0) = y_0$ and $y(x_1) = y_1$.

  \todo{Find the function $y$ that minimizes this integral.}
\end{proof}


\begin{question*}
  In 1662 Fermat proposed that light, when passing from one point to another through a material with
  varying refractive index, takes the path which takes least time\footnote{\todo{In fact, the path
      taken is a stationary point with respect to the action? time? ...not necessarily least}}. What
  is this path?
\end{question*}

\begin{proof}
  Again consider a short section of the path of length $\Delta s$ above a section of the $x$-axis of
  length $\Delta x$. Let $c$ be the speed of light and $n$ be the refractive index in this
  region. This means that the light travels at speed $c/n$, and therefore takes time $(n/c)\Delta s$
  to pass along the hypotenuse. The refractive index $n$ can vary with both $x$ and $y$, therefore a
  least-time path is a function $y(x)$ that minimizes
  \begin{align*}
    T = \int_{x_0}^{x_1} n(x, y(x)) \d s = \int_{x_0}^{x_1} n(x, y(x)) \sqrt{1 + y'(x)^2} \dx,
  \end{align*}
  with the constraint that the endpoints are fixed at $y(x_0) = y_0$ and $y(x_1) = y_1$.

  \todo{Find the function $y$ that minimizes this integral.}
\end{proof}

A naive thought would be to somehow treat $y$ similarly to how a variable is treated when minimizing
a function in in basic calculus, i.e. differentiate the expression with respect to $y$. Recall that
the definition of derivative is
\begin{align*}
  f'(y_0) = \lim_{y_1 \to y_0}\frac{f(y_1) - f(y_0)}{||y_1 - y_0||}.
\end{align*}

\todo{I think this is nonsense and the reason is that multiplication (and therefore division) of
  functions is not defined (they can be treated as vectors, so can be added and scaled, but do not
  have an obviously appropriate multiplication operation). I don't think choosing a norm would
  necessarily be problematic.}

\section{The Euler-Lagrange equations}

Note that in both example problems, the integral to be minimized can be viewed as a scalar-valued
\emph{functional} $f$ that depends on the \emph{functions} $y(x)$ and $y'(x)$\footnote{It seems to
  me that this could also be written as $S[y, y'] = \int _{x_0}^{x_1} f[y(x), y'(x)](x) \dx$}:
\begin{align*}
  S(y, y') = \int_{x_0}^{x_1} f[y(x), y'(x), x] \dx.
\end{align*}

We'll refer to $S$ as giving the \emph{cost} of traveling along the path $y$, from $(x_0, y_0)$ to
$(x_1, y_1)$. Of course, $y$ and $y'$ are not independent variables, so we could just write $S(y)$.
However, we won't: we'll write $S(y, y')$.

Recall that we seek a least-cost path $y(x)$, subject to the requirement that the endpoints are
$y(x_0) = y_0$ and $y(x_1) = y_1$. Let $y(x)$ be the least-cost path, and consider an alternative
path $Y(x)$ whose cost is greater than that of $y(x)$. We can write $Y(x)$ as
\begin{align*}
  Y(x) = y(x) + \eta(x),
\end{align*}
where the difference function $\eta$ must satisfy $\eta(x_0) = \eta(x_1) = 0$.  Now introduce a
parameter $\alpha$ and redefine $Y(x)$ as
\begin{align*}
  Y(x) = y(x) + \alpha\eta(x).
\end{align*}
So now we have a family of paths, parameterized by $\alpha$, all satisfying the endpoint requirement,
and with the least-cost path corresponding to $\alpha=0$. We can consider the cost of these paths to
be a function $S(\alpha)$:
\begin{align*}
  S(\alpha) = \int_{x_0}^{x_1} f[y(x) + \alpha\eta(x), y'(x) + \alpha\eta'(x), x] \dx.
\end{align*}
We are trying to find a $y(x)$ that is a minimum in the cost surface over the function space (or a
maximum, or saddle point). For such a $y(x)$ it must be the case that
\begin{align*}
  \pdv{S}{\alpha}\Big|_{\alpha=0} = 0.
\end{align*}
So, let's compute $\pdv*{S}{\alpha}$ and use the fact that it must evaluate to zero at $\alpha=0$ to
obtain an equation that $y$ must obey. We'll assume that $f$ satisfies the (mild) conditions
necessary to differentiate under the integral sign, i.e. that
\begin{align*}
  \pdv{S}{\alpha} = \int_{x_0}^{x_1} \pdv{f[y(x) + \alpha\eta(x), y'(x) + \alpha\eta'(x), x]}{\alpha} \dx.
\end{align*}
So we have
\begin{align*}
  \pdv{f(y + \alpha\eta, y' + \alpha\eta', x)}{\alpha} =
\end{align*}
