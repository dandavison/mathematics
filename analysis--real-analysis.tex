\section{Definitions}

\subsection{Limit point}
Let $E \subset \R$. A point $p \in \R$ is a limit point of $E$ iff for all $\delta > 0$ there
exists $x \in E$ such that $0 < |x - p| < \delta$.

Intuition: a deleted ball, of arbitrarily small radius, can be placed over $p$ and will capture a
point of $E$.

\subsection{Limit, Convergence}
A sequence $(x_n)$ has limit $L$ iff for all $\epsilon > 0$ there exists an $m \in \N$ such that
$n > m \implies |x_n - L| < \epsilon$. When this is true the sequence is said to \textit{converge}
to $L$.

A function $f$ has limit $L$ as $x$ approaches $a$ iff for all $\epsilon > 0$ there exists
$\delta > 0$ such that $0 < |x - a| < \delta \implies |f(x) - L| < \epsilon$.

\subsubsection{Uniform convergence}
A sequence of functions $\{f_n\}_{n\geq 0}$ has a limit $f$ iff for every point
$x$ in the input set the sequence $\{f_n(x)\}_{n\geq 0}$ has limit $f(x)$.

They \textit{converge uniformly} to $f$ iff the same $m$ works for all input
values.

\subsection{Continuity}
$f:\R\to\R$ is continuous at $x_0$ iff $\lim_{x\to x_0} f(x) = f(x_0)$.

Therefore, using the definition of limit, $f$ is continuous at $x_0$ iff for all $\epsilon > 0$
there exists $\delta > 0$ such that $|x - x_0| < \delta \implies |f(x) - f(x_0)| < \epsilon$.

\subsubsection{Uniform continuity}
A function $f$ is uniformly continuous iff the same $\delta$ works for all $x_0$.

A function $f$ is uniformly continuous iff for all $\epsilon$, no matter how
small, a $\delta$ exists such that for all $x_0 \in U$, if $x$ is within
$\delta$ of $x_0$ then $f(x)$ is within $\epsilon$ of $f(x_0)$.

\section{Theorems}
\begin{theorem}[Limit of product is product of limits]\label{limit-of-product}~\\
  Let $\limxa f(x) = L_f$ and $\limxa g(x) = L_g$. Then
  $\limxa f(x)g(x) = L_fL_g$.
\end{theorem}

\begin{proof}
  Note that
  \begin{align*}
    \limxa f(x)g(x) &= \limxa \Big((f(x) - L_f)(g(x) - L_g) + L_fg(x) + L_gf(x) - L_fL_g\Big)\\
                          &= L_fL_g + \limxa (f(x) - L_f)(g(x) - L_g),
  \end{align*}
  so we need to show that $\limxa (f(x) - L_f)(g(x) - L_g) = 0$. Fix $\epsilon > 0$. Since
  $\limxa (f(x) - L_f) = \limxa (g(x) - L_g) = 0$, there exists $\delta$ such that whenever
  $|x - a| < \delta$
  \begin{align*}
    |(f(x) - L_f)| < \sqrt \epsilon ~~~\text{and}~~~|(g(x) - L_g)| < \sqrt \epsilon,
  \end{align*}
  therefore $|(f(x) - L_f)(g(x) - L_g) - 0| < \epsilon$ as required.
\end{proof}

\begin{theorem*}[Limit of quotient is quotient of limits]~\\
  Let $\limxa f(x) = L_f$ and $\limxa g(x) = L_g \neq 0$. Then
  \begin{align*}
    \limxa \frac{f(x)}{g(x)} = \frac{L_f}{L_g}.
  \end{align*}
\end{theorem*}

\begin{proof}
  \red{TODO}
  \begin{align*}
    \limxa \frac{f(x)}{g(x)} - \frac{L}{M}
    = \limxa \frac{f(x)}{g(x)} - \frac{1}{g(x)} + \frac{1}{g(x)} - \frac{L}{M}
  \end{align*}

  Let $L_f = \limxa f(x)$ and $L_g = \limxa g(x) \neq 0$.

  Fix $\epsilon > 0$ and let $\delta_f$ and $\delta_g$ be such that
  \begin{align*}
    |x - a| < \delta_f \implies |f(x) - L_f| < \epsilon\\
    |x - a| < \delta_g \implies |g(x) - L_g| < \epsilon.
  \end{align*}
  Let $\delta = \min(\delta_f, \delta_g)$. Then
  \begin{align*}
    \frac{|f(x) - L_f|}{|g(x) - L_f|}
  \end{align*}
\end{proof}


\begin{theorem*}[Intermediate value theorem]
  Let $a, b \in \R$ with $b > a$, and $f:[a,b] \to \R$ be continuous. Let $u$ lie strictly between
  $f(a)$ and $f(b)$. Then there exists $c \in (a, b)$ such that $f(c) = u$.
\end{theorem*}

\begin{proof}
  Define $S := \{x \in [a, b] ~|~ f(x) < u\}$. Since $a \in S$, $S$ is non-empty. By completeness
  of reals $c := \sup S$ exists. The theorem now follows from continuity of $f$ at $c$. (Fix
  $\epsilon > 0$ and consider points $a^* \in (c - \delta, c)$ and $a^{**} \in (c, c + \delta)$,
  noting whether they are in $S$ and the $\epsilon-\delta$ continuity criterion.)
\end{proof}

\begin{theorem*}[Mean-value theorem]
  Let $a, b \in \R$ with $b > a$, and $f:[a,b] \to \R$ be continuous on $[a, b]$ and differentiable
  on $(a, b)$. Then there exists $x \in (a, b)$ such that $f'(x) = \frac{f(b) - f(a)}{(b - a)}$.
\end{theorem*}


\begin{theorem*}[Differentiability implies continuity]
  Let $f:\R\to\R$ be differentiable. Then $f$ is continuous.
\end{theorem*}

\begin{proof}~\\
  Let $a \in \R$. The claim is that $\limxa f(x) - f(a) = 0$. Since $f$ is differentiable,
  \begin{align*}
    f'(a) &= \lim_{x \to a} \frac{f(x) - f(a)}{x - a}
  \end{align*}
  exists. Therefore by \eqref{limit-of-product}
  \begin{align*}
    \lim_{x \to a} f(x) - f(a) = \lim_{x \to a} (x - a)\frac{f(x) - f(a)}{x - a} = 0\cdot f'(a) = 0.
  \end{align*}
\end{proof}

\begin{remark*}
  Intuitively it seems that differentiability implies continuity because, for the derivative to
  exist, the numerator $f(x) - f(a)$ must get small as $x\to a$, as the denominator $x - a$ does.
\end{remark*}




\section{Sequences and Series}
\footnotetext{Stewart Calculus ch. 11}

\subsection{Definition of sequences and series}

A \textbf{sequence} is an infinite sequence of numbers $a_1, a_2, a_3, ...$.

A \textbf{series} is the sum of a sequence: $s = a_1 + a_2 + a_3 + ... = \sum_{n=1}^\infty a_n$.

A sequence is often defined by giving the closed-form expression for the $\nth$ item, e.g. $a_n = \frac{1}{n}$. This refers to the sequence

$$\frac{1}{1}, \frac{1}{2}, \frac{1}{3}, ...$$

The corresponding series is

$$
\sumn \frac{1}{n} = \frac{1}{1} + \frac{1}{2} + \frac{1}{3} + ...
$$

A sequence may have a limit: $\lim_{n \rightarrow \infty} a_n$. If it does then the sequence \textbf{converges}. If not, it \textbf{diverges}.

\subsubsection{Typical questions about sequences (11.1)}


\textbf{Determine whether the sequence converges or diverges. If it converges, find the limit.}

- \textbf{Easier} (11.1.24)
  $$
  a_n = \frac{n^3}{n^3 + 1}
  $$
  It's asking whether the sequence converges, so we want to try to compute the limit of the sequence as $n \rightarrow \infty$. If the limit exists then it converges. Rearrange so that all $n$s are on the bottom of fractions:
  $$
  a_n = \frac{1}{1 + \frac{1}{n^3}}
  $$
  So now
  $$
  \limn a_n = \frac{1}{1 + \limn\frac{1}{n^3}} = \frac{1}{1 + 0} = 1.
  $$

- \textbf{Easier} (11.1.26)
  $$
  a_n = \frac{n^3}{n + 1}
  $$
  With this one, we can't get the $n$s to appear only on the bottom of fractions:
  $$
  a_n = \frac{n^2}{1 + \frac{1}{n}}
  $$
  So
  $$
  \limn a_n = \frac{\limn n^2}{1 + \limn\frac{1}{n}} = \frac{\limn n^2}{1 + 0} = \limn n^2.
  $$
  But $n^2$ increases without bound as $\ninfty$. People write that as $\limn n^2 = \infty$. In any case, the limit does not exist and this sequence diverges.

- \textbf{Harder} (11.1.56)
  $$
  a_n = \frac{(-3)^n}{n!}
  $$
  The negative factor means that the terms of the sequence alternate between being negative and positive. Rewrite it as
  $$
  a_n = (-1)^n\frac{3}{1}\frac{3}{2}\frac{3}{3}\frac{3}{4}...\frac{3}{n-1}\frac{3}{n}
  $$
  Now we use two theorems in the book. The first ("Theorem 6") says that if $\limn |a_n|$ is zero, then so is $\limn a_n$. So that means we can see what happens while ignoring the -1 term. The second theorem is the "Squeeze Theorem": if we can show that the quantity we're studying lies between 0 and something else whose limit is zero, then we've proved that the limit of our quantity is 0. To do that we notice that all the terms from $\frac{3}{4}$ to $\frac{3}{n-1}$ are less than 1. Therefore
  $$
  0 < |a_n| < \frac{9}{2}\frac{3}{n} = \frac{27}{2n}
  $$
  But $\limn \frac{27}{2n} = 0$ so this (together with the Theorem 6 argument about the $(-1)^n$) proves that the sequence converges and that its limiting value is 0.

\subsubsection{Series}

If the series sums to a finite value then it is said to converge; if not, it diverges.

A series has an associated \textbf{sequence} of \textbf{partial sums}: $a_1, a_1 + a_2, a_1 + a_2 + a_3, ...$. I.e. the sequence $a_n = \sum_{i=1}^n a_i$. If the series sums to $s$ (converges) then $s$ is the limit of sequence of partial sums.

Say you only sum the first $n$ values of a series. The \textbf{remainder} $R_n$ is the difference between the true sum (of infinitely many values) and what you got from the first $n$ (the $\nth$ partial sum): $R_n = s - \sum_{i=1}^n$.



In general it is not easy to find the sum of a series, and so a lot of the material in the book focuses on methods for determining whether the series converges or not. However, it is easy to find the sum if the series is a \textbf{geometric series}:

\textbf{Geometric series}
Several questions require recognizing a geometric series. A geometric series is a series where each term differs by a constant multiplier:

$$
s = a + ar^1 + ar^2 + ar^3 + ... = \sumn ar^{n-1}
$$

Theorem: if $|r| < 1$ then the geometric series converges. Its value is $\frac{a}{1-r}$. For $|r| \geq 1$ it diverges.

\subsubsection{Typical questions about series}

\textbf{Find the values of x for which the series converges. Find the sum of the series for those values of x.}

- 11.2.59
  $$
  \sum_{n=0}^{\infty} \frac{(x-2)^n}{3^n}
  $$
  We have to recognize that this is a geometric series. Rewrite it as
  $$
  \sum_{n=0}^{\infty} \Big(\frac{x-2}{3}\Big)^n
  $$
  and it's clear that it is a GS with $a=1$ and $r=\frac{x-2}{3}$. Therefore it converges if
  $$
  -1 < \frac{x-2}{3} < 1 \Leftrightarrow -1 < x < 5
  $$
  and when x is in that range, its sum is
  $$
  \frac{a}{1-r} = \frac{1}{1 - \frac{x-2}{3}} = \frac{3}{5-x}.
  $$

\subsubsection{Test for divergence of series}
If the \textbf{sequence} does not converge to zero then the \textbf{series} (the sum of the infinite sequence) is divergent. That's fairly intuitive: if the sequence doesn't converge to zero then the series is going to be summing infinitely many non-zero terms and so will not converge.

So that means that most of the questions about series involve decreasing sequences that converge to zero, for example $a_n = \frac{1}{n}$, or $a_n = \frac{2}{(n-3)^2}$.

What's less intuitive is that even if the sequence \textbf{does} converge to zero, the series still might not converge. The famous example is $\sum_{n=1}^\infty \frac{1}{n} = \frac{1}{1} + \frac{1}{2} + \frac{1}{3} + ...$. Although the sequence converges to zero, its sum (the series) does not converge to a finite value. On the other hand, for $\sum_{n=1}^\infty \frac{1}{n^2} = \frac{1}{1} + \frac{1}{4} + \frac{1}{9} + ...$ the sequence converges to zero and the series does have a finite value. Those facts are not obvious, they require proof: both are infinite sequences of numbers that approach zero, it's just that one aproaches zero more rapidly and has a finite sum whereas the other approaches more slowly and fails to have a finite sum. We can prove it using the \textbf{integral test}.

\subsubsection{Integral test for convergence}
Consider a series like $\sumn \frac{1}{n}$. The continuous function $f(x) = \frac{1}{x}$ passes through the discrete points of the sequence. The integral test says that we can use the area under the graph of the continuous function to assess whether the series converges. That's because we can visualize the series as summing together infinitely many rectangles of width 1, so the sum of the areas of the rectangle is related to the area under the graph.

\textbf{Integral test}: If $\int_1^\infty f(x) dx$ is convergent (has a finite value) then the series is convergent. And if the integral is divergent then the series is divergent.

For example
$$
\int_1^\infty \frac{1}{x^2} dx = \frac{-1}{x} \Big|_1^\infty = 1.
$$

so that shows that $\sumn \frac{1}{n^2}$ converges (to something, we don't know what). On the other hand

$$
\int_1^\infty \frac{1}{x} dx = \ln x \Big|_1^\infty = \lim_{x \to \infty} \ln x - 0
$$

which is divergent (not a finite value). So that shows that $\sumn \frac{1}{n}$ does not converge.

\textbf{Remainder theorem for integral test}
Because of the details of the location of the rectangle lines in the approximation, the size of the remainder lies between these two integrals:

$$
\int_{x=n+1}^\infty f(x) dx \leq R_n \leq \int_{x=n}^\infty f(x) dx
$$


\subsubsection{p-series}

A p-series is a series like $\sumn \frac{1}{n^p}$.

Theorem: a p-series is convergent if $p>1$ and divergent if $p \leq 1$.

\subsection{Comparison tests}

\subsubsection{Comparison to known-convergent series}
Consider a series for which all terms are \textbf{positive}. If we can show that the terms of our series are always less than the corresponding terms of a known-convergent series, then our series is convergent. SImilarly if our terms are larger than those of a known-divergent series, then our series is divergent.

For example, we know that $\sumn\frac{1}{2^n}$ converges to zero (p-series with $p > 1$) but what about $\sumn\frac{1}{2^n + 1}$? It seems obvious that it converges to zero, but how do you prove it? The answer is that its terms are always smaller than the terms of $\sumn\frac{1}{2^n}$, therefore it also converges to zero.


\subsubsection{Limit comparison test}
This involves looking at the limit of the \textbf{ratio} of the terms from two different series. Suppose you have a series $\sumn b_n$ which you know either converges or diverges, and you have a series $\sumn a_n$ which you don't know the behavior of. If you can show that their ratio has a finite limit:

$$
\limn \frac{a_n}{b_n} = c > 0
$$
then either \textbf{both converge} or \textbf{both diverge}. Seeing as you know how $b$ behaves, that tells you whether $a$ converges or not.


\subsection{Alternating series}
The two theorems about alternating series are fairly obvious if we draw a picture of an alternating series.

\subsection{Alternating series test}
The previous tests are all for series with positive terms only. Alternating series are often the result of a $(-1)^n$ factor, for example $\sumn (-1)^n \frac{1}{n}$. The \textbf{alternating series test} says: look at the \textbf{absolute values}; if they are decreasing (i.e. $|a_{n+1}| < |a_n|$), and if their limit is zero, then the series converges.


\subsubsection{Alternating series remainder theorem}
The absolute size of the error is less than or equal to the size of the first neglected term. (Only for alternating series!)


\subsection{Absolute and conditional convergence}

Sometimes a series with positive and negative terms might converge, but the absolute value version of the same series does \textbf{not} converge. This situation is called \textbf{conditional convergence}. An example is $\sumn (-1)^n \frac{1}{n}$. This converges (by alternating series test: the successive terms get smaller) but the absolute version doesn't converge (as discussed above).

If the absolute value version of an alternating series \textbf{does} converge, then this is called \textbf{absolute convergence}.

Theorem: if an alternating series is absolutely convergent, then the alternating version is convergent also.

In other words, there are two different categories of convergent alternating series: conditional (only the alternating version converges), and absolute (both versions converge).

\subsection{Ratio and root tests}

These can be used for alternating and non-alternating series.

\subsubsection{Ratio test}
Write down the ratio of successive terms in the series $\frac{a_{n+1}}{a_n}$, take the absolute value and take the limit of that. So $\limn \Big|\frac{a_{n+1}}{a_n}\Big|$. There are 3 possible outcomes:

$$
\begin{cases}
0 < \text{limit} < 1&Convergent\\
\text{limit} = 1&Inconclusive\\
\text{limit} > 1 ~\text{or}~ \text{limit} = \infty&Divergent\\
\end{cases}
$$

If the original series was alternating, then these conclusions of "convergent" and "divergent" apply to the alternating series too, as well as the series of absolute values.


\subsubsection{Root test}
This is like the ratio test, but you use it if there are powers of $n$.

Evaluate the limit of the absolute value of the $\nth$ root of the $\nth$: $|\sqrt[n]{a_n}|$. The cases are the same as for the ratio test:


$$
\begin{cases}
0 < \text{root} < 1&Convergent\\
\text{root} = 1&Inconclusive\\
\text{root} > 1 ~\text{or}~ \text{root} = \infty&Divergent\\
\end{cases}
$$


\subsection{Strategy}

See section 11.7 for overall strategy for testing series for convergence/divergence.


\subsection{Power series}

\subsubsection{Geometric series}

One class of tricks involves connecting the function to a geometric series. For example,

\subsubsection{Taylor and Maclaurin series}

Memorize the general formula for a Taylor series around a point $a$. (Maclaurin series is the same but with $a=0$).

A common question is: over what interval of $x$-values does the series converge? This can be answered using the Ratio Test for convergence.

\subsubsection{Maclaurin series derivation}

Suppose that any function of a real number $f(x)$ can be represented by a "power series" with certain coefficients $c_i$

$$
f(x) = c_0 + c_1x^1 + c_2x^2 + c_3x^3 + c_4x^4 + ...
$$

If that is so, there are two questions:

(1) what are the coefficients for any given function? This can be established
by differentiating and evaluating the result at $x=0$.

(2) Over what interval of $x$-values does the series converge? This can be
answered using the Ratio Test for convergence.

To answer (1), firstly, without differentiating at all, we see that $c_0 = f(0)$. Then differentiate once:

$$
f'(x) = c_1 + 2c_2x^1 + 3c_3x^2 + 4c_4x^3 + ...
$$

and we see that evaluating the first derivative at $x=0$ gives the coefficient
$c_1 = f'(0)$. Differentiating again yields $c_2$:

\begin{align*}
f''(x) &= (2\cdot 1)c_2 + (3\cdot 2)c_3x^1 + (4\cdot 3)c_4x^2 + ... \\
\frac{f''(0)}{2} &= c_2
\end{align*}

Continuing like that it becomes clear that each coefficient is equal to an $\nth$ derivative evaluated at zero, divided by a factorial term which results from the repeated differentiation:

\begin{align*}
f'''(x) &= (3\cdot2)c_3 + (4\cdot3\cdot2)c_4x^1 + ... \\
\frac{f'''(0)}{3!} &= c_2
\end{align*}

and in general $c_n = \frac{f^{(n)}(0)}{n!}$.

For example, take $f(x) = e^x$. The derivatives are all the same of course: $f^{(n)}(x) = e^x$. And $e^0 = 1$, so

$$
e^x = 1 + \frac{x}{1} + \frac{x^2}{2!} + \frac{x^3}{3!} + ... = \sum_{i=0}^\infty \frac{x^i}{i!}
$$

\newpage
\section{Oxford - M2 - Continuity and Differentiability}

\subsection{Limits of functions - Examples}

\begin{example}
  Let $E = \R\setminus \{0\}$ and define $f:E \to \R$ by $f(x) = L$. Then 0 is a limit point of
  $E$ and $f(x) \to L$ as $x \to 0$.
\end{example}

\begin{proof}
  Fix $\delta > 0$. Then $\exists x ~ 0 < |x - 0| < \delta$ is true since we can choose
  $x = \frac{\delta}{2}$. Therefore 0 is a limit point of $E$.

  Fix $\epsilon > 0$. Let $\delta = 1$. Then
  $0 < |x - 0| < \delta \implies |f(x) - L| = 0 < \epsilon$.
\end{proof}


\newpage
\section{Oxford - A2 - Metric Spaces}

\subsection{Open sets theorems}
\begin{enumerate}
\item An open ball is open
\end{enumerate}

\subsection{Closed sets theorems}
\begin{enumerate}
\item A closed ball is closed
\end{enumerate}


\subsection{Continuity theorems}
\begin{enumerate}
\item $f:X \to Y$ is continuous if for every open ball in $Y$ there is an open ball in $X$ that
  maps inside it.
\item $f:X \to Y$ is continuous if the preimage of $B(f(x), \epsilon)$ in $Y$ is a ball
  $B(x, \delta)$ in $X$.
\item $f:X \to Y$ is continuous if the preimage of the neighbourhood of $f(x)$ is a neighbourhood
  of $x$.
\item $f:X \to Y$ is continuous if the preimage of every open set in $Y$ is an open set in $X$.
\end{enumerate}



\begin{theorem}
  Let $f:V \to W$ be a linear map between normed vector spaces. Then $f$ is continuous if and only
  if $\{\norm{f(x)} : \norm{x} \leq 1\}$ is bounded.
\end{theorem}

\begin{proof}~\\
  Let $v \in V$.

  Note that $f(v) = f(v) - f(0)$ since $f$ is linear.

  Suppose $f$ is continuous. Then it is continuous at 0.

  Therefore for every $\epsilon > 0$ there exists $\delta > 0$ such that
  $\norm{v} < \delta \implies \norm{f(v)} < \epsilon.$

  $\vdots$

  For the converse, suppose that $\norm{v} \leq 1 \implies \norm{f(v)} < M$.

  Let $\epsilon > 0$ be given.

  Pick $\delta > 0$ such that $\delta M < \epsilon$.

  Now consider two points $u, v \in V$ where $\norm{u - v} < \delta$. We have
  \begin{align*}
    \norm{f(u) - f(v)} = \norm{f(u - v)} = \delta\norm{f\(\frac{u - v}{\delta}\)}.
  \end{align*}

  Note that $\norm{\frac{u - v}{\delta}} < 1$, therefore $\norm{f\(\frac{u - v}{\delta}\)} <
  M$. Therefore we have
  \begin{align*}
    \norm{f(u) - f(v)} < \delta M < \epsilon
  \end{align*}
  as required.
\end{proof}

\begin{theorem}
  $\{\norm{f(x)} : \norm{x} \leq 1\}$ is bounded for linear map $f$, under the Euclidean norm
  $\norm{}_2$.
\end{theorem}

\begin{proof}
  See Oxford A2 Sheet 1 exercises.
\end{proof}


\begin{definition}[Metric space]~\\
  Let $X$ be a set. Suppose $d:X \times X \to \R$ satisfies positivity, symmetry and the triangle
  equality. Then $d$ is a metric and $(X, d)$ is a metric space.
\end{definition}

\begin{definition}[Open ball]~\\
  Let $(X, d)$ be a metric space, $x \in X$ and $\delta > 0$. Then
  $B(x, \delta) := \{x \in X ~|~ d(x, x) < \delta\}$ iss an open ball of radius $\delta$ centred at
  $x$.
\end{definition}

\begin{remark*}
  Also closed ball, $\leq$. E.g. singleton set.
\end{remark*}

\begin{lemma}[Ball-based continuity criterion]~\\
  $f$ is continuous at $x$ if for all $\epsilon > 0$ there exists $\delta > 0$ such that
  $f\(B(x, \delta)\) \subseteq B(f(x), \epsilon))$.

  Equivalently, $B(x, \delta) \subseteq f^\1\(B(f(x), \epsilon)\)$.
\end{lemma}

\begin{definition}[Neighbourhood]~\\
  Let $(X, d)$ be a metric space. $N \subseteq X$ is a neighbourhood of $x \in X$ iff there exists
  $\delta > 0$ such that $B(x, \delta) \subseteq N$.
\end{definition}

\begin{remark*}
  $N$ is a neighbourhood of $x$ if a ball can be placed at $x$ without poking outside $N$.
\end{remark*}

\begin{definition}[Open and closed subsets of a metric space]~\\
  Let $(X, d)$ be a metric space. Then $U \subseteq X$ is open iff it is a neighbourhood of all of
  its elements.

  $V \seq X$ is closed iff its complement in $X$ is open.
\end{definition}

\begin{definition}[Topology on a metric space]~\\
  Let $(X, d)$ be a metric space. The collection $\mc T$ of all open sets in the metric space is
  called the topology of $X$.
\end{definition}

\begin{remark*}
  Note that the definitions so far have the following dependency:

  (open set) $\larrow$ (neighbourhood) $\larrow$ (ball) $\larrow$ (metric),

  so they apply to metric spaces only.
\end{remark*}

\begin{theorem}[Open set-based continuity criterion]~\\
  Let $X$ and $Y$ be metric spaces and let $f:X \to Y$. Then

  $f$ is continuous at $x$ iff for every neighbourhood $N \subseteq Y$ of $f(x)$, the preimage
  $f^\1(N)$ is a neighbourhood of $x \in X$.

  $f$ is continuous iff for every open set $U$ of $Y$, $f^\1(U)$ is an open set of $X$.
\end{theorem}

\begin{remark*}
  So we have defined continuity in terms of open sets (the topology). This means that the metric is
  only relevant insofar as it induces the topology; two metric spaces with the same topology have
  the same notion of continuity.
\end{remark*}

\begin{proof}~\\
  Let $f$ be continuous at $x \in X$, and let $N \seq Y$ be a neighbourhood of $f(x)$.

  Then by definition of neighbourhood there exists a ball at $f(x)$ that stays within $N$.

  By continuity of $f$ the preimage of that ball is a superset of a ball at $x$.

  So the preimage of the ball is a neighbourhood of $x$. Therefore the preimage of $N$ is also.

  Conversely, ... similar.

  Let $f$ be continuous on $X$. Now every open set $U$ of $Y$ contains a ball around some point $y$...
\end{proof}

\begin{definition}[Topology on a set, topological space]
  A topology on a set $X$ is a collection $\mc T$ of subsets of $X$, which are called the open
  sets. They must satisfy
  \begin{enumerate}
  \item closed under arbitrary unions. In particular, $\emptyset$ is an open set of $X$.
  \item closed under finite intersections. In particular, $X$ is an open set of $X$.
  \end{enumerate}
  A topological space is a pair $(X, \mc T)$.
\end{definition}

\begin{remark*}
  Criteria for closed sets follow by applying de Morgan's laws (closure under finite unions and
  arbitrary intersections).

  $f:X\to Y$ closed iff $f^\1(V)$ is closed for all closed sets $V \seq Y$.
\end{remark*}

\begin{definition}[Limit point]~\\
  Let $(X, d)$ be a metric space and $Z \seq X$ be any subset.

  $x \in X$ is a limit point of $Z$ if for all $\delta > 0$ the deleted open ball
  $B(x, \delta)\setminus\{x\}$ has non-empty intersection with $Z$.

  If $z$ is not a limit point of $Z$, then it is an isolated point.

  The set of limit points of $Z$ is denoted $Z'$, and it is clear that
  $Z_1 \seq Z_2 \implies Z_1' \seq Z_2'$.
\end{definition}

\begin{intuition*}[Limit point]~\\
  $x \notin Z$ is a limit point of $Z$ iff it ``touches'' $Z$.

  $z \in Z$ is a limit point of $Z$ if it ``lies in a contiguous region of $Z$''

  An isolated point of $Z$ is what it sounds like.
\end{intuition*}

\begin{example*}~\\
  Let $Z = (0, 1] \cup \{2\}$.

  Intuitively, 0 is a limit point of $Z$ because it ``touches'' $Z$.

  Formally, 0 is a limit point of $Z$ because for all $\delta > 0$ the deleted open ball
  $B(0, \delta)$ contains a point $z > 0 \in Z$.

  Intuitively, 2 is an isolated point.

  Formally, 2 is not a limit point because $\(B(2, 0.5)\setminus\{2\}\) \cap Z = \emptyset$. And
  yet $2 \in Z$, therefore 2 is an isolated point.
\end{example*}



\section{Exercises}

- \textbf{11.1: 3, 25, 37}
- \textbf{11.2: (14, 29, 42)}
    - One trick to find sums: use partial fractions and then write first few terms of series out and identify a telescoping sum (q. 14)

- \textbf{11.3: (7, 16, 25, 26) Determine whether convergent or divergent (Integral test)}
    - Use integral test (if $\int_1^\infty f(x) dx$ is finite then converges, otherwise diverges)
    - Although it seems less work to use limit comparison test for some

- \textbf{11.4: (18, 25, 26) Determine whether convergent or divergent}
    - Questions concern series with positive terms
    - Use Comparison Test or Limit Comparison Test
    - $\frac{1}{n}$ diverges. So if you can show that terms are larger than this, or that limit of ratio with this exists, then it must also be divergent.
    - $\frac{1}{n^2}$ converges. If you can show that terms are smaller than this, or that limit of ratio with this exists, then it converges.

- \textbf{11.5 (5, 8, 11) Determine whether convergent or divergent}
    - Questions typically concern alternating series
    - Alternating series test: show that successive values are decreasing \textbf{and} that limit of sequence is 0.
    - If it's not obviously decreasing, convert $a_n$ to continuous function $f(x)$ and show that derivative is negative or becomes negative for large values of $x$.

- \textbf{11.6 (5, 6, 29) Absolutely convergent | Conditionally convergent | Divergent}
    - Questions typically concern alternating series
    - Use Ratio Test on absolute values to check if convergent or divergent. If it is convergent on the Ratio Test then it is "absolutely convergent".
    - If Ratio Test is inconclusive (limit $= 1$) then the Alternating Series Test might show that it is Conditionally Convergent.

- \textbf{11.8 (10, 12) Find radius / interval of convergence}
    - Typically, use the ratio test (converges for values of $x$ that make $\limn |\frac{a_{n+1}}{a_n}|$ less than 1)
    - If they ask for interval, then you need to test the endpoints for convergence separately.

- \textbf{11.9 (3, 6) Find a power series representation for a given function, and radius/interval of convergence.}
    - Match up the function to the formula for the sum of a geometric series ($a/(1-r)$) and thus construct a geometric series that it's equal to.
    - Use ratio test to determine interval of convergence
    - A variant is: you have to differentiate the function before it looks like $a/(1-r)$. Then find the power series, and integrate the power series to get something equal to the original function.

- \textbf{11.10 (8, 9, 17) Find a Taylor / Maclaurin series for a given function, and radius/interval of convergence}
    - Memorize definition of general Taylor series (Maclaurin is $a=0$)
    - Compute derivatives and evaluate them at zero
    - Use those to write out first few terms of Taylor series
    - If there's a pattern, represent the sum using sigma notation
    - Use the ratio test to assess radius of convergence


\section{Exercises - Rudin - Principles of Mathematical Analysis}

~\\\hrule
\textbf{1.1 If $r$ is rational ($r \neq 0$) and $x$ is irrational, prove that $r + x$
and $rx$ are irrational.}

Let $r = \frac{i}{j}$, where $i, j \in \Z$.

Take $rx$ first. Suppose that $rx$ is rational, so $rx = \frac{i}{j}x =
\frac{k}{l}$ for some $k, l \in \Z$. This implies $x = \frac{jk}{il}$, which is
rational. This is a contradiction, since $x$ is irrational by
definition. Therefore $rx$ is irrational.

Now take $r + x$. Again suppose it's rational, so $r + x = \frac{i}{j} +
\frac{k}{l}$ for some $k, l \in \Z$. This implies $x = \frac{k}{l} -
\frac{i}{j} = \frac{jk - il}{lj}$ which is rational. This is a contradiction
again, since $x$ is irrational by definition. Therefore $r + x$ is irrational.

\textit{Better proof} If $rx$ is rational then $rx/r = x$ must be rational, since $\Q$
is closed under division. That's a contradiction, therefore $rx$ is not
rational. Similarly, if $r + x$ is rational then $x = r + x - r$ must be
rational, since $\Q$ is closed under addition and additive inverses. Again a
contradiction, showing that $r+x$ is irrational.

~\\\hrule
\textbf{1.2 Prove that there is no rational number whose square is 12.}

\textit{Lemma: The square root of a prime is irrational.}

\textit{Proof}: Let $p$ be prime and suppose $\sqrt{p}$ is rational. Then $\sqrt{p} =
i/j$ for some $i, j \in \Z$ with $i,j$ sharing no common factor. So $i^2 =
pj^2$. But $p$ is prime, so if $p$ is a factor of $i^2$ then $p$ must be a
factor of $i$ also. Therefore $i^2$ is divisible by $p^2$ and so $j^2$ must be
divisible by $p$ and so $j$ must be divisible by $p$. But $i,j$ share no common
factor by construction. This contradiction proves that $\sqrt{p}$ is
irrational.

Suppose $(i/j)^2 = 12$ for some $i,j \in \Z$. Then $i/j = 2\sqrt{3}$. But $3$
is prime and therefore $\sqrt{3}$ is irrational and we know that the product of
a rational and an irrational number is irrational. Therefore $i/j$ is
irrational, which is a contradiction proving that there is no rational number
whose square is 12.


~\\\hrule
\textbf{3. Prove Proposition 1.15 (implications of multiplication axioms)}

~\\\hrule
\textbf{4. Let $E$ be a nonempty subset of an ordered set ; suppose $\alpha$ is a
lower bound of E and that $\beta$ is an upper bound of $E$. Prove that $\alpha
\leq \beta$.}

Intuitively: it's non-empty, so the smallest it can be is one element. That
element could be $\alpha = \beta$ or else $\alpha < \beta$.

$\alpha$ is a lower bound for $E$, therefore $\alpha \leq e$ for every $e \in
E$. Similarly $\beta \geq e$ for every $e \in E$. Suppose that $\alpha >
\beta$. Then $\alpha > e$ for every $e \in E$. This contradicts the premise
that $\alpha$ is a lower bound for $E$, therefore $\alpha \leq \beta$.


~\\\hrule
\textbf{5. Let $A$ be a nonempty set of real numbers which is bounded below. Let $-A$
be the set of all numbers $-x$, where $x \in A$. Prove that
$$
\inf A = - \sup(-A)
$$
}

$A$ is bounded below, therefore $\inf A$ exists. Let $\alpha = \inf A$, i.e. $x
\geq \alpha$ for every $x \in A$, and $\alpha$ is the largest number for which
this is true. Therefore $-x \leq -\alpha$ for every $x \in A$ and $-\alpha$ is
the smallest number for which this is true. Therefore $-\alpha=\sup(-A)$ and
$\alpha=-\sup(-A)$.


~\\\hrule
\textbf{Definitions:}

\textbf{lower bound of $A$}: a number $\alpha$ such that $x \geq \alpha$ for every $x \in A$.

\textbf{$\inf A$}: the greatest lower bound (or $-\infty$ if there is no lower bound)

\textbf{upper bound of $A$}: a number $\alpha$ such that $x \leq \alpha$ for every $x \in A$.

\textbf{$\sup(-A)$}: the least upper bound of $-A$ (or $+\infty$ if there is no upper bound)

~\\\hrule
\textbf{6 Fix $b > 1$.}

\textbf{
(a) If $m,n,p,q$ are integers, $n>0$, $q>0$, and $r=\frac{m}{n}=\frac{p}{q}$,
prove that $(b^m)^{1/n} = (b^p)^{1/q}$. Hence it makes sense to define $b^r =
(b^m)^{1/n}$·}

Consider raising these quantities to the power of the integer $mq = np$:

$$
((b^m)^{1/n})^{np} = (((b^m)^{1/n})^{n})^p = (b^m)^p = b^{mp},
$$
and similarly
$$
((b^p)^{1/q})^{mq} = (((b^p)^{1/q})^{q})^m = (b^p)^m = b^{mp}.
$$

So both give the same result, but there is just one positive real number $r$
with the property that $r^{mq} = b^{mp}$. Therefore $(b^m)^{1/n} =
(b^p)^{1/q}$.

\textbf{
(b) Prove that $b^{r+s} = b^rb^s$ if $r$ and $s$ are rational.
}

Let $r=\frac{i}{j}$ and $s=\frac{k}{l}$. Then

$$
b^{r+s} =
b^\frac{iL + jk}{jl} =
(b^{iL + jk})^\frac{1}{jl} =
(b^{iL}b^{jk})^\frac{1}{jl} =
b^\frac{il}{jl}b^\frac{jk}{jl} =
b^{r}b^{s}
$$

\textbf{(c) If $x$ is real, define $B(x)$ to be the set of all numbers $b^t$, where
$t$ is rational and $t \leq x$. Prove that $$b^r = \sup B(r)$$ when $r$ is
rational. Hence it makes sense to define $$b^x = \sup B(x)$$ for every real $x$.
}

First consider rational $r$. $B(r)$ is the set $\{b^t: t \leq r\}$ for rational
$t$. It's clear that $b^r$ is an upper bound for this; we need to prove that it
is the least upper bound. (Transcribed from solutions) The approach we're going
to take is to show that, for any number $x$ smaller than $b^r$, we can always
construct a larger number that is in $B(r)$ and hence that $x$ is not the least
upper bound. Specifically, we're going to prove that for any $0 < x < b^r$
there exists an integer $n$ such that $x < b^{r - 1/n}$ and $b^{r - 1/n} \in
B(r)$.

$r$ is rational, so $r - 1/n$ is also rational, hence $b^{r - \frac{1}{n}}> \in B(r)$.

~\\\hrule
\textbf{Prove that $b^{x+y} = b^xb^y$ for all real $x$ and $y$. }

~\\\hrule
\textbf{7 Fix $b > 1, y > 0$ and prove that there is a unique real $x$ such that
$b^x = y$, by completing the following outline. (This is called the logarithm
of $y$ to the base $b$)}

\textbf{(a) For any positive integer $n$, $b^n - 1 \geq n(b-1)$.}

- \textit{Base case:} It's true for $n=1$, i.e. $b^1 - 1 = 1(b-1)$

- \textit{Induction:} Suppose that it is true for $n=i$, i.e. $b^i - 1 \geq i(b-1)$

    - \textit{Claim:} It is true for $n=i+1$, i.e. $b^{i+1} - 1 \geq (i+1)(b-1)$.

    - \textit{Proof:} The desired inequality can be written as $b^{i+1} \geq i(b-1) +
      b$, and the assumed inequality can be written as $b^i \geq i(b-1) +
      1$. Multiplying both sides of the latter by $b$ gives $b^{i+1} \geq
      bi(b-1) + b$. Now $i \geq 1$ and $b>1$, so $bi(b-1) > i(b-1)$, thus
      $b^{i+1} \geq i(b-1) + b$ as required.


\textbf{(b) Hence $b - 1 \geq n(b^{1/n} - 1)$}

In (a), $b$ was an arbitrary real number greater than 1. For clarity, let's
rewrite that result as $a^n - 1 \geq n(a-1)$. We can choose $a=b^{1/n}$ for
some other real number $b$. Thus $b - 1 \geq n(b^{1/n}-1)$.

\textbf{(c) If $t > 1$ and $n > \frac{b-1}{t-1}$, then $b^{1/n} < t$.}

Rearranging, we have $b^{1/n} \leq \frac{b-1}{n} + 1$. If $n > \frac{b-1}{t-1}$
then $b^{1/n} < \frac{b-1}{(b-1)/(t-1)} + 1 = t$.

\textbf{(d) If $w$ is such that $b^w < y$ then $b^{w + 1/n} < y$ for sufficiently
large $n$; to see this, apply part (c) with $t = yb^{-w}.$}

Let $t = \frac{y}{b^w}$. Since $b^w < y$, $\frac{y}{b^w} > 1$. Thus $b^{1/n} <
\frac{y}{b^w}$ and hence $b^{w + 1/n} < y$, iff $n > \frac{b-1}{(y -
b^w)/b^w}$.

\textbf{(e) If $b^w > y$, then $b^{w - 1/n} > y$ for sufficiently large $n$.}

Let $t = \frac{b^w}{y}$. Since $b^w > y$, $\frac{b^w}{y} > 1$. Thus $b^{1/n} <
\frac{b^w}{y}$ and hence $b^{w - 1/n} > y$, iff $n > \frac{b-1}{(b^w -
y)/y}$.


\textbf{(f) Let $A$ be the set of all $w$ such that $b^w < y$, and show that $x =
\sup A$ satisfies $b^x = y.$}

\textbf{(g) Prove that this $x$ is unique.}
\newpage
\section{Choimet \& Queffélec: Twelve Landmarks of Twentieth-Century Analysis}

\begin{claim*}
  For real $0 \leq x < 1$ and non-negative integer $n$, the following inequality holds:
  \begin{align*}
    1 - x^n \leq n(1 - x)
  \end{align*}
\end{claim*}

\begin{proof}[Proof 1 (induction)]~\\
  It is true for $n=0$. Suppose it is true for $n=k$. Then, after multiplying
  both sides by $x$, we have
  \begin{align*}
    x(1 - x^k)  &\leq kx(1 - x).
  \end{align*}
  Multiplying out and rearranging gives
  \begin{align*}
    x - x^{k+1}         &\leq kx - kx^2\\
    1 - 1 + x - x^{k+1} &\leq kx - kx^2\\
    1 - x^{k+1}         &\leq 1 + (k - 1)x - kx^2.
  \end{align*}
  The RHS factorises as $(1 + kx)(1 - x)$ which is less than
  $(1+k)(1-x)$. Therefore
   \begin{align*}
     1 - x^{k+1} &\leq (k + 1)(1 - x),
  \end{align*}
  which proves the claim by induction.
\end{proof}

\begin{proof}[Proof 2]~\\
  The claim is true for $n=0$, so restrict attention to $n > 0$. We want to show that
  \begin{align*}
    \frac{1 - x^n}{n(1 - x)} \leq 1.
  \end{align*}
  Note that
  \begin{align*}
    (1 - x)\sum_{i=0}^{n-1}x^i &= (1 + x + x^2 + \ldots + x^{n-1}) - (x + x^2 + x^3 + \ldots + x^n)\\
                              &= 1 - x^n.
  \end{align*}
  (This can be arrived at by performing long division of $1-x$ into $1 - x^n$.)

  Therefore the ratio is
  \begin{align*}
    \frac{1 - x^n}{n(1 - x)} &= \frac{(1 - x)\sum_{i=0}^{n-1}x^i}{n(1 - x)}\\
                             &= \frac{1}{n}\sum_{i=0}^{n-1}x^i,
  \end{align*}
  which is less than 1, as required.

\end{proof}
